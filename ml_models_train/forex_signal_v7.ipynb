{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12f565ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ FOREX SIGNAL GENERATOR V7 - Production Ready\n",
      "======================================================================\n",
      "‚úÖ Model Directory: c:\\Users\\Acer\\Desktop\\Forex-Signal-App\\models\\signal_generator_v7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "MODEL_DIR = BASE_DIR / 'models' / 'signal_generator_v7'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ FOREX SIGNAL GENERATOR V7 - Production Ready\")\n",
    "print(\"=\"*70)\n",
    "print(f\"‚úÖ Model Directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da38fe84",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fb5cf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,859,492 rows\n",
      "Test: 296,778 rows\n",
      "Train period: 2019-12-31 16:00:00+00:00 to 2024-12-30 16:00:00+00:00\n",
      "Test period: 2024-12-31 16:00:00+00:00 to 2025-10-17 06:11:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "train_df = pd.read_csv(DATA_DIR / 'EUR_USD_1min.csv')\n",
    "test_df = pd.read_csv(DATA_DIR / 'EUR_USD_test.csv')\n",
    "\n",
    "# Standardize columns\n",
    "for df in [train_df, test_df]:\n",
    "    if 'timestamp' in df.columns:\n",
    "        df.rename(columns={'timestamp': 'time'}, inplace=True)\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows\")\n",
    "print(f\"Test: {len(test_df):,} rows\")\n",
    "print(f\"Train period: {train_df['time'].min()} to {train_df['time'].max()}\")\n",
    "print(f\"Test period: {test_df['time'].min()} to {test_df['time'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351133b",
   "metadata": {},
   "source": [
    "## 2. V7 Enhanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00c61597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding V7 features...\n",
      "‚úì Features added. Total columns: 108\n",
      "‚úì Features added. Total columns: 108\n"
     ]
    }
   ],
   "source": [
    "def add_features_v7(df):\n",
    "    \"\"\"\n",
    "    V7 Features: V6 Core + Multi-Timeframe + Advanced Filters\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ==================== TIME FEATURES ====================\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['day_of_week'] = df['time'].dt.dayofweek\n",
    "    df['minute'] = df['time'].dt.minute\n",
    "    \n",
    "    # Trading Sessions (UTC) - More granular\n",
    "    df['is_asian'] = ((df['hour'] >= 0) & (df['hour'] < 8)).astype(int)\n",
    "    df['is_london'] = ((df['hour'] >= 7) & (df['hour'] < 16)).astype(int)\n",
    "    df['is_ny'] = ((df['hour'] >= 12) & (df['hour'] < 21)).astype(int)\n",
    "    df['is_overlap'] = ((df['hour'] >= 12) & (df['hour'] < 16)).astype(int)  # Best time!\n",
    "    \n",
    "    # Bad times to trade\n",
    "    df['is_low_liquidity'] = ((df['hour'] >= 21) | (df['hour'] < 2)).astype(int)\n",
    "    df['is_news_time'] = ((df['hour'].isin([8, 12, 13, 14])) & (df['minute'] < 35)).astype(int)\n",
    "    \n",
    "    # Day quality (Mon=0, Fri=4 are less reliable)\n",
    "    df['is_midweek'] = df['day_of_week'].isin([1, 2, 3]).astype(int)\n",
    "    \n",
    "    # ==================== PRICE FEATURES ====================\n",
    "    # Multi-period MAs\n",
    "    for p in [5, 10, 20, 50, 100, 200]:\n",
    "        df[f'sma_{p}'] = df['close'].rolling(p).mean()\n",
    "        df[f'ema_{p}'] = df['close'].ewm(span=p, adjust=False).mean()\n",
    "    \n",
    "    # MA Distances (normalized)\n",
    "    df['dist_sma20'] = (df['close'] - df['sma_20']) / df['sma_20'] * 100\n",
    "    df['dist_sma50'] = (df['close'] - df['sma_50']) / df['sma_50'] * 100\n",
    "    df['dist_sma200'] = (df['close'] - df['sma_200']) / df['sma_200'] * 100\n",
    "    \n",
    "    # MA Crossovers\n",
    "    df['sma_5_20_cross'] = (df['sma_5'] > df['sma_20']).astype(int)\n",
    "    df['sma_20_50_cross'] = (df['sma_20'] > df['sma_50']).astype(int)\n",
    "    df['ema_10_50_cross'] = (df['ema_10'] > df['ema_50']).astype(int)\n",
    "    df['golden_cross'] = (df['sma_50'] > df['sma_200']).astype(int)\n",
    "    \n",
    "    # ==================== MOMENTUM INDICATORS ====================\n",
    "    # RSI (multiple periods)\n",
    "    for period in [7, 14, 21]:\n",
    "        delta = df['close'].diff()\n",
    "        gain = delta.where(delta > 0, 0).rolling(period).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(period).mean()\n",
    "        rs = gain / (loss + 1e-10)\n",
    "        df[f'rsi_{period}'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # RSI Zones\n",
    "    df['rsi_oversold'] = (df['rsi_14'] < 30).astype(int)\n",
    "    df['rsi_overbought'] = (df['rsi_14'] > 70).astype(int)\n",
    "    df['rsi_neutral'] = ((df['rsi_14'] >= 40) & (df['rsi_14'] <= 60)).astype(int)\n",
    "    df['rsi_bullish_zone'] = ((df['rsi_14'] > 50) & (df['rsi_14'] < 70)).astype(int)\n",
    "    \n",
    "    # RSI Divergence (simplified)\n",
    "    df['rsi_slope'] = df['rsi_14'] - df['rsi_14'].shift(5)\n",
    "    df['price_slope'] = (df['close'] - df['close'].shift(5)) / df['close'].shift(5) * 100\n",
    "    df['rsi_divergence'] = np.sign(df['rsi_slope']) != np.sign(df['price_slope'])\n",
    "    \n",
    "    # MACD\n",
    "    ema12 = df['close'].ewm(span=12).mean()\n",
    "    ema26 = df['close'].ewm(span=26).mean()\n",
    "    df['macd'] = ema12 - ema26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    df['macd_cross_up'] = ((df['macd'] > df['macd_signal']) & \n",
    "                           (df['macd'].shift(1) <= df['macd_signal'].shift(1))).astype(int)\n",
    "    df['macd_bullish'] = ((df['macd'] > df['macd_signal']) & (df['macd_hist'] > 0)).astype(int)\n",
    "    \n",
    "    # Stochastic\n",
    "    for period in [14, 21]:\n",
    "        low_min = df['low'].rolling(period).min()\n",
    "        high_max = df['high'].rolling(period).max()\n",
    "        df[f'stoch_k_{period}'] = 100 * (df['close'] - low_min) / (high_max - low_min + 1e-10)\n",
    "        df[f'stoch_d_{period}'] = df[f'stoch_k_{period}'].rolling(3).mean()\n",
    "    \n",
    "    df['stoch_oversold'] = (df['stoch_k_14'] < 20).astype(int)\n",
    "    df['stoch_overbought'] = (df['stoch_k_14'] > 80).astype(int)\n",
    "    \n",
    "    # ==================== TREND STRENGTH (ADX) ====================\n",
    "    # True Range\n",
    "    df['tr'] = np.maximum(\n",
    "        df['high'] - df['low'],\n",
    "        np.maximum(\n",
    "            abs(df['high'] - df['close'].shift()),\n",
    "            abs(df['low'] - df['close'].shift())\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Directional Movement\n",
    "    df['up_move'] = df['high'] - df['high'].shift()\n",
    "    df['down_move'] = df['low'].shift() - df['low']\n",
    "    df['plus_dm'] = np.where((df['up_move'] > df['down_move']) & (df['up_move'] > 0), df['up_move'], 0)\n",
    "    df['minus_dm'] = np.where((df['down_move'] > df['up_move']) & (df['down_move'] > 0), df['down_move'], 0)\n",
    "    \n",
    "    # ADX calculation\n",
    "    period = 14\n",
    "    df['atr'] = df['tr'].rolling(period).mean()\n",
    "    df['plus_di'] = 100 * (df['plus_dm'].rolling(period).mean() / (df['atr'] + 1e-10))\n",
    "    df['minus_di'] = 100 * (df['minus_dm'].rolling(period).mean() / (df['atr'] + 1e-10))\n",
    "    df['dx'] = 100 * abs(df['plus_di'] - df['minus_di']) / (df['plus_di'] + df['minus_di'] + 1e-10)\n",
    "    df['adx'] = df['dx'].rolling(period).mean()\n",
    "    \n",
    "    # Trend Filters (KEY for V7!)\n",
    "    df['strong_trend'] = (df['adx'] > 25).astype(int)\n",
    "    df['very_strong_trend'] = (df['adx'] > 40).astype(int)\n",
    "    df['weak_trend'] = (df['adx'] < 20).astype(int)\n",
    "    df['trend_direction'] = np.where(df['plus_di'] > df['minus_di'], 1, -1)\n",
    "    df['trend_aligned'] = (df['strong_trend'] == 1) & (df['trend_direction'] == 1)\n",
    "    \n",
    "    # ==================== VOLATILITY ====================\n",
    "    df['atr_pips'] = df['atr'] * 10000\n",
    "    df['atr_ma'] = df['atr'].rolling(50).mean()\n",
    "    df['volatility_ratio'] = df['atr'] / (df['atr_ma'] + 1e-10)\n",
    "    \n",
    "    # Volatility Filters\n",
    "    df['normal_volatility'] = ((df['volatility_ratio'] > 0.7) & (df['volatility_ratio'] < 1.5)).astype(int)\n",
    "    df['high_volatility'] = (df['volatility_ratio'] > 1.5).astype(int)\n",
    "    df['low_volatility'] = (df['volatility_ratio'] < 0.5).astype(int)\n",
    "    \n",
    "    # ==================== BOLLINGER BANDS ====================\n",
    "    df['bb_mid'] = df['close'].rolling(20).mean()\n",
    "    df['bb_std'] = df['close'].rolling(20).std()\n",
    "    df['bb_upper'] = df['bb_mid'] + 2 * df['bb_std']\n",
    "    df['bb_lower'] = df['bb_mid'] - 2 * df['bb_std']\n",
    "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / (df['bb_mid'] + 1e-10)\n",
    "    df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'] + 1e-10)\n",
    "    df['bb_squeeze'] = (df['bb_width'] < df['bb_width'].rolling(50).mean() * 0.8).astype(int)\n",
    "    \n",
    "    # ==================== CCI ====================\n",
    "    tp = (df['high'] + df['low'] + df['close']) / 3\n",
    "    sma_tp = tp.rolling(20).mean()\n",
    "    mad_tp = tp.rolling(20).apply(lambda x: np.abs(x - x.mean()).mean())\n",
    "    df['cci'] = (tp - sma_tp) / (0.015 * mad_tp + 1e-10)\n",
    "    df['cci_bullish'] = (df['cci'] > 100).astype(int)\n",
    "    df['cci_oversold'] = (df['cci'] < -100).astype(int)\n",
    "    \n",
    "    # ==================== WILLIAMS %R ====================\n",
    "    hh = df['high'].rolling(14).max()\n",
    "    ll = df['low'].rolling(14).min()\n",
    "    df['williams_r'] = -100 * (hh - df['close']) / (hh - ll + 1e-10)\n",
    "    \n",
    "    # ==================== MULTI-TIMEFRAME (Simulated) ====================\n",
    "    # 5-min equivalent (5 bars)\n",
    "    df['close_5m'] = df['close'].rolling(5).mean()\n",
    "    df['trend_5m'] = (df['close_5m'] > df['close_5m'].shift(5)).astype(int)\n",
    "    \n",
    "    # 15-min equivalent (15 bars)\n",
    "    df['close_15m'] = df['close'].rolling(15).mean()\n",
    "    df['trend_15m'] = (df['close_15m'] > df['close_15m'].shift(15)).astype(int)\n",
    "    \n",
    "    # 1-hour equivalent (60 bars)\n",
    "    df['close_1h'] = df['close'].rolling(60).mean()\n",
    "    df['trend_1h'] = (df['close_1h'] > df['close_1h'].shift(60)).astype(int)\n",
    "    \n",
    "    # MTF Alignment (all timeframes agree)\n",
    "    df['mtf_bullish'] = ((df['trend_5m'] == 1) & (df['trend_15m'] == 1) & (df['trend_1h'] == 1)).astype(int)\n",
    "    df['mtf_bearish'] = ((df['trend_5m'] == 0) & (df['trend_15m'] == 0) & (df['trend_1h'] == 0)).astype(int)\n",
    "    \n",
    "    # ==================== CANDLE PATTERNS ====================\n",
    "    df['candle_body'] = df['close'] - df['open']\n",
    "    df['candle_range'] = df['high'] - df['low']\n",
    "    df['body_ratio'] = abs(df['candle_body']) / (df['candle_range'] + 1e-10)\n",
    "    df['upper_shadow'] = df['high'] - df[['open', 'close']].max(axis=1)\n",
    "    df['lower_shadow'] = df[['open', 'close']].min(axis=1) - df['low']\n",
    "    \n",
    "    df['is_bullish_candle'] = (df['close'] > df['open']).astype(int)\n",
    "    df['is_doji'] = (df['body_ratio'] < 0.1).astype(int)\n",
    "    df['is_hammer'] = ((df['lower_shadow'] > abs(df['candle_body']) * 2) & \n",
    "                       (df['upper_shadow'] < abs(df['candle_body']) * 0.5)).astype(int)\n",
    "    df['is_shooting_star'] = ((df['upper_shadow'] > abs(df['candle_body']) * 2) & \n",
    "                              (df['lower_shadow'] < abs(df['candle_body']) * 0.5)).astype(int)\n",
    "    \n",
    "    # Engulfing patterns\n",
    "    df['bullish_engulfing'] = ((df['is_bullish_candle'] == 1) & \n",
    "                               (df['is_bullish_candle'].shift(1) == 0) &\n",
    "                               (df['close'] > df['open'].shift(1)) &\n",
    "                               (df['open'] < df['close'].shift(1))).astype(int)\n",
    "    \n",
    "    # ==================== TREND ALIGNMENT ====================\n",
    "    df['trend_short'] = np.where(df['ema_10'] > df['ema_20'], 1, -1)\n",
    "    df['trend_medium'] = np.where(df['ema_20'] > df['ema_50'], 1, -1)\n",
    "    df['trend_long'] = np.where(df['ema_50'] > df['ema_200'], 1, -1)\n",
    "    df['all_trends_up'] = ((df['trend_short'] == 1) & (df['trend_medium'] == 1) & (df['trend_long'] == 1)).astype(int)\n",
    "    df['all_trends_down'] = ((df['trend_short'] == -1) & (df['trend_medium'] == -1) & (df['trend_long'] == -1)).astype(int)\n",
    "    \n",
    "    # ==================== COMPOSITE SCORES ====================\n",
    "    # Buy Score (combination of bullish signals)\n",
    "    df['buy_score'] = (\n",
    "        df['macd_bullish'] +\n",
    "        df['rsi_bullish_zone'] +\n",
    "        df['sma_5_20_cross'] +\n",
    "        df['golden_cross'] +\n",
    "        df['all_trends_up'] +\n",
    "        df['mtf_bullish'] +\n",
    "        (df['stoch_k_14'] > df['stoch_d_14']).astype(int) +\n",
    "        df['strong_trend'] * df['trend_aligned'].astype(int)\n",
    "    )\n",
    "    \n",
    "    # Quality Score (how good is the trading condition)\n",
    "    df['quality_score'] = (\n",
    "        df['is_overlap'] * 2 +  # Best time\n",
    "        df['is_midweek'] +      # Good day\n",
    "        df['normal_volatility'] +  # Good volatility\n",
    "        df['strong_trend'] +    # Clear trend\n",
    "        (1 - df['is_low_liquidity']) +  # Not low liquidity\n",
    "        (1 - df['is_news_time'])  # Not news time\n",
    "    )\n",
    "    \n",
    "    # Clean up temp columns\n",
    "    df.drop(['tr', 'up_move', 'down_move', 'plus_dm', 'minus_dm'], axis=1, inplace=True, errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Adding V7 features...\")\n",
    "train_df = add_features_v7(train_df)\n",
    "test_df = add_features_v7(test_df)\n",
    "print(f\"‚úì Features added. Total columns: {len(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406d61db",
   "metadata": {},
   "source": [
    "## 3. V7 Enhanced Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f2cc969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution (Train):\n",
      "  BUY:  193,796 (10.4%)\n",
      "  SELL: 199,453 (10.7%)\n",
      "  HOLD: 1,466,243 (78.9%)\n",
      "\n",
      "Label Distribution (Test):\n",
      "  BUY:  41,901 (14.1%)\n",
      "  SELL: 38,401 (12.9%)\n",
      "  HOLD: 216,476 (72.9%)\n"
     ]
    }
   ],
   "source": [
    "def create_labels_v7(df, forward_periods=60, min_pips=15, tp_sl_ratio=1.5):\n",
    "    \"\"\"\n",
    "    V7 Labeling: More realistic with quality filters\n",
    "    \n",
    "    BUY (1): Up move >= min_pips AND Up > Down * ratio\n",
    "    SELL (0): Down move >= min_pips AND Down > Up * ratio\n",
    "    \n",
    "    Additionally, we add a quality flag for filtering\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    min_move = min_pips * 0.0001\n",
    "    \n",
    "    # Future Max/Min\n",
    "    df['future_max'] = df['high'].rolling(forward_periods).max().shift(-forward_periods)\n",
    "    df['future_min'] = df['low'].rolling(forward_periods).min().shift(-forward_periods)\n",
    "    \n",
    "    df['up_move'] = df['future_max'] - df['close']\n",
    "    df['down_move'] = df['close'] - df['future_min']\n",
    "    \n",
    "    # Primary signal\n",
    "    conditions = [\n",
    "        (df['up_move'] >= min_move) & (df['up_move'] > df['down_move'] * tp_sl_ratio),\n",
    "        (df['down_move'] >= min_move) & (df['down_move'] > df['up_move'] * tp_sl_ratio)\n",
    "    ]\n",
    "    choices = [1, 0]  # BUY=1, SELL=0\n",
    "    df['signal'] = np.select(conditions, choices, default=-1)  # -1 = HOLD\n",
    "    \n",
    "    # Profit magnitude (for quality assessment)\n",
    "    df['profit_pips'] = np.where(\n",
    "        df['signal'] == 1, \n",
    "        df['up_move'] * 10000,\n",
    "        np.where(df['signal'] == 0, df['down_move'] * 10000, 0)\n",
    "    )\n",
    "    \n",
    "    # High quality signals (larger profits)\n",
    "    df['high_quality_signal'] = (df['profit_pips'] > 20).astype(int)\n",
    "    \n",
    "    # Drop helper columns\n",
    "    df.drop(['future_max', 'future_min', 'up_move', 'down_move', 'profit_pips'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = create_labels_v7(train_df)\n",
    "test_df = create_labels_v7(test_df)\n",
    "\n",
    "print(\"Label Distribution (Train):\")\n",
    "print(f\"  BUY:  {(train_df['signal'] == 1).sum():,} ({(train_df['signal'] == 1).mean()*100:.1f}%)\")\n",
    "print(f\"  SELL: {(train_df['signal'] == 0).sum():,} ({(train_df['signal'] == 0).mean()*100:.1f}%)\")\n",
    "print(f\"  HOLD: {(train_df['signal'] == -1).sum():,} ({(train_df['signal'] == -1).mean()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nLabel Distribution (Test):\")\n",
    "print(f\"  BUY:  {(test_df['signal'] == 1).sum():,} ({(test_df['signal'] == 1).mean()*100:.1f}%)\")\n",
    "print(f\"  SELL: {(test_df['signal'] == 0).sum():,} ({(test_df['signal'] == 0).mean()*100:.1f}%)\")\n",
    "print(f\"  HOLD: {(test_df['signal'] == -1).sum():,} ({(test_df['signal'] == -1).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e738fc3",
   "metadata": {},
   "source": [
    "## 4. Prepare Training Data (BUY vs SELL only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bed74b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 393,249\n",
      "Test samples: 80,296\n",
      "Features: 102\n",
      "BUY ratio (train): 49.3%\n",
      "BUY ratio (test): 52.2%\n"
     ]
    }
   ],
   "source": [
    "# Filter BUY/SELL only (remove HOLD)\n",
    "train_binary = train_df[train_df['signal'] != -1].copy()\n",
    "test_binary = test_df[test_df['signal'] != -1].copy()\n",
    "\n",
    "# Feature columns\n",
    "exclude_cols = ['time', 'signal', 'high_quality_signal', 'open', 'high', 'low', 'close', 'volume', 'tick_volume']\n",
    "feature_cols = [c for c in train_binary.columns if c not in exclude_cols]\n",
    "\n",
    "# Clean data\n",
    "train_clean = train_binary.dropna(subset=feature_cols).copy()\n",
    "test_clean = test_binary.dropna(subset=feature_cols).copy()\n",
    "\n",
    "X_train = train_clean[feature_cols].values\n",
    "y_train = train_clean['signal'].values\n",
    "X_test = test_clean[feature_cols].values\n",
    "y_test = test_clean['signal'].values\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Test samples: {len(X_test):,}\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"BUY ratio (train): {y_train.mean()*100:.1f}%\")\n",
    "print(f\"BUY ratio (test): {y_test.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b21b4",
   "metadata": {},
   "source": [
    "## 5. Train V7 Ensemble (5 GPU Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "196b763a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "\n",
      "üöÄ Training V7 Ensemble (GPU)...\n",
      "  Training XGB1... ‚úì Accuracy: 50.88%\n",
      "  Training XGB2... ‚úì Accuracy: 50.88%\n",
      "  Training XGB2... ‚úì Accuracy: 49.17%\n",
      "  Training LGB1... ‚úì Accuracy: 49.17%\n",
      "  Training LGB1... ‚úì Accuracy: 51.60%\n",
      "  Training LGB2... ‚úì Accuracy: 51.60%\n",
      "  Training LGB2... ‚úì Accuracy: 50.51%\n",
      "  Training CAT... ‚úì Accuracy: 50.51%\n",
      "  Training CAT... ‚úì Accuracy: 50.63%\n",
      "‚úì Accuracy: 50.63%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "print(f\"GPU Available: {GPU_AVAILABLE}\")\n",
    "\n",
    "models = {}\n",
    "\n",
    "# 1. XGBoost - Conservative\n",
    "models['xgb1'] = xgb.XGBClassifier(\n",
    "    n_estimators=600, max_depth=8, learning_rate=0.03,\n",
    "    subsample=0.8, colsample_bytree=0.8, \n",
    "    reg_alpha=0.1, reg_lambda=1.0,\n",
    "    random_state=42, tree_method='hist', device='cuda', verbosity=0\n",
    ")\n",
    "\n",
    "# 2. XGBoost - Aggressive\n",
    "models['xgb2'] = xgb.XGBClassifier(\n",
    "    n_estimators=400, max_depth=12, learning_rate=0.05,\n",
    "    subsample=0.7, colsample_bytree=0.7,\n",
    "    random_state=43, tree_method='hist', device='cuda', verbosity=0\n",
    ")\n",
    "\n",
    "# 3. LightGBM - Standard\n",
    "models['lgb1'] = lgb.LGBMClassifier(\n",
    "    n_estimators=600, max_depth=8, learning_rate=0.03,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    random_state=42, verbose=-1, device='gpu'\n",
    ")\n",
    "\n",
    "# 4. LightGBM - More leaves\n",
    "models['lgb2'] = lgb.LGBMClassifier(\n",
    "    n_estimators=500, max_depth=6, learning_rate=0.04,\n",
    "    num_leaves=127, min_child_samples=20,\n",
    "    subsample=0.75, colsample_bytree=0.75,\n",
    "    random_state=44, verbose=-1, device='gpu'\n",
    ")\n",
    "\n",
    "# 5. CatBoost\n",
    "models['cat'] = CatBoostClassifier(\n",
    "    iterations=600, depth=8, learning_rate=0.03,\n",
    "    l2_leaf_reg=3.0,\n",
    "    random_seed=42, task_type='GPU', devices='0', verbose=False\n",
    ")\n",
    "\n",
    "# Train all models\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "print(\"\\nüöÄ Training V7 Ensemble (GPU)...\")\n",
    "for name, model in models.items():\n",
    "    print(f\"  Training {name.upper()}...\", end=\" \")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    predictions[name] = model.predict(X_test_scaled)\n",
    "    probabilities[name] = model.predict_proba(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, predictions[name])\n",
    "    print(f\"‚úì Accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb489e2",
   "metadata": {},
   "source": [
    "## 6. V7 Ensemble with Quality Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9846412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä V7 ENSEMBLE RESULTS\n",
      "======================================================================\n",
      "All 5 models agree on BUY: 20,225\n",
      "4+ models agree on BUY: 31,346\n",
      "\n",
      "üìä BUY Signal Accuracy by Confidence:\n",
      "------------------------------------------------------------\n",
      "  Confidence |    Signals |    Correct |   Accuracy\n",
      "------------------------------------------------------------\n",
      "        50%+ |      40923 |      21327 |      52.1%\n",
      "        60%+ |      22046 |      11709 |      53.1%\n",
      "        70%+ |       5955 |       3188 |      53.5%\n",
      "        75%+ |       2104 |       1204 |      57.2%\n",
      "        80%+ |        770 |        402 |      52.2%\n",
      "        85%+ |        259 |        105 |      40.5%\n",
      "        90%+ |         88 |         43 |      48.9%\n",
      "        95%+ |         16 |         14 |      87.5%\n"
     ]
    }
   ],
   "source": [
    "# Weighted Ensemble\n",
    "weights = {'xgb1': 0.22, 'xgb2': 0.18, 'lgb1': 0.22, 'lgb2': 0.18, 'cat': 0.20}\n",
    "\n",
    "final_proba = np.zeros_like(probabilities['xgb1'])\n",
    "for name, w in weights.items():\n",
    "    final_proba += w * probabilities[name]\n",
    "\n",
    "buy_prob = final_proba[:, 1] * 100\n",
    "\n",
    "# Model Agreement Bonus\n",
    "agreement_count = sum([predictions[name] == 1 for name in models.keys()])\n",
    "all_agree_buy = agreement_count == 5\n",
    "most_agree_buy = agreement_count >= 4\n",
    "\n",
    "confidence = buy_prob.copy()\n",
    "confidence[all_agree_buy] = np.minimum(confidence[all_agree_buy] + 7, 100)\n",
    "confidence[most_agree_buy & ~all_agree_buy] = np.minimum(confidence[most_agree_buy & ~all_agree_buy] + 3, 100)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä V7 ENSEMBLE RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"All 5 models agree on BUY: {all_agree_buy.sum():,}\")\n",
    "print(f\"4+ models agree on BUY: {most_agree_buy.sum():,}\")\n",
    "\n",
    "# Results by Confidence\n",
    "print(\"\\nüìä BUY Signal Accuracy by Confidence:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Confidence':>12} | {'Signals':>10} | {'Correct':>10} | {'Accuracy':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "v7_results = {}\n",
    "for conf in [50, 60, 70, 75, 80, 85, 90, 95]:\n",
    "    mask = confidence >= conf\n",
    "    if mask.sum() > 0:\n",
    "        signals = mask.sum()\n",
    "        correct = y_test[mask].sum()\n",
    "        acc = correct / signals * 100\n",
    "        v7_results[conf] = {'signals': signals, 'correct': correct, 'accuracy': acc}\n",
    "        print(f\"{conf:>10}%+ | {signals:>10} | {correct:>10} | {acc:>9.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ca9381",
   "metadata": {},
   "source": [
    "## 7. Quality Filter Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d05a3320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä ACCURACY WITH QUALITY FILTERS\n",
      "======================================================================\n",
      "              Filter |  Conf |  Signals |   Accuracy\n",
      "-------------------------------------------------------\n",
      "           No Filter |   75% |     2104 |      57.2%\n",
      "           No Filter |   80% |      770 |      52.2%\n",
      "           No Filter |   85% |      259 |      40.5%\n",
      "        Strong Trend |   75% |     1535 |      60.1%\n",
      "        Strong Trend |   80% |      585 |      56.6%\n",
      "        Strong Trend |   85% |      200 |      48.5%\n",
      "   London/NY Overlap |   75% |      345 |      57.7%\n",
      "   London/NY Overlap |   80% |       48 |      39.6%\n",
      "   Normal Volatility |   75% |     1695 |      56.1%\n",
      "   Normal Volatility |   80% |      559 |      45.6%\n",
      "   Normal Volatility |   85% |      155 |      21.3%\n",
      "        Quality >= 4 |   75% |     1123 |      53.6%\n",
      "        Quality >= 4 |   80% |      327 |      41.9%\n",
      "        Quality >= 4 |   85% |       75 |      24.0%\n",
      "        Quality >= 5 |   75% |      649 |      56.4%\n",
      "        Quality >= 5 |   80% |      168 |      44.6%\n",
      "        Quality >= 5 |   85% |       31 |      35.5%\n",
      "     Trend + Overlap |   75% |      302 |      56.6%\n",
      "     Trend + Overlap |   80% |       45 |      35.6%\n",
      "         All Filters |   75% |      275 |      56.4%\n",
      "         All Filters |   80% |       43 |      32.6%\n",
      "\n",
      "======================================================================\n",
      "üèÜ BEST CONFIGURATION:\n",
      "   Filter: Strong Trend\n",
      "   Confidence: 75%+\n",
      "   Signals: 1535\n",
      "   Accuracy: 60.1%\n"
     ]
    }
   ],
   "source": [
    "# Get quality scores from test data\n",
    "quality_scores = test_clean['quality_score'].values\n",
    "strong_trend = test_clean['strong_trend'].values\n",
    "is_overlap = test_clean['is_overlap'].values\n",
    "normal_vol = test_clean['normal_volatility'].values\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä ACCURACY WITH QUALITY FILTERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test different filter combinations\n",
    "filters = {\n",
    "    'No Filter': np.ones(len(y_test), dtype=bool),\n",
    "    'Strong Trend': strong_trend == 1,\n",
    "    'London/NY Overlap': is_overlap == 1,\n",
    "    'Normal Volatility': normal_vol == 1,\n",
    "    'Quality >= 4': quality_scores >= 4,\n",
    "    'Quality >= 5': quality_scores >= 5,\n",
    "    'Trend + Overlap': (strong_trend == 1) & (is_overlap == 1),\n",
    "    'All Filters': (strong_trend == 1) & (is_overlap == 1) & (normal_vol == 1),\n",
    "}\n",
    "\n",
    "print(f\"{'Filter':>20} | {'Conf':>5} | {'Signals':>8} | {'Accuracy':>10}\")\n",
    "print(\"-\"*55)\n",
    "\n",
    "best_combo = {'filter': '', 'conf': 0, 'acc': 0, 'signals': 0}\n",
    "\n",
    "for filter_name, filter_mask in filters.items():\n",
    "    for conf in [75, 80, 85]:\n",
    "        mask = (confidence >= conf) & filter_mask\n",
    "        if mask.sum() >= 10:  # At least 10 signals\n",
    "            signals = mask.sum()\n",
    "            acc = y_test[mask].mean() * 100\n",
    "            print(f\"{filter_name:>20} | {conf:>4}% | {signals:>8} | {acc:>9.1f}%\")\n",
    "            \n",
    "            if acc > best_combo['acc'] and signals >= 20:\n",
    "                best_combo = {'filter': filter_name, 'conf': conf, 'acc': acc, 'signals': signals}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"üèÜ BEST CONFIGURATION:\")\n",
    "print(f\"   Filter: {best_combo['filter']}\")\n",
    "print(f\"   Confidence: {best_combo['conf']}%+\")\n",
    "print(f\"   Signals: {best_combo['signals']}\")\n",
    "print(f\"   Accuracy: {best_combo['acc']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd56dd1",
   "metadata": {},
   "source": [
    "## 8. V7 vs V6 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1eb0bf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä V7 vs V6 COMPARISON\n",
      "================================================================================\n",
      " Threshold |   V6 Sig |   V6 Acc |   V7 Sig |   V7 Acc |     Winner\n",
      "----------------------------------------------------------------------\n",
      "      70%+ |     6177 |    54.5% |     5955 |    53.5% |      ‚âà TIE\n",
      "      75%+ |     1990 |    57.1% |     2104 |    57.2% |      ‚âà TIE\n",
      "      80%+ |      475 |    65.7% |      770 |    52.2% |       V6 ‚úì\n",
      "      85%+ |       41 |    78.0% |      259 |    40.5% |       V6 ‚úì\n",
      "      90%+ |        0 |     0.0% |       88 |    48.9% |       V7 ‚úì\n"
     ]
    }
   ],
   "source": [
    "# Load V6 for comparison\n",
    "v6_dir = BASE_DIR / 'models' / 'signal_generator_v6'\n",
    "\n",
    "try:\n",
    "    v6_models = {}\n",
    "    for name in ['xgb1', 'xgb2', 'lgb1', 'lgb2', 'cat']:\n",
    "        v6_models[name] = joblib.load(v6_dir / f'{name}_v6_bin.joblib')\n",
    "    v6_scaler = joblib.load(v6_dir / 'scaler_v6_bin.joblib')\n",
    "    v6_feature_cols = joblib.load(v6_dir / 'feature_cols_v6.joblib')\n",
    "    \n",
    "    # Check missing features\n",
    "    missing = [c for c in v6_feature_cols if c not in test_clean.columns]\n",
    "    for c in missing:\n",
    "        test_clean[c] = 0\n",
    "    \n",
    "    X_test_v6 = test_clean[v6_feature_cols].values\n",
    "    X_test_v6_scaled = v6_scaler.transform(X_test_v6)\n",
    "    \n",
    "    # V6 predictions\n",
    "    v6_proba = {}\n",
    "    for name, model in v6_models.items():\n",
    "        v6_proba[name] = model.predict_proba(X_test_v6_scaled)\n",
    "    \n",
    "    v6_weights = {'xgb1': 0.20, 'xgb2': 0.20, 'lgb1': 0.20, 'lgb2': 0.20, 'cat': 0.20}\n",
    "    v6_final_proba = np.zeros_like(v6_proba['xgb1'])\n",
    "    for name, w in v6_weights.items():\n",
    "        v6_final_proba += w * v6_proba[name]\n",
    "    \n",
    "    v6_confidence = v6_final_proba[:, 1] * 100\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"üìä V7 vs V6 COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Threshold':>10} | {'V6 Sig':>8} | {'V6 Acc':>8} | {'V7 Sig':>8} | {'V7 Acc':>8} | {'Winner':>10}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for conf in [70, 75, 80, 85, 90]:\n",
    "        v6_mask = v6_confidence >= conf\n",
    "        v6_sig = v6_mask.sum()\n",
    "        v6_acc = y_test[v6_mask].mean() * 100 if v6_sig > 0 else 0\n",
    "        \n",
    "        v7_mask = confidence >= conf\n",
    "        v7_sig = v7_mask.sum()\n",
    "        v7_acc = y_test[v7_mask].mean() * 100 if v7_sig > 0 else 0\n",
    "        \n",
    "        winner = \"V7 ‚úì\" if v7_acc > v6_acc + 1 else (\"V6 ‚úì\" if v6_acc > v7_acc + 1 else \"‚âà TIE\")\n",
    "        print(f\"{conf:>8}%+ | {v6_sig:>8} | {v6_acc:>7.1f}% | {v7_sig:>8} | {v7_acc:>7.1f}% | {winner:>10}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"V6 comparison error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dbae5e",
   "metadata": {},
   "source": [
    "## 9. Save V7 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b36ff41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving V7 Models...\n",
      "\n",
      "‚úÖ V7 Models Saved Successfully!\n",
      "   Location: c:\\Users\\Acer\\Desktop\\Forex-Signal-App\\models\\signal_generator_v7\n",
      "   Best config: 75% threshold with Strong Trend\n",
      "\n",
      "‚úÖ V7 Models Saved Successfully!\n",
      "   Location: c:\\Users\\Acer\\Desktop\\Forex-Signal-App\\models\\signal_generator_v7\n",
      "   Best config: 75% threshold with Strong Trend\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving V7 Models...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    joblib.dump(model, MODEL_DIR / f'{name}_v7.joblib')\n",
    "\n",
    "joblib.dump(scaler, MODEL_DIR / 'scaler_v7.joblib')\n",
    "joblib.dump(feature_cols, MODEL_DIR / 'feature_cols_v7.joblib')\n",
    "joblib.dump(weights, MODEL_DIR / 'weights_v7.joblib')\n",
    "\n",
    "# Save config with best settings\n",
    "config = {\n",
    "    'version': 'v7',\n",
    "    'mode': 'BINARY_BUY_SELL',\n",
    "    'best_threshold': best_combo['conf'],\n",
    "    'best_filter': best_combo['filter'],\n",
    "    'models': list(models.keys()),\n",
    "    'weights': weights\n",
    "}\n",
    "joblib.dump(config, MODEL_DIR / 'config_v7.joblib')\n",
    "\n",
    "print(\"\\n‚úÖ V7 Models Saved Successfully!\")\n",
    "print(f\"   Location: {MODEL_DIR}\")\n",
    "print(f\"   Best config: {best_combo['conf']}% threshold with {best_combo['filter']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e560938",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "733306e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìã V7 SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üîß Features: 102\n",
      "üéØ Models: ['xgb1', 'xgb2', 'lgb1', 'lgb2', 'cat']\n",
      "üìä Test samples: 80,296\n",
      "\n",
      "üìà Best Results:\n",
      "   75%+ confidence: 57.2% accuracy (2104 signals)\n",
      "   80%+ confidence: 52.2% accuracy (770 signals)\n",
      "   85%+ confidence: 40.5% accuracy (259 signals)\n",
      "   90%+ confidence: 48.9% accuracy (88 signals)\n",
      "\n",
      "üèÜ Recommended: 75% threshold + Strong Trend\n",
      "   Expected Accuracy: 60.1%\n",
      "   Signals per test period: 1535\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìã V7 SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüîß Features: {len(feature_cols)}\")\n",
    "print(f\"üéØ Models: {list(models.keys())}\")\n",
    "print(f\"üìä Test samples: {len(y_test):,}\")\n",
    "\n",
    "print(\"\\nüìà Best Results:\")\n",
    "for conf in [75, 80, 85, 90]:\n",
    "    if conf in v7_results:\n",
    "        r = v7_results[conf]\n",
    "        print(f\"   {conf}%+ confidence: {r['accuracy']:.1f}% accuracy ({r['signals']} signals)\")\n",
    "\n",
    "print(f\"\\nüèÜ Recommended: {best_combo['conf']}% threshold + {best_combo['filter']}\")\n",
    "print(f\"   Expected Accuracy: {best_combo['acc']:.1f}%\")\n",
    "print(f\"   Signals per test period: {best_combo['signals']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
