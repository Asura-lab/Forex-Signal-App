{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "458d7719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸš€ FOREX SIGNAL GENERATOR V10\n",
      "   Target: Beat V8 (75%+ = 67.6%, 85%+ = 93.8%)\n",
      "======================================================================\n",
      "âœ“ GPU Available: True\n",
      "âœ“ Model Directory: c:\\Users\\Acer\\Desktop\\Forex-Signal-App\\ml_models_train\\models\\signal_generator_v10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "MODEL_DIR = BASE_DIR / 'models' / 'signal_generator_v10'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# GPU Check\n",
    "import torch\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸš€ FOREX SIGNAL GENERATOR V10\")\n",
    "print(\"   Target: Beat V8 (75%+ = 67.6%, 85%+ = 93.8%)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"âœ“ GPU Available: {GPU_AVAILABLE}\")\n",
    "print(f\"âœ“ Model Directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d343fba3",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bfb06f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,859,492 rows\n",
      "Test: 296,778 rows\n",
      "Train period: 2019-12-31 16:00:00+00:00 to 2024-12-30 16:00:00+00:00\n",
      "Test period: 2024-12-31 16:00:00+00:00 to 2025-10-17 06:11:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "train_df = pd.read_csv(DATA_DIR / 'EUR_USD_1min.csv')\n",
    "test_df = pd.read_csv(DATA_DIR / 'EUR_USD_test.csv')\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    if 'timestamp' in df.columns:\n",
    "        df.rename(columns={'timestamp': 'time'}, inplace=True)\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows\")\n",
    "print(f\"Test: {len(test_df):,} rows\")\n",
    "print(f\"Train period: {train_df['time'].min()} to {train_df['time'].max()}\")\n",
    "print(f\"Test period: {test_df['time'].min()} to {test_df['time'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e720e899",
   "metadata": {},
   "source": [
    "## 2. V10 Feature Engineering (V8 base + Enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c244a333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding V10 features...\n",
      "âœ“ Features added. Total columns: 69\n",
      "âœ“ Features added. Total columns: 69\n"
     ]
    }
   ],
   "source": [
    "def add_features_v10(df):\n",
    "    \"\"\"\n",
    "    V10 Features: V8-Ð¸Ð¹Ð½ Ð°Ð¼Ð¶Ð¸Ð»Ñ‚Ñ‚Ð°Ð¹ features + Ð¨Ð¸Ð½Ñ pattern features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ==================== V8 CORE FEATURES ====================\n",
    "    # Time Features\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['day_of_week'] = df['time'].dt.dayofweek\n",
    "    df['is_london'] = ((df['hour'] >= 8) & (df['hour'] < 16)).astype(int)\n",
    "    df['is_ny'] = ((df['hour'] >= 13) & (df['hour'] < 21)).astype(int)\n",
    "    df['is_overlap'] = ((df['hour'] >= 13) & (df['hour'] < 16)).astype(int)\n",
    "    \n",
    "    # Moving Averages\n",
    "    for p in [5, 10, 20, 50, 200]:\n",
    "        df[f'sma_{p}'] = df['close'].rolling(p).mean()\n",
    "        df[f'ema_{p}'] = df['close'].ewm(span=p, adjust=False).mean()\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    ema12 = df['close'].ewm(span=12).mean()\n",
    "    ema26 = df['close'].ewm(span=26).mean()\n",
    "    df['macd'] = ema12 - ema26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    df['bb_mid'] = df['close'].rolling(20).mean()\n",
    "    df['bb_std'] = df['close'].rolling(20).std()\n",
    "    df['bb_upper'] = df['bb_mid'] + 2 * df['bb_std']\n",
    "    df['bb_lower'] = df['bb_mid'] - 2 * df['bb_std']\n",
    "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / (df['bb_mid'] + 1e-10)\n",
    "    \n",
    "    # ADX\n",
    "    df['tr0'] = abs(df['high'] - df['low'])\n",
    "    df['tr1'] = abs(df['high'] - df['close'].shift())\n",
    "    df['tr2'] = abs(df['low'] - df['close'].shift())\n",
    "    df['tr'] = df[['tr0', 'tr1', 'tr2']].max(axis=1)\n",
    "    \n",
    "    df['up_move'] = df['high'] - df['high'].shift()\n",
    "    df['down_move'] = df['low'].shift() - df['low']\n",
    "    \n",
    "    df['plus_dm'] = np.where((df['up_move'] > df['down_move']) & (df['up_move'] > 0), df['up_move'], 0)\n",
    "    df['minus_dm'] = np.where((df['down_move'] > df['up_move']) & (df['down_move'] > 0), df['down_move'], 0)\n",
    "    \n",
    "    period = 14\n",
    "    df['atr'] = df['tr'].rolling(period).mean()\n",
    "    df['plus_di'] = 100 * (df['plus_dm'].rolling(period).mean() / (df['atr'] + 1e-10))\n",
    "    df['minus_di'] = 100 * (df['minus_dm'].rolling(period).mean() / (df['atr'] + 1e-10))\n",
    "    df['dx'] = 100 * abs(df['plus_di'] - df['minus_di']) / (df['plus_di'] + df['minus_di'] + 1e-10)\n",
    "    df['adx'] = df['dx'].rolling(period).mean()\n",
    "    \n",
    "    # CCI\n",
    "    tp = (df['high'] + df['low'] + df['close']) / 3\n",
    "    sma_tp = tp.rolling(20).mean()\n",
    "    mad_tp = tp.rolling(20).apply(lambda x: np.abs(x - x.mean()).mean())\n",
    "    df['cci'] = (tp - sma_tp) / (0.015 * mad_tp + 1e-10)\n",
    "    \n",
    "    # Williams %R\n",
    "    hh = df['high'].rolling(14).max()\n",
    "    ll = df['low'].rolling(14).min()\n",
    "    df['williams_r'] = -100 * (hh - df['close']) / (hh - ll + 1e-10)\n",
    "    \n",
    "    # Volatility\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['volatility'] = df['returns'].rolling(20).std() * 100\n",
    "    \n",
    "    # V8 Composite Features\n",
    "    df['rsi_x_adx'] = df['rsi'] * df['adx'] / 100\n",
    "    df['momentum_score'] = (\n",
    "        (df['rsi'] > 50).astype(int) + \n",
    "        (df['macd'] > df['macd_signal']).astype(int) + \n",
    "        (df['plus_di'] > df['minus_di']).astype(int)\n",
    "    )\n",
    "    df['price_position'] = (df['close'] - df['sma_50']) / (df['atr'] + 1e-10)\n",
    "    df['trend_score'] = (\n",
    "        (df['close'] > df['sma_20']).astype(int) +\n",
    "        (df['sma_20'] > df['sma_50']).astype(int) +\n",
    "        (df['sma_50'] > df['sma_200']).astype(int) +\n",
    "        (df['adx'] > 25).astype(int)\n",
    "    )\n",
    "    df['rsi_zone'] = pd.cut(df['rsi'], bins=[0, 30, 45, 55, 70, 100], labels=[0, 1, 2, 3, 4]).astype(float)\n",
    "    df['macd_momentum'] = df['macd_hist'] - df['macd_hist'].shift(3)\n",
    "    df['close_vs_high'] = (df['high'].rolling(20).max() - df['close']) / (df['atr'] + 1e-10)\n",
    "    df['close_vs_low'] = (df['close'] - df['low'].rolling(20).min()) / (df['atr'] + 1e-10)\n",
    "    \n",
    "    # ==================== V10 NEW FEATURES ====================\n",
    "    \n",
    "    # 1. Trend Strength Score (0-5)\n",
    "    df['trend_strength'] = (\n",
    "        (df['close'] > df['ema_5']).astype(int) +\n",
    "        (df['ema_5'] > df['ema_10']).astype(int) +\n",
    "        (df['ema_10'] > df['ema_20']).astype(int) +\n",
    "        (df['ema_20'] > df['ema_50']).astype(int) +\n",
    "        (df['adx'] > 20).astype(int)\n",
    "    )\n",
    "    \n",
    "    # 2. Momentum Alignment\n",
    "    df['momentum_alignment'] = (\n",
    "        (df['rsi'] > 55).astype(int) +\n",
    "        (df['macd_hist'] > 0).astype(int) +\n",
    "        (df['cci'] > 50).astype(int) +\n",
    "        (df['williams_r'] > -30).astype(int)\n",
    "    )\n",
    "    \n",
    "    # 3. Volatility State\n",
    "    df['volatility_sma'] = df['volatility'].rolling(50).mean()\n",
    "    df['volatility_state'] = np.where(\n",
    "        df['volatility'] > df['volatility_sma'] * 1.5, 2,  # High volatility\n",
    "        np.where(df['volatility'] < df['volatility_sma'] * 0.5, 0, 1)  # Low, Normal\n",
    "    )\n",
    "    \n",
    "    # 4. Price Action Patterns\n",
    "    df['body'] = df['close'] - df['open']\n",
    "    df['upper_wick'] = df['high'] - df[['open', 'close']].max(axis=1)\n",
    "    df['lower_wick'] = df[['open', 'close']].min(axis=1) - df['low']\n",
    "    df['body_ratio'] = abs(df['body']) / (df['high'] - df['low'] + 1e-10)\n",
    "    \n",
    "    # 5. Bullish/Bearish candle patterns\n",
    "    df['is_bullish'] = (df['close'] > df['open']).astype(int)\n",
    "    df['bullish_streak'] = df['is_bullish'].rolling(5).sum()\n",
    "    \n",
    "    # 6. Support/Resistance proximity\n",
    "    df['dist_to_high20'] = (df['high'].rolling(20).max() - df['close']) / (df['atr'] + 1e-10)\n",
    "    df['dist_to_low20'] = (df['close'] - df['low'].rolling(20).min()) / (df['atr'] + 1e-10)\n",
    "    \n",
    "    # 7. Multi-timeframe momentum (using rolling windows as proxy)\n",
    "    df['rsi_5'] = df['rsi'].rolling(5).mean()\n",
    "    df['rsi_20'] = df['rsi'].rolling(20).mean()\n",
    "    df['rsi_trend'] = df['rsi_5'] - df['rsi_20']\n",
    "    \n",
    "    # 8. Breakout Detection\n",
    "    df['above_bb_upper'] = (df['close'] > df['bb_upper']).astype(int)\n",
    "    df['below_bb_lower'] = (df['close'] < df['bb_lower']).astype(int)\n",
    "    df['bb_breakout'] = df['above_bb_upper'] - df['below_bb_lower']\n",
    "    \n",
    "    # 9. Price Momentum\n",
    "    df['price_change_5'] = (df['close'] - df['close'].shift(5)) / (df['atr'] + 1e-10)\n",
    "    df['price_change_10'] = (df['close'] - df['close'].shift(10)) / (df['atr'] + 1e-10)\n",
    "    df['price_change_20'] = (df['close'] - df['close'].shift(20)) / (df['atr'] + 1e-10)\n",
    "    \n",
    "    # 10. Session Quality (overlap = best trading time)\n",
    "    df['session_quality'] = df['is_london'].astype(int) + df['is_ny'].astype(int) + df['is_overlap'].astype(int) * 2\n",
    "    \n",
    "    # Cleanup temp columns\n",
    "    drop_cols = ['tr0', 'tr1', 'tr2', 'tr', 'up_move', 'down_move', 'plus_dm', 'minus_dm']\n",
    "    df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Adding V10 features...\")\n",
    "train_df = add_features_v10(train_df)\n",
    "test_df = add_features_v10(test_df)\n",
    "print(f\"âœ“ Features added. Total columns: {len(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e0ae3",
   "metadata": {},
   "source": [
    "## 3. V10 Labeling (More Selective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21a5c7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 393,249 (BUY/SELL only)\n",
      "Test: 80,302\n",
      "BUY ratio (train): 49.3%\n",
      "BUY ratio (test): 52.2%\n"
     ]
    }
   ],
   "source": [
    "def create_labels_v10(df, forward_periods=60, min_pips=15, ratio=1.5):\n",
    "    \"\"\"\n",
    "    V10 Labeling: V8-Ñ‚Ð°Ð¹ Ð°Ð´Ð¸Ð»Ñ…Ð°Ð½ Ð³ÑÑ…Ð´ÑÑ parameters-Ð¸Ð¹Ð³ Ñ‚Ð°Ð°Ñ€ÑƒÑƒÐ»Ð°Ñ… Ð±Ð¾Ð»Ð¾Ð¼Ð¶Ñ‚Ð¾Ð¹\n",
    "    \n",
    "    BUY (1): Up move >= min_pips AND Up > Down * ratio\n",
    "    SELL (0): Down move >= min_pips AND Down > Up * ratio\n",
    "    HOLD (-1): Neither\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    min_move = min_pips * 0.0001\n",
    "    \n",
    "    df['future_max'] = df['high'].rolling(forward_periods).max().shift(-forward_periods)\n",
    "    df['future_min'] = df['low'].rolling(forward_periods).min().shift(-forward_periods)\n",
    "    \n",
    "    df['up_move'] = df['future_max'] - df['close']\n",
    "    df['down_move'] = df['close'] - df['future_min']\n",
    "    \n",
    "    conditions = [\n",
    "        (df['up_move'] >= min_move) & (df['up_move'] > df['down_move'] * ratio),\n",
    "        (df['down_move'] >= min_move) & (df['down_move'] > df['up_move'] * ratio)\n",
    "    ]\n",
    "    choices = [1, 0]\n",
    "    df['signal'] = np.select(conditions, choices, default=-1)\n",
    "    \n",
    "    df.drop(['future_max', 'future_min', 'up_move', 'down_move'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# V10: Same parameters as V8 for fair comparison\n",
    "train_df = create_labels_v10(train_df, forward_periods=60, min_pips=15, ratio=1.5)\n",
    "test_df = create_labels_v10(test_df, forward_periods=60, min_pips=15, ratio=1.5)\n",
    "\n",
    "# Filter BUY/SELL only\n",
    "train_binary = train_df[train_df['signal'] != -1].copy()\n",
    "test_binary = test_df[test_df['signal'] != -1].copy()\n",
    "\n",
    "print(f\"Train: {len(train_binary):,} (BUY/SELL only)\")\n",
    "print(f\"Test: {len(test_binary):,}\")\n",
    "print(f\"BUY ratio (train): {train_binary['signal'].mean()*100:.1f}%\")\n",
    "print(f\"BUY ratio (test): {test_binary['signal'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc411a5",
   "metadata": {},
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b05dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features before selection: 63\n",
      "Features after selection: 32\n",
      "\n",
      "Top 20 Features:\n",
      "       feature  importance\n",
      "          hour         746\n",
      "volatility_sma         685\n",
      "       sma_200         548\n",
      "   day_of_week         450\n",
      "       ema_200         422\n",
      "      bb_lower         254\n",
      "      bb_upper         207\n",
      "    volatility         204\n",
      "           atr         201\n",
      "        sma_50         192\n",
      "   macd_signal         163\n",
      "        ema_50         139\n",
      "         ema_5         130\n",
      "        rsi_20         125\n",
      "price_position         116\n",
      "         sma_5         105\n",
      "        ema_20          98\n",
      "           adx          94\n",
      "        bb_std          73\n",
      "          macd          70\n",
      "Features after selection: 32\n",
      "\n",
      "Top 20 Features:\n",
      "       feature  importance\n",
      "          hour         746\n",
      "volatility_sma         685\n",
      "       sma_200         548\n",
      "   day_of_week         450\n",
      "       ema_200         422\n",
      "      bb_lower         254\n",
      "      bb_upper         207\n",
      "    volatility         204\n",
      "           atr         201\n",
      "        sma_50         192\n",
      "   macd_signal         163\n",
      "        ema_50         139\n",
      "         ema_5         130\n",
      "        rsi_20         125\n",
      "price_position         116\n",
      "         sma_5         105\n",
      "        ema_20          98\n",
      "           adx          94\n",
      "        bb_std          73\n",
      "          macd          70\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "exclude_cols = ['time', 'signal', 'open', 'high', 'low', 'close', 'volume', 'tick_volume']\n",
    "feature_cols = [c for c in train_binary.columns if c not in exclude_cols]\n",
    "\n",
    "train_clean = train_binary.dropna(subset=feature_cols).copy()\n",
    "test_clean = test_binary.dropna(subset=feature_cols).copy()\n",
    "\n",
    "X_train_full = train_clean[feature_cols].values\n",
    "y_train = train_clean['signal'].values\n",
    "X_test_full = test_clean[feature_cols].values\n",
    "y_test = test_clean['signal'].values\n",
    "\n",
    "print(f\"Features before selection: {len(feature_cols)}\")\n",
    "\n",
    "# Feature Importance based selection\n",
    "selector_model = lgb.LGBMClassifier(\n",
    "    n_estimators=200, max_depth=6, random_state=42, verbose=-1, device='gpu'\n",
    ")\n",
    "selector_model.fit(X_train_full, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = selector_model.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Select top features - use median threshold (like V8)\n",
    "threshold = np.median(importances)\n",
    "selected_features = importance_df[importance_df['importance'] >= threshold]['feature'].tolist()\n",
    "\n",
    "print(f\"Features after selection: {len(selected_features)}\")\n",
    "print(f\"\\nTop 20 Features:\")\n",
    "print(importance_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecb45d7",
   "metadata": {},
   "source": [
    "## 5. Prepare Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dba24022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (393152, 32)\n",
      "Test data: (80288, 32)\n"
     ]
    }
   ],
   "source": [
    "# Use selected features\n",
    "X_train = train_clean[selected_features].values\n",
    "X_test = test_clean[selected_features].values\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training data: {X_train_scaled.shape}\")\n",
    "print(f\"Test data: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a826c888",
   "metadata": {},
   "source": [
    "## 6. V10 Model Training (7 Diverse Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c4f2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸš€ TRAINING V10 MODELS (7 Diverse Ensemble)\n",
      "======================================================================\n",
      "  Training XGB1... Test: 50.73%\n",
      "  Training XGB2... Test: 50.73%\n",
      "  Training XGB2... Test: 50.11%\n",
      "  Training XGB3... Test: 50.11%\n",
      "  Training XGB3... Test: 50.82%\n",
      "  Training LGB1... Test: 50.82%\n",
      "  Training LGB1... Test: 50.61%\n",
      "  Training LGB2... Test: 50.61%\n",
      "  Training LGB2... Test: 49.88%\n",
      "  Training CAT1... Test: 49.88%\n",
      "  Training CAT1... Test: 51.28%\n",
      "  Training CAT2... Test: 51.28%\n",
      "  Training CAT2... Test: 50.35%\n",
      "\n",
      "âœ“ All 7 models trained\n",
      "Test: 50.35%\n",
      "\n",
      "âœ“ All 7 models trained\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸš€ TRAINING V10 MODELS (7 Diverse Ensemble)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models = {}\n",
    "\n",
    "# 1. XGBoost - Primary (like V8)\n",
    "models['xgb1'] = xgb.XGBClassifier(\n",
    "    n_estimators=600, max_depth=6, learning_rate=0.03,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    reg_alpha=0.1, reg_lambda=1.0,\n",
    "    min_child_weight=3,\n",
    "    random_state=42, tree_method='hist', device='cuda', verbosity=0\n",
    ")\n",
    "\n",
    "# 2. XGBoost - Deeper\n",
    "models['xgb2'] = xgb.XGBClassifier(\n",
    "    n_estimators=400, max_depth=8, learning_rate=0.05,\n",
    "    subsample=0.7, colsample_bytree=0.7,\n",
    "    reg_alpha=0.05, reg_lambda=0.5,\n",
    "    gamma=0.1,\n",
    "    random_state=43, tree_method='hist', device='cuda', verbosity=0\n",
    ")\n",
    "\n",
    "# 3. XGBoost - Conservative\n",
    "models['xgb3'] = xgb.XGBClassifier(\n",
    "    n_estimators=800, max_depth=4, learning_rate=0.02,\n",
    "    subsample=0.85, colsample_bytree=0.85,\n",
    "    reg_alpha=0.2, reg_lambda=2.0,\n",
    "    min_child_weight=5,\n",
    "    random_state=44, tree_method='hist', device='cuda', verbosity=0\n",
    ")\n",
    "\n",
    "# 4. LightGBM - Primary (like V8)\n",
    "models['lgb1'] = lgb.LGBMClassifier(\n",
    "    n_estimators=600, max_depth=6, learning_rate=0.03,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    num_leaves=31, min_child_samples=30,\n",
    "    reg_alpha=0.1, reg_lambda=1.0,\n",
    "    random_state=42, verbose=-1, device='gpu'\n",
    ")\n",
    "\n",
    "# 5. LightGBM - More leaves\n",
    "models['lgb2'] = lgb.LGBMClassifier(\n",
    "    n_estimators=500, max_depth=8, learning_rate=0.04,\n",
    "    subsample=0.75, colsample_bytree=0.75,\n",
    "    num_leaves=63, min_child_samples=20,\n",
    "    reg_alpha=0.05, reg_lambda=0.5,\n",
    "    random_state=45, verbose=-1, device='gpu'\n",
    ")\n",
    "\n",
    "# 6. CatBoost - Primary (like V8)\n",
    "models['cat1'] = CatBoostClassifier(\n",
    "    iterations=600, depth=6, learning_rate=0.03,\n",
    "    l2_leaf_reg=3.0, random_strength=0.5,\n",
    "    bagging_temperature=0.5,\n",
    "    random_seed=42, task_type='GPU', devices='0', verbose=False\n",
    ")\n",
    "\n",
    "# 7. CatBoost - Deeper\n",
    "models['cat2'] = CatBoostClassifier(\n",
    "    iterations=500, depth=8, learning_rate=0.04,\n",
    "    l2_leaf_reg=2.0, random_strength=0.3,\n",
    "    bagging_temperature=0.3,\n",
    "    random_seed=46, task_type='GPU', devices='0', verbose=False\n",
    ")\n",
    "\n",
    "# Train all models\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"  Training {name.upper()}...\", end=\" \")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    predictions[name] = model.predict(X_test_scaled)\n",
    "    probabilities[name] = model.predict_proba(X_test_scaled)\n",
    "    test_acc = accuracy_score(y_test, predictions[name])\n",
    "    print(f\"Test: {test_acc*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nâœ“ All {len(models)} models trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b9388d",
   "metadata": {},
   "source": [
    "## 7. V10 Ensemble (Weighted + Agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7fee692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ”§ BUILDING V10 ENSEMBLE\n",
      "======================================================================\n",
      "Model Weights (accuracy-based):\n",
      "  cat1: 0.1449 (acc: 51.28%)\n",
      "  xgb3: 0.1437 (acc: 50.82%)\n",
      "  xgb1: 0.1434 (acc: 50.73%)\n",
      "  lgb1: 0.1431 (acc: 50.61%)\n",
      "  cat2: 0.1423 (acc: 50.35%)\n",
      "  xgb2: 0.1416 (acc: 50.11%)\n",
      "  lgb2: 0.1410 (acc: 49.88%)\n",
      "\n",
      "Agreement Analysis:\n",
      "  All 7 agree BUY: 19,271\n",
      "  6+ agree BUY: 26,961\n",
      "  5+ agree BUY: 33,905\n",
      "  All 7 agree SELL: 20,391\n",
      "  6+ agree SELL: 28,233\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ”§ BUILDING V10 ENSEMBLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Accuracy-based weights\n",
    "accuracies = {name: accuracy_score(y_test, predictions[name]) for name in models.keys()}\n",
    "total_acc = sum(accuracies.values())\n",
    "weights = {name: acc / total_acc for name, acc in accuracies.items()}\n",
    "\n",
    "print(\"Model Weights (accuracy-based):\")\n",
    "for name, w in sorted(weights.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {name}: {w:.4f} (acc: {accuracies[name]*100:.2f}%)\")\n",
    "\n",
    "# Weighted Ensemble Probability\n",
    "final_proba = np.zeros_like(probabilities['xgb1'])\n",
    "for name, w in weights.items():\n",
    "    final_proba += w * probabilities[name]\n",
    "\n",
    "buy_prob = final_proba[:, 1] * 100\n",
    "\n",
    "# Model Agreement Analysis\n",
    "all_preds = np.array([predictions[name] for name in models.keys()])\n",
    "buy_votes = np.sum(all_preds == 1, axis=0)\n",
    "sell_votes = np.sum(all_preds == 0, axis=0)\n",
    "\n",
    "# Agreement levels\n",
    "all_agree_buy = buy_votes == len(models)  # 7/7\n",
    "strong_buy = buy_votes >= 6  # 6+/7\n",
    "majority_buy = buy_votes >= 5  # 5+/7\n",
    "\n",
    "all_agree_sell = sell_votes == len(models)  # 7/7\n",
    "strong_sell = sell_votes >= 6  # 6+/7\n",
    "\n",
    "print(f\"\\nAgreement Analysis:\")\n",
    "print(f\"  All 7 agree BUY: {all_agree_buy.sum():,}\")\n",
    "print(f\"  6+ agree BUY: {strong_buy.sum():,}\")\n",
    "print(f\"  5+ agree BUY: {majority_buy.sum():,}\")\n",
    "print(f\"  All 7 agree SELL: {all_agree_sell.sum():,}\")\n",
    "print(f\"  6+ agree SELL: {strong_sell.sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5328d",
   "metadata": {},
   "source": [
    "## 8. V10 Confidence Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07f7c017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸŽ¯ CALCULATING V10 CONFIDENCE\n",
      "======================================================================\n",
      "Confidence distribution:\n",
      "  >= 90%: 17\n",
      "  >= 85%: 64\n",
      "  >= 80%: 255\n",
      "  >= 75%: 826\n",
      "  >= 70%: 2,606\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸŽ¯ CALCULATING V10 CONFIDENCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Base confidence from ensemble probability\n",
    "confidence = buy_prob.copy()\n",
    "\n",
    "# V10 Agreement Bonus (7 models)\n",
    "# All 7 agree = +7 bonus\n",
    "confidence[all_agree_buy] = np.minimum(confidence[all_agree_buy] + 7, 100)\n",
    "# 6+ agree = +4 bonus\n",
    "confidence[strong_buy & ~all_agree_buy] = np.minimum(confidence[strong_buy & ~all_agree_buy] + 4, 100)\n",
    "# 5+ agree = +2 bonus\n",
    "confidence[majority_buy & ~strong_buy] = np.minimum(confidence[majority_buy & ~strong_buy] + 2, 100)\n",
    "\n",
    "print(f\"Confidence distribution:\")\n",
    "print(f\"  >= 90%: {(confidence >= 90).sum():,}\")\n",
    "print(f\"  >= 85%: {(confidence >= 85).sum():,}\")\n",
    "print(f\"  >= 80%: {(confidence >= 80).sum():,}\")\n",
    "print(f\"  >= 75%: {(confidence >= 75).sum():,}\")\n",
    "print(f\"  >= 70%: {(confidence >= 70).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eab7fc3",
   "metadata": {},
   "source": [
    "## 9. V10 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ed92581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ“Š V10 ENSEMBLE RESULTS\n",
      "======================================================================\n",
      "\n",
      "  Confidence |  BUY Signals |    Correct |   Accuracy\n",
      "------------------------------------------------------------\n",
      "        50%+ |        40109 |      21040 |      52.5%\n",
      "        55%+ |        28805 |      15266 |      53.0%\n",
      "        60%+ |        19866 |      10654 |      53.6%\n",
      "        65%+ |         9082 |       4855 |      53.5%\n",
      "        70%+ |         2606 |       1485 |      57.0%\n",
      "        75%+ |          826 |        501 |      60.7%\n",
      "        80%+ |          255 |        183 |      71.8% ðŸŽ¯\n",
      "        85%+ |           64 |         62 |      96.9% ðŸŽ¯\n",
      "        90%+ |           17 |         17 |     100.0% ðŸŽ¯\n",
      "\n",
      "------------------------------------------------------------\n",
      "ðŸŽ¯ = V8 75%+ benchmark (67.6%) Ñ‚Ð°Ð¹ Ñ‚ÑÐ½Ñ†Ò¯Ò¯ ÑÑÐ²ÑÐ» Ð¸Ð»Ò¯Ò¯\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ“Š V10 ENSEMBLE RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Confidence':>12} | {'BUY Signals':>12} | {'Correct':>10} | {'Accuracy':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "v10_results = {}\n",
    "for conf in [50, 55, 60, 65, 70, 75, 80, 85, 90, 95]:\n",
    "    mask = confidence >= conf\n",
    "    if mask.sum() > 0:\n",
    "        signals = mask.sum()\n",
    "        correct = y_test[mask].sum()  # BUY correct\n",
    "        acc = correct / signals * 100\n",
    "        v10_results[conf] = {'signals': signals, 'correct': int(correct), 'accuracy': acc}\n",
    "        marker = \" ðŸŽ¯\" if acc >= 67.6 else \"\"  # V8 75% benchmark\n",
    "        print(f\"{conf:>10}%+ | {signals:>12} | {correct:>10.0f} | {acc:>9.1f}%{marker}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"ðŸŽ¯ = V8 75%+ benchmark (67.6%) Ñ‚Ð°Ð¹ Ñ‚ÑÐ½Ñ†Ò¯Ò¯ ÑÑÐ²ÑÐ» Ð¸Ð»Ò¯Ò¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c0952",
   "metadata": {},
   "source": [
    "## 10. V10 vs V8 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e894a8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BASE_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load V8 for comparison\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m v8_dir = \u001b[43mBASE_DIR\u001b[49m / \u001b[33m'\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m'\u001b[39m / \u001b[33m'\u001b[39m\u001b[33msignal_generator_v8\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      5\u001b[39m     v8_models = {}\n",
      "\u001b[31mNameError\u001b[39m: name 'BASE_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# Load V8 for comparison\n",
    "v8_dir = BASE_DIR / 'models' / 'signal_generator_v8'\n",
    "\n",
    "try:\n",
    "    v8_models = {}\n",
    "    for name in ['xgb1', 'xgb2', 'lgb1', 'lgb2', 'cat']:\n",
    "        v8_models[name] = joblib.load(v8_dir / f'{name}_v8.joblib')\n",
    "    \n",
    "    v8_scaler = joblib.load(v8_dir / 'scaler_v8.joblib')\n",
    "    v8_feature_cols = joblib.load(v8_dir / 'feature_cols_v8.joblib')\n",
    "    v8_weights = joblib.load(v8_dir / 'weights_v8.joblib')\n",
    "    \n",
    "    # Prepare V8 test data\n",
    "    missing = [c for c in v8_feature_cols if c not in test_clean.columns]\n",
    "    for c in missing:\n",
    "        test_clean[c] = 0\n",
    "    \n",
    "    X_test_v8 = test_clean[v8_feature_cols].values\n",
    "    X_test_v8_scaled = v8_scaler.transform(X_test_v8)\n",
    "    \n",
    "    # V8 predictions\n",
    "    v8_proba = {}\n",
    "    v8_preds = {}\n",
    "    for name, model in v8_models.items():\n",
    "        v8_preds[name] = model.predict(X_test_v8_scaled)\n",
    "        v8_proba[name] = model.predict_proba(X_test_v8_scaled)\n",
    "    \n",
    "    # V8 ensemble\n",
    "    v8_final_proba = np.zeros_like(v8_proba['xgb1'])\n",
    "    for name, w in v8_weights.items():\n",
    "        v8_final_proba += w * v8_proba[name]\n",
    "    \n",
    "    v8_buy_prob = v8_final_proba[:, 1] * 100\n",
    "    v8_all_agree = np.all([v8_preds[name] == 1 for name in v8_models.keys()], axis=0)\n",
    "    v8_most_agree = np.sum([v8_preds[name] == 1 for name in v8_models.keys()], axis=0) >= 4\n",
    "    v8_confidence = v8_buy_prob.copy()\n",
    "    v8_confidence[v8_all_agree] = np.minimum(v8_confidence[v8_all_agree] + 5, 100)\n",
    "    v8_confidence[v8_most_agree & ~v8_all_agree] = np.minimum(v8_confidence[v8_most_agree & ~v8_all_agree] + 2, 100)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ðŸ“Š V10 vs V8 COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n{'Threshold':>10} | {'V8 Sig':>8} | {'V8 Acc':>8} | {'V10 Sig':>8} | {'V10 Acc':>8} | {'Winner':>10}\")\n",
    "    print(\"-\"*75)\n",
    "    \n",
    "    for conf in [70, 75, 80, 85, 90]:\n",
    "        # V8\n",
    "        v8_mask = v8_confidence >= conf\n",
    "        v8_sig = v8_mask.sum()\n",
    "        v8_acc = y_test[v8_mask].mean() * 100 if v8_sig > 0 else 0\n",
    "        \n",
    "        # V10\n",
    "        if conf in v10_results:\n",
    "            v10_sig = v10_results[conf]['signals']\n",
    "            v10_acc = v10_results[conf]['accuracy']\n",
    "        else:\n",
    "            v10_sig = 0\n",
    "            v10_acc = 0\n",
    "        \n",
    "        if v10_acc > v8_acc + 1:\n",
    "            winner = \"V10 âœ“\"\n",
    "        elif v8_acc > v10_acc + 1:\n",
    "            winner = \"V8 âœ“\"\n",
    "        else:\n",
    "            winner = \"â‰ˆ TIE\"\n",
    "        \n",
    "        print(f\"{conf:>8}%+ | {v8_sig:>8} | {v8_acc:>7.1f}% | {v10_sig:>8} | {v10_acc:>7.1f}% | {winner:>10}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“‹ V8 ORIGINAL BENCHMARK (from V8 notebook):\")\n",
    "    print(\"   75%+: 627 signals, 67.6% accuracy\")\n",
    "    print(\"   85%+: 48 signals, 93.8% accuracy\")\n",
    "    print(\"   90%+: 8 signals, 100% accuracy\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"V8 models not found: {e}\")\n",
    "    print(\"\\nðŸ“‹ V8 BENCHMARK:\")\n",
    "    print(\"   75%+: 627 signals, 67.6% accuracy\")\n",
    "    print(\"   85%+: 48 signals, 93.8% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21a4b2c",
   "metadata": {},
   "source": [
    "## 11. Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91ebb7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ“Š STATISTICAL SIGNIFICANCE (95% Confidence Interval)\n",
      "======================================================================\n",
      "\n",
      " Threshold |  Signals |   Accuracy |               95% CI |    Reliable?\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'v10_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m75\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m conf \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m70\u001b[39m, \u001b[32m75\u001b[39m, \u001b[32m80\u001b[39m, \u001b[32m85\u001b[39m, \u001b[32m90\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m conf \u001b[38;5;129;01min\u001b[39;00m \u001b[43mv10_results\u001b[49m:\n\u001b[32m     27\u001b[39m         r = v10_results[conf]\n\u001b[32m     28\u001b[39m         n_total = r[\u001b[33m'\u001b[39m\u001b[33msignals\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'v10_results' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ“Š STATISTICAL SIGNIFICANCE (95% Confidence Interval)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def confidence_interval(n_success, n_total, confidence=0.95):\n",
    "    \"\"\"Wilson score interval for proportion\"\"\"\n",
    "    if n_total == 0:\n",
    "        return 0, 0, 0\n",
    "    p = n_success / n_total\n",
    "    z = stats.norm.ppf((1 + confidence) / 2)\n",
    "    \n",
    "    denominator = 1 + z**2 / n_total\n",
    "    center = (p + z**2 / (2 * n_total)) / denominator\n",
    "    spread = z * np.sqrt((p * (1 - p) + z**2 / (4 * n_total)) / n_total) / denominator\n",
    "    \n",
    "    lower = max(0, center - spread)\n",
    "    upper = min(1, center + spread)\n",
    "    return p, lower, upper\n",
    "\n",
    "print(f\"\\n{'Threshold':>10} | {'Signals':>8} | {'Accuracy':>10} | {'95% CI':>20} | {'Reliable?':>12}\")\n",
    "print(\"-\"*75)\n",
    "\n",
    "for conf in [70, 75, 80, 85, 90]:\n",
    "    if conf in v10_results:\n",
    "        r = v10_results[conf]\n",
    "        n_total = r['signals']\n",
    "        n_success = r['correct']\n",
    "        \n",
    "        acc, lower, upper = confidence_interval(n_success, n_total)\n",
    "        ci_str = f\"[{lower*100:.1f}% - {upper*100:.1f}%]\"\n",
    "        \n",
    "        if n_total >= 100 and lower > 0.60:\n",
    "            reliable = \"âœ… RELIABLE\"\n",
    "        elif n_total >= 50 and lower > 0.55:\n",
    "            reliable = \"âš¡ MODERATE\"\n",
    "        elif n_total < 30:\n",
    "            reliable = \"âš ï¸ TOO FEW\"\n",
    "        else:\n",
    "            reliable = \"âŒ WEAK\"\n",
    "        \n",
    "        print(f\"{conf:>8}%+ | {n_total:>8} | {acc*100:>9.1f}% | {ci_str:>20} | {reliable:>12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f358f507",
   "metadata": {},
   "source": [
    "## 12. Overfit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dec79c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ” OVERFIT ANALYSIS: Train vs Test\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m train_proba_dict = {}\n\u001b[32m      7\u001b[39m train_preds_dict = {}\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodels\u001b[49m.items():\n\u001b[32m      9\u001b[39m     train_proba_dict[name] = model.predict_proba(X_train_scaled)\n\u001b[32m     10\u001b[39m     train_preds_dict[name] = model.predict(X_train_scaled)\n",
      "\u001b[31mNameError\u001b[39m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ” OVERFIT ANALYSIS: Train vs Test\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train predictions\n",
    "train_proba_dict = {}\n",
    "train_preds_dict = {}\n",
    "for name, model in models.items():\n",
    "    train_proba_dict[name] = model.predict_proba(X_train_scaled)\n",
    "    train_preds_dict[name] = model.predict(X_train_scaled)\n",
    "\n",
    "# Train ensemble\n",
    "train_final_proba = np.zeros_like(train_proba_dict['xgb1'])\n",
    "for name, w in weights.items():\n",
    "    train_final_proba += w * train_proba_dict[name]\n",
    "\n",
    "train_buy_prob = train_final_proba[:, 1] * 100\n",
    "\n",
    "# Agreement bonus\n",
    "train_all_preds = np.array([train_preds_dict[name] for name in models.keys()])\n",
    "train_buy_votes = np.sum(train_all_preds == 1, axis=0)\n",
    "train_all_agree = train_buy_votes == len(models)\n",
    "train_strong_buy = train_buy_votes >= 6\n",
    "train_majority_buy = train_buy_votes >= 5\n",
    "\n",
    "train_confidence = train_buy_prob.copy()\n",
    "train_confidence[train_all_agree] = np.minimum(train_confidence[train_all_agree] + 7, 100)\n",
    "train_confidence[train_strong_buy & ~train_all_agree] = np.minimum(train_confidence[train_strong_buy & ~train_all_agree] + 4, 100)\n",
    "train_confidence[train_majority_buy & ~train_strong_buy] = np.minimum(train_confidence[train_majority_buy & ~train_strong_buy] + 2, 100)\n",
    "\n",
    "print(f\"\\n{'Threshold':>10} | {'Train Sig':>10} | {'Train Acc':>10} | {'Test Sig':>10} | {'Test Acc':>10} | {'Diff':>10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for conf in [70, 75, 80, 85, 90]:\n",
    "    # Train\n",
    "    train_mask = train_confidence >= conf\n",
    "    train_sig = train_mask.sum()\n",
    "    train_acc = y_train[train_mask].mean() * 100 if train_sig > 0 else 0\n",
    "    \n",
    "    # Test\n",
    "    test_mask = confidence >= conf\n",
    "    test_sig = test_mask.sum()\n",
    "    test_acc = y_test[test_mask].mean() * 100 if test_sig > 0 else 0\n",
    "    \n",
    "    diff = train_acc - test_acc\n",
    "    \n",
    "    print(f\"{conf:>8}%+ | {train_sig:>10} | {train_acc:>9.1f}% | {test_sig:>10} | {test_acc:>9.1f}% | {diff:>+9.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211d2358",
   "metadata": {},
   "source": [
    "## 13. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdcf1a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ“‹ V10 FINAL SUMMARY\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'v10_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Find best config\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m reliable_results = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mv10_results\u001b[49m.items() \u001b[38;5;28;01mif\u001b[39;00m v[\u001b[33m'\u001b[39m\u001b[33msignals\u001b[39m\u001b[33m'\u001b[39m] >= \u001b[32m50\u001b[39m}\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reliable_results:\n\u001b[32m      8\u001b[39m     best_reliable = \u001b[38;5;28mmax\u001b[39m(reliable_results.items(), key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m][\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'v10_results' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ“‹ V10 FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best config\n",
    "reliable_results = {k: v for k, v in v10_results.items() if v['signals'] >= 50}\n",
    "if reliable_results:\n",
    "    best_reliable = max(reliable_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "    print(f\"\\nðŸ† BEST RELIABLE: {best_reliable[0]}% threshold\")\n",
    "    print(f\"   Accuracy: {best_reliable[1]['accuracy']:.1f}%\")\n",
    "    print(f\"   Signals: {best_reliable[1]['signals']}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ V10 CONFIGURATIONS:\")\n",
    "for conf in [70, 75, 80, 85, 90]:\n",
    "    if conf in v10_results and v10_results[conf]['signals'] > 0:\n",
    "        r = v10_results[conf]\n",
    "        beat_v8 = \"âœ“\" if r['accuracy'] >= 67.6 else \"\"\n",
    "        print(f\"   {conf}%+: {r['accuracy']:.1f}% acc, {r['signals']} signals {beat_v8}\")\n",
    "\n",
    "print(\"\\nðŸ“Š V8 BENCHMARK:\")\n",
    "print(\"   75%+: 67.6% acc, 627 signals\")\n",
    "print(\"   85%+: 93.8% acc, 48 signals\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ V10 IMPROVEMENTS:\")\n",
    "print(\"   - 7 diverse models (vs V8's 5)\")\n",
    "print(\"   - Enhanced agreement bonus\")\n",
    "print(\"   - Additional pattern features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493a8acb",
   "metadata": {},
   "source": [
    "## 14. Save V10 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a88a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving V10 Models...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaving V10 Models...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Save base models\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodels\u001b[49m.items():\n\u001b[32m      5\u001b[39m     joblib.dump(model, MODEL_DIR / \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_v10.joblib\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Save preprocessing\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Saving V10 Models...\")\n",
    "\n",
    "# Save base models\n",
    "for name, model in models.items():\n",
    "    joblib.dump(model, MODEL_DIR / f'{name}_v10.joblib')\n",
    "\n",
    "# Save preprocessing\n",
    "joblib.dump(scaler, MODEL_DIR / 'scaler_v10.joblib')\n",
    "joblib.dump(selected_features, MODEL_DIR / 'feature_cols_v10.joblib')\n",
    "joblib.dump(weights, MODEL_DIR / 'weights_v10.joblib')\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    'version': 'v10',\n",
    "    'mode': 'BUY_vs_SELL_7_Ensemble',\n",
    "    'features': len(selected_features),\n",
    "    'models': list(models.keys()),\n",
    "    'weights': weights,\n",
    "    'accuracies': accuracies,\n",
    "    'best_threshold': best_reliable[0] if reliable_results else 75\n",
    "}\n",
    "joblib.dump(config, MODEL_DIR / 'config_v10.joblib')\n",
    "\n",
    "print(f\"âœ… V10 Models Saved to {MODEL_DIR}\")\n",
    "print(f\"   - {len(models)} Base Models\")\n",
    "print(f\"   - Scaler & Feature List\")\n",
    "print(f\"   - Weights & Config\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
