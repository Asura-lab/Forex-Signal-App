{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a80291a",
   "metadata": {},
   "source": [
    "# forex_signal_v27 — PF-first Top‑Down Multi‑Timeframe Ensemble (4H/1H → 15m → 1m)\n",
    "Энэ notebook нь PF (Profit Factor)‑ийг өсгөх зорилготой **шаталсан (gated) ensemble** систем.\n",
    "\n",
    "**Гол өөрчлөлт (v26 → v27):**\n",
    "- Validation дээр threshold‑ийг **жинхэнэ backtest engine**‑ээр (cost+next-open+worst-case) тааруулдаг\n",
    "- **Prob. calibration** (Platt scaling) → threshold тогтвортой\n",
    "- Macro gate нь rule‑based дээр нэмээд optional **RandomForest regime score** (soft gate)\n",
    "- Feature set: BB width, trend/volatility scaling (…/ATR), micro entry filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f49177e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(train_dir='../data/train', test_dir='../data/test', master_tf='15m', macro_tfs=('4h', '1h'), entry_tfs=('1m',), atr_period=14, tp_atr=2.0, sl_atr=1.5, max_hold_bars=16, initial_equity=10000.0, risk_per_trade=0.01, max_lots=3.0, spread_pips=1.2, slippage_pips=0.2, commission_per_lot_usd=0.0, pip_size=0.0001, lot_size=100000, usd_per_pip_per_lot=10.0, train_core_end='2021-12-31', val_start='2022-01-01', val_end='2023-12-31', min_trades_val=120, dd_limit=0.2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 0) Imports & Global Config =====\n",
    "import os, glob, re, math, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Paths\n",
    "    train_dir: str = \"../data/train\"\n",
    "    test_dir: str  = \"../data/test\"\n",
    "\n",
    "    # Master decision timeframe\n",
    "    master_tf: str = \"15m\"  # PF-first default: 15m\n",
    "\n",
    "    # Timeframes used\n",
    "    macro_tfs: tuple = (\"4h\", \"1h\")\n",
    "    entry_tfs: tuple = (\"1m\",)\n",
    "\n",
    "    # Label / triple-barrier on master TF\n",
    "    atr_period: int = 14\n",
    "    tp_atr: float = 2.0\n",
    "    sl_atr: float = 1.5\n",
    "    max_hold_bars: int = 16\n",
    "\n",
    "    # Risk / execution\n",
    "    initial_equity: float = 10_000.0\n",
    "    risk_per_trade: float = 0.01\n",
    "    max_lots: float = 3.0  # safety cap\n",
    "\n",
    "    # Costs (edit to your broker)\n",
    "    spread_pips: float = 1.2\n",
    "    slippage_pips: float = 0.2\n",
    "    commission_per_lot_usd: float = 0.0\n",
    "\n",
    "    # FX conventions (EURUSD)\n",
    "    pip_size: float = 0.0001\n",
    "    lot_size: float = 100_000\n",
    "    usd_per_pip_per_lot: float = 10.0  # EURUSD approx\n",
    "\n",
    "    # Validation split inside train\n",
    "    train_core_end: str = \"2021-12-31\"\n",
    "    val_start: str = \"2022-01-01\"\n",
    "    val_end: str = \"2023-12-31\"\n",
    "\n",
    "    # Threshold search\n",
    "    min_trades_val: int = 120\n",
    "    dd_limit: float = 0.20  # 20% max DD on validation\n",
    "\n",
    "CFG = Config()\n",
    "print(CFG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16422609",
   "metadata": {},
   "source": [
    "## 1) Data loading\n",
    "Файл нэршил өөр байж болох тул timeframe string (`1m`, `15m`, `1h`, `4h`)‑ээр хайж олно. CSV дотор дор хаяж: `timestamp, open, high, low, close` байх ёстой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceb28a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train master: (224382, 6) 2015-01-01 22:00:00 2023-12-29 21:45:00\n",
      "Test master : (49807, 6) 2024-01-01 22:00:00 2025-12-30 23:45:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 1) Data Loading Helpers =====\n",
    "REQUIRED_OHLC = [\"timestamp\",\"open\",\"high\",\"low\",\"close\"]\n",
    "\n",
    "def _guess_tf_from_filename(fn: str) -> str:\n",
    "    s = fn.lower()\n",
    "    # map \"m15\" -> \"15m\", \"h1\" -> \"1h\"\n",
    "    # standard\n",
    "    for tf in [\"1m\",\"5m\",\"15m\",\"30m\",\"1h\",\"4h\",\"1d\"]:\n",
    "        if re.search(rf\"(?:_|-|\\b){tf}(?:_|-|\\b)\", s):\n",
    "            return tf\n",
    "            \n",
    "    # reversed m15, h4 etc.\n",
    "    match = re.search(r\"(?:_|-|\\b)([mh])(\\d+)(?:_|-|\\b|\\.)\", s)\n",
    "    if match:\n",
    "        unit, val = match.groups()\n",
    "        return f\"{val}{unit}\"\n",
    "    match = re.search(r\"(?:_|-|\\b)(\\d+)([mh])(?:_|-|\\b|\\.)\", s)\n",
    "    if match:\n",
    "        val, unit = match.groups()\n",
    "        return f\"{val}{unit}\"\n",
    "\n",
    "    # fallback\n",
    "    for tf in [\"1m\",\"5m\",\"15m\",\"30m\",\"1h\",\"4h\",\"1d\"]:\n",
    "        if tf in s:\n",
    "            return tf\n",
    "    return \"unknown\"\n",
    "\n",
    "def load_tf_csv(folder: str, tf: str) -> pd.DataFrame:\n",
    "    files = glob.glob(os.path.join(folder, \"*.csv\"))\n",
    "    # first pass: just strict match\n",
    "    cand = [f for f in files if tf in f.lower()]\n",
    "    if not cand:\n",
    "        # try by guessing tf from filename\n",
    "        cand = [f for f in files if _guess_tf_from_filename(os.path.basename(f)) == tf]\n",
    "    if not cand:\n",
    "        raise FileNotFoundError(f\"No CSV for tf={tf} in {folder}. Found files={len(files)}\")\n",
    "    # pick the largest file (often the real one)\n",
    "    cand = sorted(cand, key=lambda p: os.path.getsize(p), reverse=True)\n",
    "    path = cand[0]\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # normalize columns\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    \n",
    "    # flexible renaming\n",
    "    aliases = {\n",
    "        \"timestamp\": [\"time\", \"date\", \"datetime\"],\n",
    "        \"open\": [\"o\"],\n",
    "        \"high\": [\"h\"],\n",
    "        \"low\": [\"l\"],\n",
    "        \"close\": [\"c\"]\n",
    "    }\n",
    "    \n",
    "    for std, vars_ in aliases.items():\n",
    "        if std not in df.columns:\n",
    "            for v in vars_:\n",
    "                if v in df.columns:\n",
    "                    df.rename(columns={v: std}, inplace=True)\n",
    "                    break\n",
    "                    \n",
    "    for c in REQUIRED_OHLC:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"Missing column {c} in {path}. Columns={df.columns.tolist()[:20]}\")\n",
    "            \n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=False)\n",
    "    df = df.sort_values(\"timestamp\").drop_duplicates(\"timestamp\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "train_master = load_tf_csv(CFG.train_dir, CFG.master_tf)\n",
    "test_master  = load_tf_csv(CFG.test_dir,  CFG.master_tf)\n",
    "\n",
    "print(\"Train master:\", train_master.shape, train_master[\"timestamp\"].min(), train_master[\"timestamp\"].max())\n",
    "print(\"Test master :\", test_master.shape,  test_master[\"timestamp\"].min(),  test_master[\"timestamp\"].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd49c02",
   "metadata": {},
   "source": [
    "## 2) Technical indicators\n",
    "PF‑д хэрэгтэй цөөн, давхцахгүй indicator багц:\n",
    "- Trend: EMA50/EMA200 + slope\n",
    "- Strength: ADX14\n",
    "- Volatility: ATR14, BB width\n",
    "- Momentum: RSI14, MACD hist\n",
    "\n",
    "Бүх зай/диапазон төрлийн feature‑үүдийг **ATR‑аар нормализац** хийнэ (…/ATR) → volatility regime өөрчлөгдөхөд тогтвортой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38c27045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== 2) Indicators =====\n",
    "def ema(s: pd.Series, span: int) -> pd.Series:\n",
    "    return s.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "def rsi(close: pd.Series, period: int = 14) -> pd.Series:\n",
    "    delta = close.diff()\n",
    "    up = delta.clip(lower=0).ewm(alpha=1/period, adjust=False).mean()\n",
    "    down = (-delta.clip(upper=0)).ewm(alpha=1/period, adjust=False).mean()\n",
    "    rs = up / (down + 1e-12)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def true_range(df: pd.DataFrame) -> pd.Series:\n",
    "    prev_close = df[\"close\"].shift(1)\n",
    "    tr = pd.concat([\n",
    "        (df[\"high\"] - df[\"low\"]),\n",
    "        (df[\"high\"] - prev_close).abs(),\n",
    "        (df[\"low\"] - prev_close).abs()\n",
    "    ], axis=1).max(axis=1)\n",
    "    return tr\n",
    "\n",
    "def atr(df: pd.DataFrame, period: int = 14) -> pd.Series:\n",
    "    return true_range(df).ewm(alpha=1/period, adjust=False).mean()\n",
    "\n",
    "def adx(df: pd.DataFrame, period: int = 14) -> pd.Series:\n",
    "    high, low, close = df[\"high\"], df[\"low\"], df[\"close\"]\n",
    "    plus_dm = (high.diff()).clip(lower=0)\n",
    "    minus_dm = (-low.diff()).clip(lower=0)\n",
    "    tr = true_range(df)\n",
    "    atr_ = tr.ewm(alpha=1/period, adjust=False).mean()\n",
    "    plus_di = 100 * (plus_dm.ewm(alpha=1/period, adjust=False).mean() / (atr_ + 1e-12))\n",
    "    minus_di = 100 * (minus_dm.ewm(alpha=1/period, adjust=False).mean() / (atr_ + 1e-12))\n",
    "    dx = (100 * (plus_di - minus_di).abs() / (plus_di + minus_di + 1e-12))\n",
    "    return dx.ewm(alpha=1/period, adjust=False).mean()\n",
    "\n",
    "def macd_hist(close: pd.Series, fast=12, slow=26, signal=9) -> pd.Series:\n",
    "    macd_line = ema(close, fast) - ema(close, slow)\n",
    "    sig = ema(macd_line, signal)\n",
    "    return macd_line - sig\n",
    "\n",
    "def bb_width(close: pd.Series, period=20, nstd=2.0) -> pd.Series:\n",
    "    ma = close.rolling(period).mean()\n",
    "    sd = close.rolling(period).std()\n",
    "    upper = ma + nstd*sd\n",
    "    lower = ma - nstd*sd\n",
    "    width = (upper - lower) / (ma.abs() + 1e-12)\n",
    "    return width\n",
    "\n",
    "def add_tf_features(df: pd.DataFrame, prefix: str) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[f\"{prefix}_atr14\"] = atr(out, CFG.atr_period)\n",
    "    out[f\"{prefix}_ema50\"] = ema(out[\"close\"], 50)\n",
    "    out[f\"{prefix}_ema200\"] = ema(out[\"close\"], 200)\n",
    "    out[f\"{prefix}_ema200_slope\"] = out[f\"{prefix}_ema200\"].diff(5)\n",
    "    out[f\"{prefix}_rsi14\"] = rsi(out[\"close\"], 14)\n",
    "    out[f\"{prefix}_macdh\"] = macd_hist(out[\"close\"])\n",
    "    out[f\"{prefix}_adx14\"] = adx(out, 14)\n",
    "    out[f\"{prefix}_bbw\"] = bb_width(out[\"close\"], 20, 2.0)\n",
    "    # Normalize distances by ATR\n",
    "    out[f\"{prefix}_dist_ema200_atr\"] = (out[\"close\"] - out[f\"{prefix}_ema200\"]) / (out[f\"{prefix}_atr14\"] + 1e-12)\n",
    "    out[f\"{prefix}_dist_ema50_atr\"]  = (out[\"close\"] - out[f\"{prefix}_ema50\"]) / (out[f\"{prefix}_atr14\"] + 1e-12)\n",
    "    # candle anatomy\n",
    "    out[f\"{prefix}_range_atr\"] = (out[\"high\"] - out[\"low\"]) / (out[f\"{prefix}_atr14\"] + 1e-12)\n",
    "    out[f\"{prefix}_body_atr\"]  = (out[\"close\"] - out[\"open\"]).abs() / (out[f\"{prefix}_atr14\"] + 1e-12)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c999f9",
   "metadata": {},
   "source": [
    "## 3) Build multi-timeframe feature table on master TF\n",
    "- 4H/1H: merge_asof backward (зөвхөн өмнө хаагдсан лаа)\n",
    "- 1m: 15 минутын цонхоор aggregation (mean/max/std/last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39f03248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224382, 50) (49807, 50)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 3) Multi-TF Alignment =====\n",
    "def merge_asof_backward(master: pd.DataFrame, other: pd.DataFrame, other_tf: str, prefix: str) -> pd.DataFrame:\n",
    "    # ensure other has features\n",
    "    if prefix not in [c.split('_')[0] for c in other.columns]: # check if already added\n",
    "        o = add_tf_features(other, prefix)\n",
    "    else:\n",
    "        o = other.copy()\n",
    "    \n",
    "    cols_to_use = [\"timestamp\"] + [c for c in o.columns if c.startswith(prefix+\"_\")]\n",
    "    o = o[cols_to_use].drop_duplicates(\"timestamp\").sort_values(\"timestamp\")\n",
    "    \n",
    "    m = master.sort_values(\"timestamp\")\n",
    "    merged = pd.merge_asof(m, o, on=\"timestamp\", direction=\"backward\")\n",
    "    return merged\n",
    "\n",
    "def agg_entry_1m_to_master(master: pd.DataFrame, one_m: pd.DataFrame, window_minutes: int = 15) -> pd.DataFrame:\n",
    "    # Optimized Vectorized Implementation\n",
    "    df = one_m.copy()\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    df = df.set_index(\"timestamp\").sort_index()\n",
    "    \n",
    "    # 1. Pre-calc 1m features\n",
    "    df[\"ret1\"] = np.log(df[\"close\"]).diff()\n",
    "    df[\"rng\"] = (df[\"high\"] - df[\"low\"])\n",
    "    df[\"body\"] = (df[\"close\"] - df[\"open\"]).abs()\n",
    "    df[\"wick_down\"] = (df[[\"open\",\"close\"]].min(axis=1) - df[\"low\"]).clip(lower=0)\n",
    "    \n",
    "    # 2. Rolling aggregation on 1m grid\n",
    "    # \"15min\" window, closed='right' means for row at 10:15, include (10:00, 10:15]\n",
    "    r = df.rolling(window=f\"{window_minutes}min\", min_periods=5, closed='right')\n",
    "    \n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    out[\"1m_ret_mean\"] = r[\"ret1\"].mean()\n",
    "    out[\"1m_ret_std\"]  = r[\"ret1\"].std()\n",
    "    out[\"1m_ret_sum\"]  = r[\"ret1\"].sum()\n",
    "    out[\"1m_rng_mean\"] = r[\"rng\"].mean()\n",
    "    out[\"1m_rng_max\"]  = r[\"rng\"].max()\n",
    "    out[\"1m_body_mean\"] = r[\"body\"].mean()\n",
    "    out[\"1m_wick_down_mean\"] = r[\"wick_down\"].mean()\n",
    "    \n",
    "    # net_move: approximation. Original was last_close - first_close in window.\n",
    "    # We can approximate with sum of diffs (close - prev_close).\n",
    "    # Since ret1 is log diff, sum(ret1) is log total return. \n",
    "    # Let's just use ret_sum as the feature for \"net move\" magnitude/direction.\n",
    "    # It carries similar info.\n",
    "    out[\"1m_net_move\"] = out[\"1m_ret_sum\"]\n",
    "    \n",
    "    out = out.reset_index()\n",
    "    \n",
    "    # 3. Join to master\n",
    "    # master row at T gets 1m rolling stats at T (which cover T-15m..T)\n",
    "    master = master.sort_values(\"timestamp\")\n",
    "    merged = pd.merge_asof(master, out, on=\"timestamp\", direction=\"backward\", tolerance=pd.Timedelta(\"15m\")) # tolerance?\n",
    "    return merged\n",
    "\n",
    "def build_master_table(folder: str) -> pd.DataFrame:\n",
    "    master = load_tf_csv(folder, CFG.master_tf)\n",
    "    master = add_tf_features(master, \"m\")\n",
    "    # macro merges\n",
    "    for tf in CFG.macro_tfs:\n",
    "        other = load_tf_csv(folder, tf)\n",
    "        # Note: assuming files exist, if load_tf_csv fails it raises error\n",
    "        master = merge_asof_backward(master, other, tf, tf)\n",
    "    # entry 1m agg\n",
    "    try:\n",
    "        one_m = load_tf_csv(folder, \"1m\")\n",
    "        master = agg_entry_1m_to_master(master, one_m, window_minutes=15)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: 1m data not found in {folder}. skipping micro features.\")\n",
    "        # fill micro cols with nan\n",
    "        for c in [\"1m_ret_mean\",\"1m_ret_std\",\"1m_ret_sum\",\"1m_rng_mean\",\"1m_rng_max\",\"1m_body_mean\",\"1m_wick_down_mean\",\"1m_net_move\"]:\n",
    "            master[c] = np.nan\n",
    "            \n",
    "    return master\n",
    "\n",
    "train_df = build_master_table(CFG.train_dir)\n",
    "test_df  = build_master_table(CFG.test_dir)\n",
    "\n",
    "print(train_df.shape, test_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925dc2aa",
   "metadata": {},
   "source": [
    "## 4) Macro gate + optional RF regime score\n",
    "Rule-based gate PF өсгөхөд хамгийн үр дүнтэй. Үүнийг soft score-оор нэмэгдүүлж болно.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8271e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== 4) Macro Gate =====\n",
    "def macro_gate(row) -> int:\n",
    "    ok = True\n",
    "    # 4h: price above EMA200, positive slope, trend strength\n",
    "    ok &= (row[\"4h_dist_ema200_atr\"] > 0)\n",
    "    ok &= (row[\"4h_ema200_slope\"] > 0)\n",
    "    ok &= (row[\"4h_adx14\"] > 18)\n",
    "    # 1h confirmation\n",
    "    ok &= (row[\"1h_dist_ema200_atr\"] > -0.25)   # allow mild pullback\n",
    "    ok &= (row[\"1h_adx14\"] > 16)\n",
    "    # volatility sanity (avoid extreme)\n",
    "    ok &= (row[\"1h_bbw\"] > 0.002)               # avoid dead market\n",
    "    ok &= (row[\"1h_bbw\"] < 0.08)                # avoid explosive chaos\n",
    "    return int(ok)\n",
    "\n",
    "for df in (train_df, test_df):\n",
    "    df[\"macro_ok\"] = df.apply(macro_gate, axis=1)\n",
    "\n",
    "# Optional: RF regime score trained on TRAIN only (no test leakage)\n",
    "macro_features = [\n",
    "    \"4h_dist_ema200_atr\",\"4h_ema200_slope\",\"4h_adx14\",\"4h_bbw\",\"4h_range_atr\",\n",
    "    \"1h_dist_ema200_atr\",\"1h_ema200_slope\",\"1h_adx14\",\"1h_bbw\",\"1h_range_atr\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07abad61",
   "metadata": {},
   "source": [
    "## 5) Triple-barrier labels on master TF (15m)\n",
    "Decision: close(t)\n",
    "Entry: open(t+1)\n",
    "Barrier evaluation uses future OHLC, но label нь зөвхөн training/validation дээр ашиглагдана.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41e5ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== 5) Triple-barrier label on master TF =====\n",
    "def make_triple_barrier_label(df: pd.DataFrame) -> pd.Series:\n",
    "    atr_ = df[\"m_atr14\"].values\n",
    "    tp = df[\"close\"].values + CFG.tp_atr * atr_\n",
    "    sl = df[\"close\"].values - CFG.sl_atr * atr_\n",
    "    y = np.full(len(df), np.nan)\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tp_i = tp[i]; sl_i = sl[i]\n",
    "        # start checking from next bar\n",
    "        end = min(len(df)-1, i + CFG.max_hold_bars)\n",
    "        hit = None\n",
    "        for j in range(i+1, end+1):\n",
    "            hi = df.loc[j, \"high\"]; lo = df.loc[j, \"low\"]\n",
    "            hit_sl = lo <= sl_i\n",
    "            hit_tp = hi >= tp_i\n",
    "            if hit_sl or hit_tp:\n",
    "                # worst-case: if both same bar, count as loss\n",
    "                if hit_sl and hit_tp:\n",
    "                    hit = 0\n",
    "                else:\n",
    "                    hit = 1 if hit_tp else 0\n",
    "                break\n",
    "        if hit is None:\n",
    "            hit = 0\n",
    "        y[i] = hit\n",
    "    return pd.Series(y, index=df.index, name=\"y_edge\")\n",
    "\n",
    "train_df[\"y_edge\"] = make_triple_barrier_label(train_df)\n",
    "# test_df label not used for training; keep None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d6359",
   "metadata": {},
   "source": [
    "## 6) Train/Validation split (time-based)\n",
    "Train-core: 2015–2021\n",
    "Validation: 2022–2023\n",
    "Final test: 2024–2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61cb0868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_core: (174436, 57) 2015-01-01 22:00:00 2021-12-31 00:00:00\n",
      "val       : (49859, 57) 2022-01-02 22:00:00 2023-12-29 21:45:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 6) Split (and pre-calc return features) =====\n",
    "# Calculate returns BEFORE splitting so they exist in train_core/val\n",
    "train_df[\"m_ret1\"] = np.log(train_df[\"close\"]).diff()\n",
    "train_df[\"m_ret3\"] = np.log(train_df[\"close\"]).diff(3)\n",
    "train_df[\"m_ret6\"] = np.log(train_df[\"close\"]).diff(6)\n",
    "train_df[\"m_ret12\"]= np.log(train_df[\"close\"]).diff(12)\n",
    "\n",
    "test_df[\"m_ret1\"] = np.log(test_df[\"close\"]).diff()\n",
    "test_df[\"m_ret3\"] = np.log(test_df[\"close\"]).diff(3)\n",
    "test_df[\"m_ret6\"] = np.log(test_df[\"close\"]).diff(6)\n",
    "test_df[\"m_ret12\"]= np.log(test_df[\"close\"]).diff(12)\n",
    "\n",
    "train_df = train_df.replace([np.inf,-np.inf], np.nan).ffill()\n",
    "test_df  = test_df.replace([np.inf,-np.inf], np.nan).ffill()\n",
    "\n",
    "train_core = train_df[train_df[\"timestamp\"] <= pd.to_datetime(CFG.train_core_end)]\n",
    "val = train_df[(train_df[\"timestamp\"] >= pd.to_datetime(CFG.val_start)) & (train_df[\"timestamp\"] <= pd.to_datetime(CFG.val_end))]\n",
    "\n",
    "print(\"train_core:\", train_core.shape, train_core[\"timestamp\"].min(), train_core[\"timestamp\"].max())\n",
    "print(\"val       :\", val.shape,        val[\"timestamp\"].min(),        val[\"timestamp\"].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f27d4d",
   "metadata": {},
   "source": [
    "## 7) Feature sets\n",
    "### Edge model (15m)\n",
    "- returns + RSI + MACD + ATR + BB width + distance-to-EMA features + macro context\n",
    "\n",
    "### Entry model (1m agg)\n",
    "- micro volatility spike + wick/structure aggregates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e0c1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== 7) Feature sets =====\n",
    "edge_features = [\n",
    "    # master tf\n",
    "    \"m_dist_ema200_atr\",\"m_dist_ema50_atr\",\"m_ema200_slope\",\"m_adx14\",\"m_rsi14\",\"m_macdh\",\"m_bbw\",\"m_range_atr\",\"m_body_atr\",\n",
    "    # simple returns\n",
    "    # (log returns on master)\n",
    "]\n",
    "# Returns already calculated in Split step\n",
    "\n",
    "edge_features += [\"m_ret1\",\"m_ret3\",\"m_ret6\",\"m_ret12\"]\n",
    "\n",
    "# macro context\n",
    "edge_features += [\n",
    "    \"4h_dist_ema200_atr\",\"4h_ema200_slope\",\"4h_adx14\",\"4h_bbw\",\"4h_range_atr\",\n",
    "    \"1h_dist_ema200_atr\",\"1h_ema200_slope\",\"1h_adx14\",\"1h_bbw\",\"1h_range_atr\",\n",
    "]\n",
    "\n",
    "entry_features = [\n",
    "    \"1m_ret_mean\",\"1m_ret_std\",\"1m_ret_sum\",\"1m_rng_mean\",\"1m_rng_max\",\"1m_body_mean\",\"1m_wick_down_mean\",\"1m_net_move\",\n",
    "    # include current master context to avoid silly entries\n",
    "    \"m_bbw\",\"m_range_atr\",\"m_dist_ema50_atr\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faf4206",
   "metadata": {},
   "source": [
    "## 8) Models\n",
    "- **Edge:** XGBoost + calibration (Platt)\n",
    "- **Entry:** LogisticRegression + calibration\n",
    "- **Optional regime score:** RandomForest (macro features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f5821c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge XGB CV AUC: 0.6438650508750831 +/- 0.029016944185844856\n",
      "Edge cal failed: The 'cv' parameter of CalibratedClassifierCV must be an int in the range [2, inf), an object implementing 'split' and 'get_n_splits', an iterable or None. Got 'prefit' instead.. Using manual.\n",
      "Entry cal failed: The 'cv' parameter of CalibratedClassifierCV must be an int in the range [2, inf), an object implementing 'split' and 'get_n_splits', an iterable or None. Got 'prefit' instead.. Using manual.\n",
      "Models ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 8) Train models =====\n",
    "def _prep_xy(df: pd.DataFrame, feats: list, ycol: str):\n",
    "    d = df.copy()\n",
    "    d = d.dropna(subset=feats + [ycol, \"macro_ok\"])\n",
    "    X = d[feats].values\n",
    "    y = d[ycol].astype(int).values\n",
    "    return d, X, y\n",
    "\n",
    "# --- Optional RF regime score target: use y_edge but only when macro_ok==1 (learn \"good regime\")\n",
    "rf_train = train_core.dropna(subset=macro_features + [\"y_edge\",\"macro_ok\"]).copy()\n",
    "rf_train = rf_train[rf_train[\"macro_ok\"]==1]\n",
    "Xr = rf_train[macro_features].values\n",
    "yr = rf_train[\"y_edge\"].astype(int).values\n",
    "\n",
    "rf_regime = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=40,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "if len(rf_train) > 5000:\n",
    "    rf_regime.fit(Xr, yr)\n",
    "    # regime_score = probability of \"good trade\" given macro state\n",
    "    train_df.loc[:, \"regime_score\"] = rf_regime.predict_proba(train_df[macro_features].fillna(method=\"ffill\").values)[:,1]\n",
    "    test_df.loc[:,  \"regime_score\"] = rf_regime.predict_proba(test_df[macro_features].fillna(method=\"ffill\").values)[:,1]\n",
    "else:\n",
    "    train_df[\"regime_score\"] = 1.0\n",
    "    test_df[\"regime_score\"]  = 1.0\n",
    "\n",
    "# --- Edge model (XGB) trained on macro_ok==1 only\n",
    "edge_train = train_core[(train_core[\"macro_ok\"]==1)].copy()\n",
    "edge_train, Xe, ye = _prep_xy(edge_train, edge_features, \"y_edge\")\n",
    "\n",
    "# imbalance\n",
    "pos = ye.sum()\n",
    "neg = len(ye)-pos\n",
    "scale_pos_weight = (neg/(pos+1e-9)) if pos>0 else 1.0\n",
    "\n",
    "edge_clf = xgb.XGBClassifier(\n",
    "    n_estimators=1200,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.02,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    min_child_weight=6,\n",
    "    reg_lambda=1.0,\n",
    "    gamma=0.0,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "# time-series CV-ish evaluation (optional)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "aucs=[]\n",
    "for tr, te in tscv.split(Xe):\n",
    "    edge_clf.fit(Xe[tr], ye[tr])\n",
    "    p = edge_clf.predict_proba(Xe[te])[:,1]\n",
    "    aucs.append(roc_auc_score(ye[te], p))\n",
    "print(\"Edge XGB CV AUC:\", np.mean(aucs), \"+/-\", np.std(aucs))\n",
    "\n",
    "# Fit final + calibrate on validation slice (from train_df)\n",
    "edge_clf.fit(Xe, ye)\n",
    "\n",
    "# Calibration using a held-out part of train (use 2022 as calibrator subset to avoid peeking too much)\n",
    "cal_df = train_df[(train_df[\"timestamp\"]>=pd.to_datetime(\"2022-01-01\")) & (train_df[\"timestamp\"]<=pd.to_datetime(\"2022-12-31\"))]\n",
    "cal_df = cal_df[(cal_df[\"macro_ok\"]==1)].dropna(subset=edge_features+[\"y_edge\"])\n",
    "Xcal = cal_df[edge_features].values\n",
    "ycal = cal_df[\"y_edge\"].astype(int).values\n",
    "\n",
    "class ManualCalibratedClassifier:\n",
    "    def __init__(self, base_estimator):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.lr = LogisticRegression(C=9999999)\n",
    "    def fit(self, X, y):\n",
    "        preds = self.base_estimator.predict_proba(X)[:, 1].reshape(-1, 1)\n",
    "        self.lr.fit(preds, y)\n",
    "    def predict_proba(self, X):\n",
    "        preds = self.base_estimator.predict_proba(X)[:, 1].reshape(-1, 1)\n",
    "        return self.lr.predict_proba(preds)\n",
    "\n",
    "try:\n",
    "    edge_cal = CalibratedClassifierCV(edge_clf, method=\"sigmoid\", cv=\"prefit\")\n",
    "    if len(cal_df) > 1000:\n",
    "        edge_cal.fit(Xcal, ycal)\n",
    "    else:\n",
    "        edge_cal = edge_clf\n",
    "except Exception as e:\n",
    "    print(f\"Edge cal failed: {e}. Using manual.\")\n",
    "    if len(cal_df) > 1000:\n",
    "        edge_cal = ManualCalibratedClassifier(edge_clf)\n",
    "        edge_cal.fit(Xcal, ycal)\n",
    "    else:\n",
    "        edge_cal = edge_clf\n",
    "\n",
    "# --- Entry model: label = avoid immediate adverse move proxy (on train_core)\n",
    "# Create a simple adverse-move label on master TF using next 3 bars low vs next-open entry.\n",
    "def make_entry_label(df: pd.DataFrame, lookahead_bars: int = 3, adverse_atr: float = 0.5) -> pd.Series:\n",
    "    atr_ = df[\"m_atr14\"].values\n",
    "    y = np.full(len(df), np.nan)\n",
    "    for i in range(len(df)-lookahead_bars-1):\n",
    "        entry = df.loc[i+1, \"open\"]  # next-open\n",
    "        future_low = df.loc[i+1:i+lookahead_bars, \"low\"].min()\n",
    "        adverse = (entry - future_low) / (atr_[i] + 1e-12)\n",
    "        y[i] = 1 if adverse < adverse_atr else 0   # 1 = clean entry (not too adverse)\n",
    "    return pd.Series(y, index=df.index, name=\"y_entry\")\n",
    "\n",
    "train_df[\"y_entry\"] = make_entry_label(train_df)\n",
    "# Need to update train_core entries or recreate\n",
    "# Since we modified train_df, let's just re-slice train_core logic for entry training\n",
    "# train_core was defined by timestamp.\n",
    "entry_train_df = train_df[train_df[\"timestamp\"] <= pd.to_datetime(CFG.train_core_end)]\n",
    "entry_train = entry_train_df[(entry_train_df[\"macro_ok\"]==1)].dropna(subset=entry_features+[\"y_entry\"]).copy()\n",
    "Xen = entry_train[entry_features].values\n",
    "yen = entry_train[\"y_entry\"].astype(int).values\n",
    "\n",
    "entry_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(max_iter=2000, C=1.0, class_weight=\"balanced\", random_state=SEED))\n",
    "])\n",
    "\n",
    "# calibrate entry probabilities too\n",
    "entry_pipe.fit(Xen, yen)\n",
    "\n",
    "# calibrate on 2022 subset\n",
    "cal2 = train_df[(train_df[\"timestamp\"]>=pd.to_datetime(\"2022-01-01\")) & (train_df[\"timestamp\"]<=pd.to_datetime(\"2022-12-31\"))]\n",
    "cal2 = cal2[(cal2[\"macro_ok\"]==1)].dropna(subset=entry_features+[\"y_entry\"])\n",
    "\n",
    "try:\n",
    "    entry_cal = CalibratedClassifierCV(entry_pipe, method=\"sigmoid\", cv=\"prefit\")\n",
    "    if len(cal2) > 1000:\n",
    "        entry_cal.fit(cal2[entry_features].values, cal2[\"y_entry\"].astype(int).values)\n",
    "    else:\n",
    "        entry_cal = entry_pipe\n",
    "except Exception as e:\n",
    "    print(f\"Entry cal failed: {e}. Using manual.\")\n",
    "    if len(cal2) > 1000:\n",
    "        entry_cal = ManualCalibratedClassifier(entry_pipe)\n",
    "        entry_cal.fit(cal2[entry_features].values, cal2[\"y_entry\"].astype(int).values)\n",
    "    else:\n",
    "        entry_cal = entry_pipe\n",
    "\n",
    "print(\"Models ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aeedab",
   "metadata": {},
   "source": [
    "## 9) Backtest engine (үнэн зөв)\n",
    "Дүрэм:\n",
    "- signal at close(t)\n",
    "- entry at open(t+1)\n",
    "- long entry uses ask (mid + spread/2 + slip)\n",
    "- exit uses bid (mid - spread/2 - slip)\n",
    "- TP/SL нэг лаанд зэрэг хүрвэл worst-case → SL гэж үзнэ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70a9977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== 9) Backtest Engine =====\n",
    "@dataclass\n",
    "class Trade:\n",
    "    entry_time: pd.Timestamp\n",
    "    exit_time: pd.Timestamp\n",
    "    entry_price: float\n",
    "    exit_price: float\n",
    "    sl: float\n",
    "    tp: float\n",
    "    lots: float\n",
    "    pnl_usd: float\n",
    "    pnl_pips: float\n",
    "    outcome: str\n",
    "\n",
    "def pip_to_price(pips: float) -> float:\n",
    "    return pips * CFG.pip_size\n",
    "\n",
    "def price_to_pips(delta_price: float) -> float:\n",
    "    return delta_price / CFG.pip_size\n",
    "\n",
    "def calc_lots_for_risk(equity: float, stop_pips: float) -> float:\n",
    "    if stop_pips <= 0:\n",
    "        return 0.0\n",
    "    risk_usd = equity * CFG.risk_per_trade\n",
    "    lots = risk_usd / (stop_pips * CFG.usd_per_pip_per_lot)\n",
    "    return float(max(0.0, min(lots, CFG.max_lots)))\n",
    "\n",
    "def run_backtest(df: pd.DataFrame, signal: np.ndarray) -> tuple[list, pd.DataFrame]:\n",
    "    spread = pip_to_price(CFG.spread_pips)\n",
    "    slip = pip_to_price(CFG.slippage_pips)\n",
    "\n",
    "    equity = CFG.initial_equity\n",
    "    eq = []\n",
    "    trades = []\n",
    "\n",
    "    in_pos = False\n",
    "    entry_i = None\n",
    "    entry_price = sl = tp = lots = None\n",
    "\n",
    "    for i in range(len(df)-1):\n",
    "        t = df.loc[i, \"timestamp\"]\n",
    "        eq.append((t, equity))\n",
    "\n",
    "        if not in_pos:\n",
    "            if signal[i] == 1:\n",
    "                # decide at close(i), enter at open(i+1) using ask\n",
    "                entry_mid = df.loc[i+1, \"open\"]\n",
    "                entry_price = entry_mid + spread/2 + slip\n",
    "                # stop/tp based on ATR at decision bar i\n",
    "                atr_i = df.loc[i, \"m_atr14\"]\n",
    "                sl = entry_mid - CFG.sl_atr * atr_i\n",
    "                tp = entry_mid + CFG.tp_atr * atr_i\n",
    "                stop_pips = price_to_pips(entry_price - (sl + spread/2 + slip))  # worst-ish\n",
    "                lots = calc_lots_for_risk(equity, stop_pips)\n",
    "                if lots > 0:\n",
    "                    in_pos = True\n",
    "                    entry_i = i+1\n",
    "        else:\n",
    "            hi = df.loc[i, \"high\"]\n",
    "            lo = df.loc[i, \"low\"]\n",
    "\n",
    "            hit_sl = lo <= sl\n",
    "            hit_tp = hi >= tp\n",
    "\n",
    "            if hit_sl or hit_tp:\n",
    "                if hit_sl and hit_tp:\n",
    "                    exit_mid = sl\n",
    "                    outcome = \"SL(ambiguous)\"\n",
    "                elif hit_sl:\n",
    "                    exit_mid = sl\n",
    "                    outcome = \"SL\"\n",
    "                else:\n",
    "                    exit_mid = tp\n",
    "                    outcome = \"TP\"\n",
    "\n",
    "                # long exit uses bid\n",
    "                exit_price = exit_mid - spread/2 - slip\n",
    "                pnl_price = exit_price - entry_price\n",
    "                pnl_pips = price_to_pips(pnl_price)\n",
    "                pnl_usd = pnl_pips * CFG.usd_per_pip_per_lot * lots\n",
    "                pnl_usd -= CFG.commission_per_lot_usd * lots\n",
    "\n",
    "                equity += pnl_usd\n",
    "                trades.append(Trade(\n",
    "                    entry_time=df.loc[entry_i, \"timestamp\"],\n",
    "                    exit_time=t,\n",
    "                    entry_price=float(entry_price),\n",
    "                    exit_price=float(exit_price),\n",
    "                    sl=float(sl), tp=float(tp),\n",
    "                    lots=float(lots),\n",
    "                    pnl_usd=float(pnl_usd),\n",
    "                    pnl_pips=float(pnl_pips),\n",
    "                    outcome=outcome\n",
    "                ))\n",
    "                in_pos = False\n",
    "                entry_i = None\n",
    "\n",
    "    eq_df = pd.DataFrame(eq, columns=[\"timestamp\",\"equity\"])\n",
    "    eq_df[\"peak\"] = eq_df[\"equity\"].cummax()\n",
    "    eq_df[\"dd\"] = (eq_df[\"equity\"] - eq_df[\"peak\"]) / eq_df[\"peak\"]\n",
    "    return trades, eq_df\n",
    "\n",
    "def summarize(trades: list[Trade], eq_df: pd.DataFrame) -> dict:\n",
    "    if not trades:\n",
    "        return {\"trades\":0, \"profit_factor\":0.0, \"winrate\":0.0, \"net_profit_usd\":0.0, \"max_drawdown\":0.0}\n",
    "    pnl = np.array([t.pnl_usd for t in trades])\n",
    "    gp = pnl[pnl>0].sum()\n",
    "    gl = -pnl[pnl<0].sum()\n",
    "    pf = gp/gl if gl>0 else float(\"inf\")\n",
    "    return {\n",
    "        \"trades\": int(len(trades)),\n",
    "        \"profit_factor\": float(pf),\n",
    "        \"winrate\": float((pnl>0).mean()),\n",
    "        \"net_profit_usd\": float(pnl.sum()),\n",
    "        \"max_drawdown\": float(eq_df[\"dd\"].min()) if len(eq_df) else 0.0,\n",
    "        \"avg_trade_usd\": float(pnl.mean()),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5d60b0",
   "metadata": {},
   "source": [
    "## 10) Threshold tuning on validation (жинхэнэ backtest)\n",
    "Энд PF‑ийг *жинхэнэ engine*-ээр maximize хийнэ. Шүүлт:\n",
    "- trades >= `min_trades_val`\n",
    "- maxDD <= 20%\n",
    "\n",
    "Дараа нь хамгийн сайн 1–3 тохиргооноос сонгоод TEST дээр нэг л удаа үнэлнэ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82574414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates: 0\n",
      "BEST: None\n",
      "Using thresholds: 0.65 0.58 0.55\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 10) Validation threshold tuning with real backtest =====\n",
    "def compute_probs(df: pd.DataFrame):\n",
    "    d = df.copy()\n",
    "    # edge probs only meaningful when macro_ok=1; else set 0\n",
    "    mask = (d[\"macro_ok\"].values==1)\n",
    "    p_edge = np.zeros(len(d))\n",
    "    p_entry = np.zeros(len(d))\n",
    "    if mask.sum() > 0:\n",
    "        xe = d.loc[mask, edge_features].fillna(method=\"ffill\").values\n",
    "        xn = d.loc[mask, entry_features].fillna(method=\"ffill\").values\n",
    "        pe = edge_cal.predict_proba(xe)[:,1]\n",
    "        pn = entry_cal.predict_proba(xn)[:,1]\n",
    "        p_edge[mask] = pe\n",
    "        p_entry[mask] = pn\n",
    "    # regime score as soft gate (0..1)\n",
    "    if \"regime_score\" in d.columns:\n",
    "        rs = d[\"regime_score\"].fillna(1.0).values\n",
    "    else:\n",
    "        rs = np.ones(len(d))\n",
    "    return p_edge, p_entry, rs\n",
    "\n",
    "val_use = val.copy().reset_index(drop=True)\n",
    "pve, pvn, rsv = compute_probs(val_use)\n",
    "\n",
    "def make_signal(p_edge, p_entry, regime_score, edge_th, entry_th, regime_th):\n",
    "    sig = (val_use[\"macro_ok\"].values==1) & (p_edge>edge_th) & (p_entry>entry_th) & (regime_score>regime_th)\n",
    "    return sig.astype(int)\n",
    "\n",
    "edge_grid  = np.round(np.linspace(0.58, 0.78, 11), 2)\n",
    "entry_grid = np.round(np.linspace(0.52, 0.72, 11), 2)\n",
    "reg_grid   = np.round(np.linspace(0.45, 0.75, 7), 2)\n",
    "\n",
    "best = None\n",
    "results=[]\n",
    "for eth in edge_grid:\n",
    "    for ith in entry_grid:\n",
    "        for rth in reg_grid:\n",
    "            sig = make_signal(pve, pvn, rsv, eth, ith, rth)\n",
    "            trades, eq = run_backtest(val_use, sig)\n",
    "            s = summarize(trades, eq)\n",
    "            if s[\"trades\"] < CFG.min_trades_val:\n",
    "                continue\n",
    "            if abs(s[\"max_drawdown\"]) > CFG.dd_limit:\n",
    "                continue\n",
    "            results.append((eth, ith, rth, s[\"profit_factor\"], s[\"trades\"], s[\"net_profit_usd\"], s[\"max_drawdown\"]))\n",
    "            if (best is None) or (s[\"profit_factor\"] > best[3]):\n",
    "                best = (eth, ith, rth, s[\"profit_factor\"], s[\"trades\"], s[\"net_profit_usd\"], s[\"max_drawdown\"])\n",
    "\n",
    "res_df = pd.DataFrame(results, columns=[\"edge_th\",\"entry_th\",\"regime_th\",\"pf\",\"trades\",\"net_usd\",\"max_dd\"])\n",
    "print(\"Candidates:\", len(res_df))\n",
    "if len(res_df):\n",
    "    print(\"Top 10 by PF:\")\n",
    "    display(res_df.sort_values(\"pf\", ascending=False).head(10))\n",
    "print(\"BEST:\", best)\n",
    "\n",
    "EDGE_TH, ENTRY_TH, REG_TH = (best[0], best[1], best[2]) if best else (0.65, 0.58, 0.55)\n",
    "print(\"Using thresholds:\", EDGE_TH, ENTRY_TH, REG_TH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8519f239",
   "metadata": {},
   "source": [
    "## 11) Final backtest on TEST (2024–2025)\n",
    "TEST дээр threshold‑ийг дахиж тааруулахгүй. Зөвхөн нэг удаа үнэл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf504b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST summary: {'trades': 0, 'profit_factor': 0.0, 'winrate': 0.0, 'net_profit_usd': 0.0, 'max_drawdown': 0.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 11) Final TEST backtest =====\n",
    "test_use = test_df.copy().reset_index(drop=True)\n",
    "pte, ptn, rst = compute_probs(test_use)\n",
    "\n",
    "sig_test = ((test_use[\"macro_ok\"].values==1) & (pte>EDGE_TH) & (ptn>ENTRY_TH) & (rst>REG_TH)).astype(int)\n",
    "trades, eq = run_backtest(test_use, sig_test)\n",
    "summary = summarize(trades, eq)\n",
    "print(\"TEST summary:\", summary)\n",
    "\n",
    "trades_df = pd.DataFrame([t.__dict__ for t in trades]) if trades else pd.DataFrame()\n",
    "if len(trades_df):\n",
    "    trades_df[\"year\"] = pd.to_datetime(trades_df[\"exit_time\"]).dt.year\n",
    "    print(\"\\nTrades per year:\")\n",
    "    print(trades_df.groupby(\"year\")[\"pnl_usd\"].agg([\"count\",\"sum\",\"mean\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c9419",
   "metadata": {},
   "source": [
    "## 12) Next steps to raise PF further\n",
    "Хэрвээ PF бага байвал хамгийн үр дүнтэй 3 knob:\n",
    "1) **EDGE_TH өсгөх** (selectivity ↑ → PF ↑, trades ↓)\n",
    "2) Macro gate‑ийг чангаруулах (ADX/BB width/EMA slope)\n",
    "3) TP/SL харьцаа (TP=2.2×ATR, SL=1.4×ATR гэх мэт) — validation дээр л тааруул\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
