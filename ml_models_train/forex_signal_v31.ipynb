{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e69e00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1) Imports & Configuration (Improved for PF > 3) =====\n",
    "import os, glob, math, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, precision_score\n",
    "\n",
    "# Try import XGBoost\n",
    "HAS_XGB = False\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class CFG:\n",
    "    # Adjust paths as needed\n",
    "    train_dir = \"../data/train\"\n",
    "    test_dir  = \"../data/test\"\n",
    "    pair_hint = \"EURUSD\"\n",
    "\n",
    "    # Execution Costs\n",
    "    spread_pips: float = 1.0\n",
    "    slippage_pips: float = 0.2\n",
    "    commission_per_lot_usd: float = 0.0\n",
    "    risk_per_trade: float = 0.01\n",
    "\n",
    "    # Strategy Goals: PF > 3.0 requires High Reward:Risk or Ultra High Winrate.\n",
    "    # We choose High Reward:Risk (2:1) + Moderate Winrate (>50%).\n",
    "    # Target: Win 55-60% with 2:1 Reward => PF ~ 3.0\n",
    "    \n",
    "    tp_atr: float = 3.0   # Large Target (Trend following)\n",
    "    sl_atr: float = 1.5   # Tighter Stop\n",
    "    max_hold_bars: int = 24  # 6 hours on 15m (give trade room to breathe)\n",
    "    \n",
    "    # Filters\n",
    "    min_volatility_pips: float = 5.0 # Skip dead markets\n",
    "    \n",
    "    # Split\n",
    "    core_end = \"2021-12-31\"\n",
    "    val_end  = \"2023-12-31\"\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "# ===== Helper Functions (Data Loading) =====\n",
    "def _guess_tf_from_name(name: str) -> str:\n",
    "    n = name.lower()\n",
    "    if \"m15\" in n or \"15m\" in n: return \"15m\"\n",
    "    if \"h1\" in n or \"1h\" in n: return \"1h\"\n",
    "    if \"h4\" in n or \"4h\" in n: return \"4h\"\n",
    "    if \"m1\" in n or \"1m\" in n: return \"1m\"\n",
    "    return \"\"\n",
    "\n",
    "def _pick_files(data_dir: str, pair_hint: str):\n",
    "    if not os.path.exists(data_dir):\n",
    "        if os.path.exists(os.path.join(\"..\", data_dir)):\n",
    "             data_dir = os.path.join(\"..\", data_dir)\n",
    "             \n",
    "    files = glob.glob(os.path.join(data_dir, \"*.csv\")) + glob.glob(os.path.join(data_dir, \"*.parquet\"))\n",
    "    if not files:\n",
    "        abs_path = os.path.abspath(data_dir)\n",
    "        files = glob.glob(os.path.join(abs_path, \"*.csv\"))\n",
    "    if not files: raise FileNotFoundError(f\"No data in {data_dir}\")\n",
    "\n",
    "    hinted = [f for f in files if pair_hint.lower() in os.path.basename(f).lower()]\n",
    "    if hinted: files = hinted\n",
    "    \n",
    "    by_tf = {}\n",
    "    for f in files:\n",
    "        tf = _guess_tf_from_name(os.path.basename(f))\n",
    "        if tf and tf not in by_tf: by_tf[tf] = f\n",
    "    return by_tf\n",
    "\n",
    "def _standardize_ohlc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    rename_map = {}\n",
    "    for c in df.columns:\n",
    "        if \"time\" in c or \"date\" in c: rename_map[c] = \"time\"\n",
    "        elif \"open\" in c: rename_map[c] = \"open\"\n",
    "        elif \"high\" in c: rename_map[c] = \"high\"\n",
    "        elif \"low\" in c: rename_map[c] = \"low\"\n",
    "        elif \"close\" in c: rename_map[c] = \"close\"\n",
    "        elif \"vol\" in c: rename_map[c] = \"volume\"\n",
    "    df = df.rename(columns=rename_map)\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], utc=True, errors='coerce')\n",
    "    df = df.dropna(subset=[\"time\"]).sort_values(\"time\").set_index(\"time\")\n",
    "    cols = [\"open\",\"high\",\"low\",\"close\",\"volume\"]\n",
    "    for c in cols:\n",
    "        if c in df.columns: df[c] = df[c].astype(float)\n",
    "    return df[ [c for c in cols if c in df.columns] ]\n",
    "\n",
    "def load_tf(data_dir: str, pair_hint: str):\n",
    "    by_tf = _pick_files(data_dir, pair_hint)\n",
    "    out = {}\n",
    "    for tf in [\"1m\",\"15m\",\"1h\",\"4h\"]:\n",
    "        if tf in by_tf:\n",
    "            try:\n",
    "                out[tf] = _standardize_ohlc(pd.read_csv(by_tf[tf]))\n",
    "                print(f\"Loaded {tf}: {len(out[tf])} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {by_tf[tf]}: {e}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58f8a42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1m: 3354904 rows\n",
      "Loaded 15m: 224382 rows\n",
      "Loaded 1h: 56098 rows\n",
      "Loaded 4h: 14498 rows\n",
      "Master DF shape: (223974, 38)\n"
     ]
    }
   ],
   "source": [
    "# ===== 2) Advanced Feature Engineering (Trend + Volatility) =====\n",
    "def ema(s: pd.Series, span: int) -> pd.Series:\n",
    "    return s.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "def rsi(s: pd.Series, n: int=14) -> pd.Series:\n",
    "    delta = s.diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    dn = -1 * delta.clip(upper=0)\n",
    "    ema_up = up.ewm(com=n-1, adjust=False).mean()\n",
    "    ema_dn = dn.ewm(com=n-1, adjust=False).mean()\n",
    "    rs = ema_up / (ema_dn + 1e-9)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def atr(df: pd.DataFrame, n: int=14) -> pd.Series:\n",
    "    h,l,c = df[\"high\"], df[\"low\"], df[\"close\"]\n",
    "    prev_c = c.shift(1)\n",
    "    tr = pd.concat([(h-l).abs(), (h-prev_c).abs(), (l-prev_c).abs()], axis=1).max(axis=1)\n",
    "    return tr.rolling(n).mean()\n",
    "\n",
    "def adx(df: pd.DataFrame, n: int=14) -> pd.Series:\n",
    "    h,l,c = df[\"high\"], df[\"low\"], df[\"close\"]\n",
    "    up, dn = h.diff(), -l.diff()\n",
    "    plus_dm = np.where((up > dn) & (up > 0), up, 0.0)\n",
    "    minus_dm = np.where((dn > up) & (dn > 0), dn, 0.0)\n",
    "    tr = pd.concat([(h-l).abs(), (h-c.shift(1)).abs(), (l-c.shift(1)).abs()], axis=1).max(axis=1)\n",
    "    atr_n = tr.rolling(n).sum()\n",
    "    plus_di = 100 * pd.Series(plus_dm, index=df.index).rolling(n).sum() / (atr_n+1e-9)\n",
    "    minus_di= 100 * pd.Series(minus_dm, index=df.index).rolling(n).sum() / (atr_n+1e-9)\n",
    "    dx = (100 * (plus_di - minus_di).abs() / (plus_di + minus_di)).replace([np.inf,-np.inf], 0)\n",
    "    return dx.rolling(n).mean()\n",
    "\n",
    "def add_features(df: pd.DataFrame, prefix: str) -> pd.DataFrame:\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    c = df[\"close\"]\n",
    "    \n",
    "    # 1. Trend\n",
    "    e50 = ema(c, 50)\n",
    "    e200 = ema(c, 200)\n",
    "    out[f\"{prefix}trend_biase\"] = np.where(e50 > e200, 1, -1)\n",
    "    out[f\"{prefix}dist_e200\"] = (c - e200) / (e200 + 1e-9)\n",
    "    out[f\"{prefix}adx\"] = adx(df, 14)\n",
    "    \n",
    "    # 2. Momentum\n",
    "    out[f\"{prefix}rsi\"] = rsi(c, 14)\n",
    "    out[f\"{prefix}rsi_slope\"] = out[f\"{prefix}rsi\"].diff(3)\n",
    "    \n",
    "    # 3. Volatility\n",
    "    out[f\"{prefix}atr\"] = atr(df, 14)\n",
    "    out[f\"{prefix}atr_rel\"] = out[f\"{prefix}atr\"] / c # Normalized ATR\n",
    "    out[f\"{prefix}bb_width\"] = (df[\"close\"].rolling(20).std() * 2 * 2) / df[\"close\"].rolling(20).mean()\n",
    "    \n",
    "    # 4. Candlestick Anatomy\n",
    "    out[f\"{prefix}body_rel\"] = (df[\"close\"] - df[\"open\"]).abs() / (df[\"high\"] - df[\"low\"] + 1e-9)\n",
    "    \n",
    "    # 5. Time\n",
    "    if prefix == \"15m_\":\n",
    "        out[\"hour\"] = df.index.hour\n",
    "        out[\"dayofweek\"] = df.index.dayofweek\n",
    "    \n",
    "    return out\n",
    "\n",
    "def build_master(bundle: dict) -> pd.DataFrame:\n",
    "    m15 = bundle[\"15m\"].copy()\n",
    "    f15 = add_features(m15, \"15m_\")\n",
    "    df = m15.join(f15)\n",
    "    \n",
    "    # Higher TF Context (Trend)\n",
    "    for tf in [\"1h\", \"4h\"]:\n",
    "        if tf in bundle:\n",
    "            feat = add_features(bundle[tf], f\"{tf}_\")\n",
    "            \n",
    "            # CRITICAL FIX: Shift higher TF features by 1 to prevent future leakage.\n",
    "            # 1h candle at 10:00 closes at 11:00. \n",
    "            # We shift it so that 10:00 index contains data from 09:00 (which is closed/known).\n",
    "            feat = feat.shift(1)\n",
    "            \n",
    "            aligned = feat.reindex(df.index, method=\"ffill\")\n",
    "            df = df.join(aligned)\n",
    "            \n",
    "    # Returns (Lagged)\n",
    "    for i in [1, 3, 6, 12]:\n",
    "        df[f\"ret{i}\"] = df[\"close\"].pct_change(i)\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Load & Build\n",
    "train_tf = load_tf(cfg.train_dir, cfg.pair_hint)\n",
    "master_df = build_master(train_tf)\n",
    "master_df = master_df.dropna()\n",
    "print(\"Master DF shape:\", master_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "209b9dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Balance: {0.0: 0.7557216462625126, 1.0: 0.24427835373748738}\n",
      "Train size: 174115, Val size: 49947\n"
     ]
    }
   ],
   "source": [
    "# ===== 3) Labeling (Triple Barrier) =====\n",
    "def get_triple_barrier_labels(df: pd.DataFrame):\n",
    "    # vectorized approximation is risky for exact hit-sequence, so we use loop\n",
    "    # 0 = No Enter (Flat/Loss), 1 = Profit\n",
    "    # Actually, let's target Direction.\n",
    "    # But for \"Entry\" model, we want: \"If I enter now, will I hit TP before SL?\"\n",
    "    \n",
    "    n = len(df)\n",
    "    y = np.zeros(n)\n",
    "    \n",
    "    closes = df[\"close\"].values\n",
    "    highs = df[\"high\"].values\n",
    "    lows = df[\"low\"].values\n",
    "    atrs = df[\"15m_atr\"].values\n",
    "    opens = df[\"open\"].values # Fill on Next Open\n",
    "    \n",
    "    tp_mult = cfg.tp_atr\n",
    "    sl_mult = cfg.sl_atr\n",
    "    hold = cfg.max_hold_bars\n",
    "    \n",
    "    for i in range(n - hold - 1):\n",
    "        # Trade Logic\n",
    "        # Entry at Open[i+1]\n",
    "        entry_px = opens[i+1]\n",
    "        vol = atrs[i]\n",
    "        \n",
    "        # Volatility Filter applied in logic? No, label truth is independent of filter.\n",
    "        # But if vol is NaN or 0, skip.\n",
    "        if np.isnan(vol) or vol <= 0: continue\n",
    "        \n",
    "        tp = entry_px + vol * tp_mult # Long bias label\n",
    "        sl = entry_px - vol * sl_mult\n",
    "        \n",
    "        outcome = 0\n",
    "        for j in range(1, hold + 1):\n",
    "            h = highs[i+j]\n",
    "            l = lows[i+j]\n",
    "            \n",
    "            # Check SL First (Conservative)\n",
    "            if l <= sl:\n",
    "                outcome = 0\n",
    "                break\n",
    "            if h >= tp:\n",
    "                outcome = 1\n",
    "                break\n",
    "        \n",
    "        y[i] = outcome\n",
    "        \n",
    "    return y\n",
    "\n",
    "master_df[\"y\"] = get_triple_barrier_labels(master_df)\n",
    "print(\"Target Balance:\", master_df[\"y\"].value_counts(normalize=True).to_dict())\n",
    "\n",
    "# Features List\n",
    "FEATURES = [\n",
    "    \"15m_rsi\", \"15m_rsi_slope\", \"15m_adx\", \"15m_bb_width\", \"15m_atr_rel\",\n",
    "    \"15m_dist_e200\", \"15m_body_rel\",\n",
    "    \"1h_trend_biase\", \"1h_adx\", \"1h_rsi\",\n",
    "    \"4h_trend_biase\",\n",
    "    \"ret1\", \"ret3\", \"ret6\", \"ret12\",\n",
    "    \"hour\"\n",
    "]\n",
    "\n",
    "# Clean\n",
    "master_df = master_df.dropna(subset=FEATURES + [\"y\"])\n",
    "\n",
    "# Split\n",
    "train = master_df.loc[:cfg.core_end]\n",
    "val   = master_df.loc[cfg.core_end:cfg.val_end]\n",
    "\n",
    "print(f\"Train size: {len(train)}, Val size: {len(val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61c20e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble AUC (Val): 0.6224954884559099\n"
     ]
    }
   ],
   "source": [
    "# ===== 4) Train Ensemble (Primary) =====\n",
    "X_train = train[FEATURES]\n",
    "y_train = train[\"y\"]\n",
    "X_val   = val[FEATURES]\n",
    "y_val   = val[\"y\"]\n",
    "\n",
    "# 1. XGB\n",
    "if HAS_XGB:\n",
    "    model_xgb = XGBClassifier(\n",
    "        n_estimators=300, max_depth=5, learning_rate=0.03,\n",
    "        subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    model_xgb.fit(X_train, y_train)\n",
    "    p_xgb = model_xgb.predict_proba(X_val)[:, 1]\n",
    "    p_xgb_train = model_xgb.predict_proba(X_train)[:, 1]\n",
    "else:\n",
    "    p_xgb = np.zeros(len(val))\n",
    "    p_xgb_train = np.zeros(len(train))\n",
    "\n",
    "# 2. RF\n",
    "model_rf = RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=8, min_samples_leaf=50, \n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "model_rf.fit(X_train, y_train)\n",
    "p_rf = model_rf.predict_proba(X_val)[:, 1]\n",
    "p_rf_train = model_rf.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# 3. LR\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "model_lr = LogisticRegression(C=0.1, class_weight='balanced')\n",
    "model_lr.fit(X_train_s, y_train)\n",
    "p_lr = model_lr.predict_proba(X_val_s)[:, 1]\n",
    "p_lr_train = model_lr.predict_proba(X_train_s)[:, 1]\n",
    "\n",
    "# Ensemble\n",
    "if HAS_XGB:\n",
    "    p_ens_train = (p_xgb_train + p_rf_train + p_lr_train) / 3\n",
    "    p_ens_val   = (p_xgb + p_rf + p_lr) / 3\n",
    "else:\n",
    "    p_ens_train = (p_rf_train + p_lr_train) / 2\n",
    "    p_ens_val   = (p_rf + p_lr) / 2\n",
    "\n",
    "print(\"Ensemble AUC (Val):\", roc_auc_score(y_val, p_ens_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7fbd4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold  | Trades   | Winrate  | PF      \n",
      "--------------------------------------------------\n",
      "0.50       | 0        | 0.0%     | 0.00    \n",
      "0.52       | 0        | 0.0%     | 0.00    \n",
      "0.54       | 0        | 0.0%     | 0.00    \n",
      "0.55       | 0        | 0.0%     | 0.00    \n",
      "0.56       | 0        | 0.0%     | 0.00    \n",
      "0.58       | 0        | 0.0%     | 0.00    \n",
      "0.60       | 0        | 0.0%     | 0.00    \n"
     ]
    }
   ],
   "source": [
    "# ===== 5) Volatility Filter & Fast Backtest =====\n",
    "def advanced_backtest(df, probs, prob_threshold):\n",
    "    # Apply Volatility Filter\n",
    "    # 5 pips = 0.0005\n",
    "    min_vol = cfg.min_volatility_pips * 0.0001\n",
    "    \n",
    "    atrs = df[\"15m_atr\"].values\n",
    "    opens = df[\"open\"].values\n",
    "    highs = df[\"high\"].values\n",
    "    lows  = df[\"low\"].values\n",
    "    closes= df[\"close\"].values\n",
    "    \n",
    "    hold = cfg.max_hold_bars\n",
    "    tp_mult = cfg.tp_atr\n",
    "    sl_mult = cfg.sl_atr\n",
    "    \n",
    "    trades_pnl = []\n",
    "    \n",
    "    i = 0\n",
    "    end_idx = len(df) - hold - 1\n",
    "    \n",
    "    while i < end_idx:\n",
    "        # 1. Filter Check\n",
    "        curr_vol = atrs[i]\n",
    "        if np.isnan(curr_vol) or curr_vol < min_vol:\n",
    "            i += 1; continue\n",
    "            \n",
    "        # 2. Probability Check\n",
    "        if probs[i] < prob_threshold:\n",
    "            i += 1; continue\n",
    "            \n",
    "        # 3. Enter Trade\n",
    "        entry_px = opens[i+1]\n",
    "        tp_px = entry_px + curr_vol * tp_mult\n",
    "        sl_px = entry_px - curr_vol * sl_mult\n",
    "        \n",
    "        outcome = \"time\"\n",
    "        pnl_r = 0.0\n",
    "        \n",
    "        for j in range(1, hold + 1):\n",
    "            h = highs[i+j]\n",
    "            l = lows[i+j]\n",
    "            \n",
    "            hit_sl = l <= sl_px\n",
    "            hit_tp = h >= tp_px\n",
    "            \n",
    "            if hit_sl and hit_tp:\n",
    "                pnl_r = -1.0 # SL First assumption\n",
    "                outcome = \"sl\"\n",
    "                break\n",
    "            elif hit_sl:\n",
    "                pnl_r = -1.0\n",
    "                outcome = \"sl\"\n",
    "                break\n",
    "            elif hit_tp:\n",
    "                # Reward Value = TP Dist / Risk Dist = 2.0 / 1.0 = 2.0\n",
    "                pnl_r = float(tp_mult / sl_mult)\n",
    "                outcome = \"tp\"\n",
    "                break\n",
    "        \n",
    "        if outcome == \"time\":\n",
    "            exit_px = closes[i+hold]\n",
    "            raw_pnl = exit_px - entry_px\n",
    "            risk_dist = entry_px - sl_px\n",
    "            pnl_r = raw_pnl / risk_dist\n",
    "            \n",
    "        trades_pnl.append(pnl_r)\n",
    "        \n",
    "        # Skip overlapping trades to simulate simple portfolio\n",
    "        i += j\n",
    "        \n",
    "    trades_pnl = np.array(trades_pnl)\n",
    "    if len(trades_pnl) == 0: return 0, 0.0, 0.0\n",
    "    \n",
    "    wins = trades_pnl[trades_pnl > 0]\n",
    "    losses = trades_pnl[trades_pnl <= 0]\n",
    "    \n",
    "    n = len(trades_pnl)\n",
    "    wr = len(wins) / n\n",
    "    \n",
    "    gross_win = wins.sum()\n",
    "    gross_loss = -losses.sum()\n",
    "    pf = gross_win / gross_loss if gross_loss > 0 else 999.0\n",
    "    \n",
    "    return n, wr, pf\n",
    "\n",
    "# Sweep\n",
    "print(f\"{'Threshold':<10} | {'Trades':<8} | {'Winrate':<8} | {'PF':<8}\")\n",
    "print(\"-\" * 50)\n",
    "results = []\n",
    "for th in [0.50, 0.52, 0.54, 0.55, 0.56, 0.58, 0.60]:\n",
    "    n, wr, pf = advanced_backtest(val, p_ens_val, th)\n",
    "    results.append({\"Threshold\": th, \"Trades\": n, \"Winrate\": wr, \"PF\": pf})\n",
    "    print(f\"{th:<10.2f} | {n:<8} | {wr:<8.1%} | {pf:<8.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
