{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ae2832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ FOREX SIGNAL GENERATOR V8\n",
      "   Based on V6 + Optimizations\n",
      "============================================================\n",
      "‚úì GPU Available: True\n",
      "‚úì Model Directory: c:\\Users\\Acer\\Desktop\\Forex-Signal-App\\models\\signal_generator_v8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "MODEL_DIR = BASE_DIR / 'models' / 'signal_generator_v8'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# GPU Check\n",
    "import torch\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ FOREX SIGNAL GENERATOR V8\")\n",
    "print(\"   Based on V6 + Optimizations\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úì GPU Available: {GPU_AVAILABLE}\")\n",
    "print(f\"‚úì Model Directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1e50f1",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a0a410b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,859,492 rows\n",
      "Test: 296,778 rows\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "train_df = pd.read_csv(DATA_DIR / 'EUR_USD_1min.csv')\n",
    "test_df = pd.read_csv(DATA_DIR / 'EUR_USD_test.csv')\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    if 'timestamp' in df.columns:\n",
    "        df.rename(columns={'timestamp': 'time'}, inplace=True)\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows\")\n",
    "print(f\"Test: {len(test_df):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2a12f8",
   "metadata": {},
   "source": [
    "## 2. V8 Feature Engineering (V6 Base + Selected Improvements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2285861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding V8 features...\n",
      "‚úì Features added. Total columns: 49\n",
      "‚úì Features added. Total columns: 49\n"
     ]
    }
   ],
   "source": [
    "def add_features_v8(df):\n",
    "    \"\"\"\n",
    "    V8 Features: V6 Core (33) + Carefully Selected New Features\n",
    "    Goal: Improve without overfitting\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ==================== V6 CORE FEATURES ====================\n",
    "    # Time Features\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['day_of_week'] = df['time'].dt.dayofweek\n",
    "    df['is_london'] = ((df['hour'] >= 8) & (df['hour'] < 16)).astype(int)\n",
    "    df['is_ny'] = ((df['hour'] >= 13) & (df['hour'] < 21)).astype(int)\n",
    "    \n",
    "    # Moving Averages\n",
    "    for p in [5, 10, 20, 50, 200]:\n",
    "        df[f'sma_{p}'] = df['close'].rolling(p).mean()\n",
    "        df[f'ema_{p}'] = df['close'].ewm(span=p, adjust=False).mean()\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    ema12 = df['close'].ewm(span=12).mean()\n",
    "    ema26 = df['close'].ewm(span=26).mean()\n",
    "    df['macd'] = ema12 - ema26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    df['bb_mid'] = df['close'].rolling(20).mean()\n",
    "    df['bb_std'] = df['close'].rolling(20).std()\n",
    "    df['bb_upper'] = df['bb_mid'] + 2 * df['bb_std']\n",
    "    df['bb_lower'] = df['bb_mid'] - 2 * df['bb_std']\n",
    "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_mid']\n",
    "    \n",
    "    # ADX (V6 core)\n",
    "    df['tr0'] = abs(df['high'] - df['low'])\n",
    "    df['tr1'] = abs(df['high'] - df['close'].shift())\n",
    "    df['tr2'] = abs(df['low'] - df['close'].shift())\n",
    "    df['tr'] = df[['tr0', 'tr1', 'tr2']].max(axis=1)\n",
    "    \n",
    "    df['up_move'] = df['high'] - df['high'].shift()\n",
    "    df['down_move'] = df['low'].shift() - df['low']\n",
    "    \n",
    "    df['plus_dm'] = np.where((df['up_move'] > df['down_move']) & (df['up_move'] > 0), df['up_move'], 0)\n",
    "    df['minus_dm'] = np.where((df['down_move'] > df['up_move']) & (df['down_move'] > 0), df['down_move'], 0)\n",
    "    \n",
    "    period = 14\n",
    "    df['atr'] = df['tr'].rolling(period).mean()\n",
    "    df['plus_di'] = 100 * (df['plus_dm'].rolling(period).mean() / (df['atr'] + 1e-10))\n",
    "    df['minus_di'] = 100 * (df['minus_dm'].rolling(period).mean() / (df['atr'] + 1e-10))\n",
    "    df['dx'] = 100 * abs(df['plus_di'] - df['minus_di']) / (df['plus_di'] + df['minus_di'] + 1e-10)\n",
    "    df['adx'] = df['dx'].rolling(period).mean()\n",
    "    \n",
    "    # CCI\n",
    "    tp = (df['high'] + df['low'] + df['close']) / 3\n",
    "    sma_tp = tp.rolling(20).mean()\n",
    "    mad_tp = tp.rolling(20).apply(lambda x: np.abs(x - x.mean()).mean())\n",
    "    df['cci'] = (tp - sma_tp) / (0.015 * mad_tp + 1e-10)\n",
    "    \n",
    "    # Williams %R\n",
    "    hh = df['high'].rolling(14).max()\n",
    "    ll = df['low'].rolling(14).min()\n",
    "    df['williams_r'] = -100 * (hh - df['close']) / (hh - ll + 1e-10)\n",
    "    \n",
    "    # Volatility\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['volatility'] = df['returns'].rolling(20).std() * 100\n",
    "    \n",
    "    # Interaction (V6)\n",
    "    df['rsi_x_adx'] = df['rsi'] * df['adx'] / 100\n",
    "    df['momentum_score'] = (\n",
    "        (df['rsi'] > 50).astype(int) + \n",
    "        (df['macd'] > df['macd_signal']).astype(int) + \n",
    "        (df['plus_di'] > df['minus_di']).astype(int)\n",
    "    )\n",
    "    \n",
    "    # ==================== V8 NEW FEATURES (Carefully Selected) ====================\n",
    "    \n",
    "    # 1. Price Position (normalized)\n",
    "    df['price_position'] = (df['close'] - df['sma_50']) / (df['atr'] + 1e-10)\n",
    "    \n",
    "    # 2. Trend Strength Score (simple)\n",
    "    df['trend_score'] = (\n",
    "        (df['close'] > df['sma_20']).astype(int) +\n",
    "        (df['sma_20'] > df['sma_50']).astype(int) +\n",
    "        (df['sma_50'] > df['sma_200']).astype(int) +\n",
    "        (df['adx'] > 25).astype(int)\n",
    "    )\n",
    "    \n",
    "    # 3. RSI Zones (categorical encoded)\n",
    "    df['rsi_zone'] = pd.cut(df['rsi'], bins=[0, 30, 45, 55, 70, 100], labels=[0, 1, 2, 3, 4]).astype(float)\n",
    "    \n",
    "    # 4. MACD Momentum (rate of change)\n",
    "    df['macd_momentum'] = df['macd_hist'] - df['macd_hist'].shift(3)\n",
    "    \n",
    "    # 5. Volume/ATR Ratio (if volume available)\n",
    "    if 'volume' in df.columns and df['volume'].sum() > 0:\n",
    "        df['volume_ma'] = df['volume'].rolling(20).mean()\n",
    "        df['volume_ratio'] = df['volume'] / (df['volume_ma'] + 1e-10)\n",
    "    else:\n",
    "        df['volume_ratio'] = 1.0\n",
    "    \n",
    "    # 6. Session Overlap (London + NY)\n",
    "    df['is_overlap'] = ((df['hour'] >= 13) & (df['hour'] < 16)).astype(int)\n",
    "    \n",
    "    # 7. Recent Price Action\n",
    "    df['close_vs_high'] = (df['high'].rolling(20).max() - df['close']) / (df['atr'] + 1e-10)\n",
    "    df['close_vs_low'] = (df['close'] - df['low'].rolling(20).min()) / (df['atr'] + 1e-10)\n",
    "    \n",
    "    # Cleanup temp columns\n",
    "    drop_cols = ['tr0', 'tr1', 'tr2', 'tr', 'up_move', 'down_move', 'plus_dm', 'minus_dm']\n",
    "    df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Adding V8 features...\")\n",
    "train_df = add_features_v8(train_df)\n",
    "test_df = add_features_v8(test_df)\n",
    "print(f\"‚úì Features added. Total columns: {len(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473dde4f",
   "metadata": {},
   "source": [
    "## 3. Labeling (BUY vs SELL - Like V6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a7186a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 393,249 (BUY/SELL only)\n",
      "Test: 80,302\n",
      "BUY ratio (train): 49.3%\n",
      "BUY ratio (test): 52.2%\n"
     ]
    }
   ],
   "source": [
    "def create_labels(df, forward_periods=60, min_pips=15, ratio=1.5):\n",
    "    \"\"\"\n",
    "    BUY (1): Up move >= min_pips AND Up > Down * ratio\n",
    "    SELL (0): Down move >= min_pips AND Down > Up * ratio\n",
    "    HOLD (-1): Neither\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    min_move = min_pips * 0.0001\n",
    "    \n",
    "    df['future_max'] = df['high'].rolling(forward_periods).max().shift(-forward_periods)\n",
    "    df['future_min'] = df['low'].rolling(forward_periods).min().shift(-forward_periods)\n",
    "    \n",
    "    df['up_move'] = df['future_max'] - df['close']\n",
    "    df['down_move'] = df['close'] - df['future_min']\n",
    "    \n",
    "    conditions = [\n",
    "        (df['up_move'] >= min_move) & (df['up_move'] > df['down_move'] * ratio),\n",
    "        (df['down_move'] >= min_move) & (df['down_move'] > df['up_move'] * ratio)\n",
    "    ]\n",
    "    choices = [1, 0]\n",
    "    df['signal'] = np.select(conditions, choices, default=-1)\n",
    "    \n",
    "    df.drop(['future_max', 'future_min', 'up_move', 'down_move'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "train_df = create_labels(train_df)\n",
    "test_df = create_labels(test_df)\n",
    "\n",
    "# Filter BUY/SELL only (remove HOLD)\n",
    "train_binary = train_df[train_df['signal'] != -1].copy()\n",
    "test_binary = test_df[test_df['signal'] != -1].copy()\n",
    "\n",
    "print(f\"Train: {len(train_binary):,} (BUY/SELL only)\")\n",
    "print(f\"Test: {len(test_binary):,}\")\n",
    "print(f\"BUY ratio (train): {train_binary['signal'].mean()*100:.1f}%\")\n",
    "print(f\"BUY ratio (test): {test_binary['signal'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed708002",
   "metadata": {},
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "342ff578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features before selection: 43\n",
      "Features after selection: 22\n",
      "\n",
      "Top 10 Features:\n",
      "    feature  importance\n",
      "       hour         404\n",
      "    sma_200         326\n",
      "  volume_ma         293\n",
      "day_of_week         267\n",
      "    ema_200         215\n",
      "   bb_lower         141\n",
      "        atr         138\n",
      " volatility         130\n",
      "     sma_50         119\n",
      "   bb_upper         100\n",
      "Features after selection: 22\n",
      "\n",
      "Top 10 Features:\n",
      "    feature  importance\n",
      "       hour         404\n",
      "    sma_200         326\n",
      "  volume_ma         293\n",
      "day_of_week         267\n",
      "    ema_200         215\n",
      "   bb_lower         141\n",
      "        atr         138\n",
      " volatility         130\n",
      "     sma_50         119\n",
      "   bb_upper         100\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "exclude_cols = ['time', 'signal', 'open', 'high', 'low', 'close', 'volume', 'tick_volume']\n",
    "feature_cols = [c for c in train_binary.columns if c not in exclude_cols]\n",
    "\n",
    "train_clean = train_binary.dropna(subset=feature_cols).copy()\n",
    "test_clean = test_binary.dropna(subset=feature_cols).copy()\n",
    "\n",
    "X_train = train_clean[feature_cols].values\n",
    "y_train = train_clean['signal'].values\n",
    "X_test = test_clean[feature_cols].values\n",
    "y_test = test_clean['signal'].values\n",
    "\n",
    "print(f\"Features before selection: {len(feature_cols)}\")\n",
    "\n",
    "# Feature Importance based selection using LightGBM\n",
    "selector_model = lgb.LGBMClassifier(\n",
    "    n_estimators=100, max_depth=6, random_state=42, verbose=-1, device='gpu'\n",
    ")\n",
    "selector_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = selector_model.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Select top features (importance > median)\n",
    "threshold = np.median(importances)\n",
    "selected_features = importance_df[importance_df['importance'] >= threshold]['feature'].tolist()\n",
    "\n",
    "print(f\"Features after selection: {len(selected_features)}\")\n",
    "print(f\"\\nTop 10 Features:\")\n",
    "print(importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b147cd8",
   "metadata": {},
   "source": [
    "## 5. Prepare Final Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6001f76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (393152, 22)\n",
      "Test data: (80288, 22)\n"
     ]
    }
   ],
   "source": [
    "# Use selected features\n",
    "X_train_selected = train_clean[selected_features].values\n",
    "X_test_selected = test_clean[selected_features].values\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "print(f\"Training data: {X_train_scaled.shape}\")\n",
    "print(f\"Test data: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5722349",
   "metadata": {},
   "source": [
    "## 6. Train Optimized Models (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "080b67b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ TRAINING V8 MODELS (Optimized Hyperparameters)\n",
      "============================================================\n",
      "  Training XGB1... ‚úì Accuracy: 50.31%\n",
      "  Training XGB2... ‚úì Accuracy: 50.31%\n",
      "  Training XGB2... ‚úì Accuracy: 50.19%\n",
      "  Training LGB1... ‚úì Accuracy: 50.19%\n",
      "  Training LGB1... ‚úì Accuracy: 49.90%\n",
      "  Training LGB2... ‚úì Accuracy: 49.90%\n",
      "  Training LGB2... ‚úì Accuracy: 49.57%\n",
      "  Training CAT... ‚úì Accuracy: 49.57%\n",
      "  Training CAT... ‚úì Accuracy: 51.27%\n",
      "‚úì Accuracy: 51.27%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üöÄ TRAINING V8 MODELS (Optimized Hyperparameters)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models = {}\n",
    "\n",
    "# 1. XGBoost - Tuned for higher precision\n",
    "models['xgb1'] = xgb.XGBClassifier(\n",
    "    n_estimators=600, max_depth=6, learning_rate=0.03,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    reg_alpha=0.1, reg_lambda=1.0,\n",
    "    min_child_weight=3,\n",
    "    random_state=42, tree_method='hist', device='cuda', verbosity=0\n",
    ")\n",
    "\n",
    "# 2. XGBoost - Different config\n",
    "models['xgb2'] = xgb.XGBClassifier(\n",
    "    n_estimators=400, max_depth=8, learning_rate=0.05,\n",
    "    subsample=0.7, colsample_bytree=0.7,\n",
    "    reg_alpha=0.05, reg_lambda=0.5,\n",
    "    gamma=0.1,\n",
    "    random_state=43, tree_method='hist', device='cuda', verbosity=0\n",
    ")\n",
    "\n",
    "# 3. LightGBM - Tuned\n",
    "models['lgb1'] = lgb.LGBMClassifier(\n",
    "    n_estimators=600, max_depth=6, learning_rate=0.03,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    num_leaves=31, min_child_samples=30,\n",
    "    reg_alpha=0.1, reg_lambda=1.0,\n",
    "    random_state=42, verbose=-1, device='gpu'\n",
    ")\n",
    "\n",
    "# 4. LightGBM - Different config\n",
    "models['lgb2'] = lgb.LGBMClassifier(\n",
    "    n_estimators=500, max_depth=8, learning_rate=0.04,\n",
    "    subsample=0.75, colsample_bytree=0.75,\n",
    "    num_leaves=63, min_child_samples=20,\n",
    "    random_state=44, verbose=-1, device='gpu'\n",
    ")\n",
    "\n",
    "# 5. CatBoost - Tuned\n",
    "models['cat'] = CatBoostClassifier(\n",
    "    iterations=600, depth=6, learning_rate=0.03,\n",
    "    l2_leaf_reg=3.0, random_strength=0.5,\n",
    "    bagging_temperature=0.5,\n",
    "    random_seed=42, task_type='GPU', devices='0', verbose=False\n",
    ")\n",
    "\n",
    "# Train\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"  Training {name.upper()}...\", end=\" \")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    predictions[name] = model.predict(X_test_scaled)\n",
    "    probabilities[name] = model.predict_proba(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, predictions[name])\n",
    "    print(f\"‚úì Accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021473fb",
   "metadata": {},
   "source": [
    "## 7. Optimized Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d4f419e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Weights:\n",
      "  xgb1: 0.200\n",
      "  xgb2: 0.200\n",
      "  lgb1: 0.199\n",
      "  lgb2: 0.197\n",
      "  cat: 0.204\n",
      "\n",
      "All 5 agree on BUY: 19,277\n",
      "4+ agree on BUY: 28,511\n"
     ]
    }
   ],
   "source": [
    "# Find optimal weights based on individual performance\n",
    "accuracies = {name: accuracy_score(y_test, predictions[name]) for name in models.keys()}\n",
    "total_acc = sum(accuracies.values())\n",
    "\n",
    "# Weight based on accuracy\n",
    "weights = {name: acc / total_acc for name, acc in accuracies.items()}\n",
    "print(\"Optimized Weights:\")\n",
    "for name, w in weights.items():\n",
    "    print(f\"  {name}: {w:.3f}\")\n",
    "\n",
    "# Weighted Ensemble\n",
    "final_proba = np.zeros_like(probabilities['xgb1'])\n",
    "for name, w in weights.items():\n",
    "    final_proba += w * probabilities[name]\n",
    "\n",
    "buy_prob = final_proba[:, 1] * 100\n",
    "\n",
    "# Agreement bonus (all 5 agree)\n",
    "all_agree_buy = np.all([predictions[name] == 1 for name in models.keys()], axis=0)\n",
    "most_agree_buy = np.sum([predictions[name] == 1 for name in models.keys()], axis=0) >= 4\n",
    "\n",
    "confidence = buy_prob.copy()\n",
    "confidence[all_agree_buy] = np.minimum(confidence[all_agree_buy] + 5, 100)\n",
    "confidence[most_agree_buy & ~all_agree_buy] = np.minimum(confidence[most_agree_buy & ~all_agree_buy] + 2, 100)\n",
    "\n",
    "print(f\"\\nAll 5 agree on BUY: {all_agree_buy.sum():,}\")\n",
    "print(f\"4+ agree on BUY: {most_agree_buy.sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed41b6",
   "metadata": {},
   "source": [
    "## 8. V8 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8be1a714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä V8 ENSEMBLE RESULTS\n",
      "======================================================================\n",
      "\n",
      "  Confidence |    Signals |    Correct |   Accuracy\n",
      "------------------------------------------------------------\n",
      "        50%+ |      36815 |      19288 |      52.4%\n",
      "        60%+ |      16273 |       8711 |      53.5%\n",
      "        70%+ |       1948 |       1147 |      58.9%\n",
      "        75%+ |        627 |        424 |      67.6%\n",
      "        80%+ |        229 |        149 |      65.1%\n",
      "        85%+ |         48 |         45 |      93.8%\n",
      "        90%+ |          8 |          8 |     100.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìä V8 ENSEMBLE RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Confidence':>12} | {'Signals':>10} | {'Correct':>10} | {'Accuracy':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "v8_results = {}\n",
    "for conf in [50, 60, 70, 75, 80, 85, 90, 95]:\n",
    "    mask = confidence >= conf\n",
    "    if mask.sum() > 0:\n",
    "        signals = mask.sum()\n",
    "        correct = y_test[mask].sum()\n",
    "        acc = correct / signals * 100\n",
    "        v8_results[conf] = {'signals': signals, 'correct': correct, 'accuracy': acc}\n",
    "        print(f\"{conf:>10}%+ | {signals:>10} | {correct:>10} | {acc:>9.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f3434",
   "metadata": {},
   "source": [
    "## 9. V8 vs V6 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f862fabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä V8 vs V6 COMPARISON\n",
      "================================================================================\n",
      "\n",
      " Threshold |   V6 Sig |   V6 Acc |   V8 Sig |   V8 Acc |     Winner\n",
      "----------------------------------------------------------------------\n",
      "      70%+ |     4480 |    55.3% |     1948 |    58.9% |       V8 ‚úì\n",
      "      75%+ |     1406 |    56.8% |      627 |    67.6% |       V8 ‚úì\n",
      "      80%+ |      408 |    58.1% |      229 |    65.1% |       V8 ‚úì\n",
      "      85%+ |      104 |    63.5% |       48 |    93.8% |       V8 ‚úì\n",
      "      90%+ |       21 |    76.2% |        8 |   100.0% |       V8 ‚úì\n"
     ]
    }
   ],
   "source": [
    "# Load V6 models\n",
    "v6_dir = BASE_DIR / 'models' / 'signal_generator_v6'\n",
    "\n",
    "v6_models = {}\n",
    "for name in ['xgb1', 'xgb2', 'lgb1', 'lgb2', 'cat']:\n",
    "    v6_models[name] = joblib.load(v6_dir / f'{name}_v6_bin.joblib')\n",
    "\n",
    "v6_scaler = joblib.load(v6_dir / 'scaler_v6_bin.joblib')\n",
    "v6_feature_cols = joblib.load(v6_dir / 'feature_cols_v6.joblib')\n",
    "\n",
    "# Prepare V6 test data\n",
    "missing = [c for c in v6_feature_cols if c not in test_clean.columns]\n",
    "for c in missing:\n",
    "    test_clean[c] = 0\n",
    "\n",
    "X_test_v6 = test_clean[v6_feature_cols].values\n",
    "X_test_v6_scaled = v6_scaler.transform(X_test_v6)\n",
    "\n",
    "# V6 predictions\n",
    "v6_proba = {}\n",
    "v6_preds = {}\n",
    "for name, model in v6_models.items():\n",
    "    v6_preds[name] = model.predict(X_test_v6_scaled)\n",
    "    v6_proba[name] = model.predict_proba(X_test_v6_scaled)\n",
    "\n",
    "# V6 ensemble\n",
    "v6_weights = {'xgb1': 0.20, 'xgb2': 0.20, 'lgb1': 0.20, 'lgb2': 0.20, 'cat': 0.20}\n",
    "v6_final_proba = np.zeros_like(v6_proba['xgb1'])\n",
    "for name, w in v6_weights.items():\n",
    "    v6_final_proba += w * v6_proba[name]\n",
    "\n",
    "v6_buy_prob = v6_final_proba[:, 1] * 100\n",
    "v6_all_agree = np.all([v6_preds[name] == 1 for name in v6_models.keys()], axis=0)\n",
    "v6_confidence = v6_buy_prob.copy()\n",
    "v6_confidence[v6_all_agree] = np.minimum(v6_confidence[v6_all_agree] + 5, 100)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä V8 vs V6 COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Threshold':>10} | {'V6 Sig':>8} | {'V6 Acc':>8} | {'V8 Sig':>8} | {'V8 Acc':>8} | {'Winner':>10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for conf in [70, 75, 80, 85, 90]:\n",
    "    # V6\n",
    "    v6_mask = v6_confidence >= conf\n",
    "    v6_sig = v6_mask.sum()\n",
    "    v6_acc = y_test[v6_mask].mean() * 100 if v6_sig > 0 else 0\n",
    "    \n",
    "    # V8\n",
    "    v8_mask = confidence >= conf\n",
    "    v8_sig = v8_mask.sum()\n",
    "    v8_acc = y_test[v8_mask].mean() * 100 if v8_sig > 0 else 0\n",
    "    \n",
    "    if v8_acc > v6_acc + 1:\n",
    "        winner = \"V8 ‚úì\"\n",
    "    elif v6_acc > v8_acc + 1:\n",
    "        winner = \"V6 ‚úì\"\n",
    "    else:\n",
    "        winner = \"‚âà TIE\"\n",
    "    \n",
    "    print(f\"{conf:>8}%+ | {v6_sig:>8} | {v6_acc:>7.1f}% | {v8_sig:>8} | {v8_acc:>7.1f}% | {winner:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379f213",
   "metadata": {},
   "source": [
    "## 10. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb1ba5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìã FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "V8 Best: 85% threshold ‚Üí 93.8% accuracy (48 signals)\n",
      "\n",
      "üí° RECOMMENDED CONFIGURATIONS:\n",
      "   75%+: 67.6% acc, 627 signals\n",
      "   80%+: 65.1% acc, 229 signals\n",
      "   85%+: 93.8% acc, 48 signals\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìã FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best configs\n",
    "v8_best = max(v8_results.items(), key=lambda x: x[1]['accuracy'] if x[1]['signals'] >= 30 else 0)\n",
    "\n",
    "print(f\"\\nV8 Best: {v8_best[0]}% threshold ‚Üí {v8_best[1]['accuracy']:.1f}% accuracy ({v8_best[1]['signals']} signals)\")\n",
    "\n",
    "# Practical recommendation\n",
    "print(\"\\nüí° RECOMMENDED CONFIGURATIONS:\")\n",
    "for conf in [75, 80, 85]:\n",
    "    if conf in v8_results and v8_results[conf]['signals'] >= 20:\n",
    "        r = v8_results[conf]\n",
    "        print(f\"   {conf}%+: {r['accuracy']:.1f}% acc, {r['signals']} signals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa542b4a",
   "metadata": {},
   "source": [
    "## 11. Save V8 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a522bf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving V8 Models...\n",
      "‚úÖ V8 Models Saved to c:\\Users\\Acer\\Desktop\\Forex-Signal-App\\models\\signal_generator_v8\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving V8 Models...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    joblib.dump(model, MODEL_DIR / f'{name}_v8.joblib')\n",
    "\n",
    "joblib.dump(scaler, MODEL_DIR / 'scaler_v8.joblib')\n",
    "joblib.dump(selected_features, MODEL_DIR / 'feature_cols_v8.joblib')\n",
    "joblib.dump(weights, MODEL_DIR / 'weights_v8.joblib')\n",
    "\n",
    "config = {\n",
    "    'version': 'v8',\n",
    "    'mode': 'BUY_vs_SELL',\n",
    "    'best_threshold': v8_best[0],\n",
    "    'features': len(selected_features),\n",
    "    'models': list(models.keys())\n",
    "}\n",
    "joblib.dump(config, MODEL_DIR / 'config_v8.joblib')\n",
    "\n",
    "print(f\"‚úÖ V8 Models Saved to {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d99d9414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîç OVERFIT ANALYSIS: Train vs Test\n",
      "======================================================================\n",
      "\n",
      " Threshold |  Train Sig |  Train Acc |   Test Sig |   Test Acc |     Diff\n",
      "----------------------------------------------------------------------\n",
      "      70%+ |       1799 |      43.8% |       1948 |      58.9% |   -15.1% ‚úÖ OK\n",
      "      75%+ |        272 |      57.7% |        627 |      67.6% |    -9.9% ‚úÖ OK\n",
      "      80%+ |         19 |     100.0% |        229 |      65.1% |   +34.9% ‚ö†Ô∏è OVERFIT\n",
      "      85%+ |          0 |       0.0% |         48 |      93.8% |   -93.8% ‚úÖ OK\n",
      "      90%+ |          0 |       0.0% |          8 |     100.0% |  -100.0% ‚úÖ OK\n",
      "\n",
      "======================================================================\n",
      "üìã OVERFIT INTERPRETATION:\n",
      "   - Diff < 5%: ‚úÖ Generalization —Å–∞–π–Ω\n",
      "   - Diff 5-10%: ‚ö° –ë–∞–≥–∞ –∑—ç—Ä—ç–≥ overfit\n",
      "   - Diff > 10%: ‚ö†Ô∏è Overfit - Test –¥—ç—ç—Ä—Ö “Ø—Ä –¥“Ø–Ω–¥ –∞–Ω—Ö–∞–∞—Ä–∞—Ö\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# OVERFIT CHECK: Train vs Test Accuracy\n",
    "# ================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"üîç OVERFIT ANALYSIS: Train vs Test\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train predictions\n",
    "train_proba = {}\n",
    "train_preds = {}\n",
    "for name, model in models.items():\n",
    "    train_proba[name] = model.predict_proba(X_train_selected)\n",
    "    train_preds[name] = model.predict(X_train_selected)\n",
    "\n",
    "# Train ensemble\n",
    "train_final_proba = np.zeros_like(train_proba['xgb1'])\n",
    "for name, w in weights.items():\n",
    "    train_final_proba += w * train_proba[name]\n",
    "\n",
    "train_buy_prob = train_final_proba[:, 1] * 100\n",
    "\n",
    "# Agreement bonus\n",
    "train_all_agree = np.all([train_preds[name] == 1 for name in models.keys()], axis=0)\n",
    "train_confidence = train_buy_prob.copy()\n",
    "train_confidence[train_all_agree] = np.minimum(train_confidence[train_all_agree] + 5, 100)\n",
    "\n",
    "print(f\"\\n{'Threshold':>10} | {'Train Sig':>10} | {'Train Acc':>10} | {'Test Sig':>10} | {'Test Acc':>10} | {'Diff':>8}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for conf in [70, 75, 80, 85, 90]:\n",
    "    # Train\n",
    "    train_mask = train_confidence >= conf\n",
    "    train_sig = train_mask.sum()\n",
    "    train_acc = y_train[train_mask].mean() * 100 if train_sig > 0 else 0\n",
    "    \n",
    "    # Test\n",
    "    test_mask = confidence >= conf\n",
    "    test_sig = test_mask.sum()\n",
    "    test_acc = y_test[test_mask].mean() * 100 if test_sig > 0 else 0\n",
    "    \n",
    "    diff = train_acc - test_acc\n",
    "    overfit = \"‚ö†Ô∏è OVERFIT\" if diff > 10 else (\"‚úÖ OK\" if diff < 5 else \"‚ö° SLIGHT\")\n",
    "    \n",
    "    print(f\"{conf:>8}%+ | {train_sig:>10} | {train_acc:>9.1f}% | {test_sig:>10} | {test_acc:>9.1f}% | {diff:>+7.1f}% {overfit}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã OVERFIT INTERPRETATION:\")\n",
    "print(\"   - Diff < 5%: ‚úÖ Generalization —Å–∞–π–Ω\")\n",
    "print(\"   - Diff 5-10%: ‚ö° –ë–∞–≥–∞ –∑—ç—Ä—ç–≥ overfit\")\n",
    "print(\"   - Diff > 10%: ‚ö†Ô∏è Overfit - Test –¥—ç—ç—Ä—Ö “Ø—Ä –¥“Ø–Ω–¥ –∞–Ω—Ö–∞–∞—Ä–∞—Ö\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e9b59e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä STATISTICAL SIGNIFICANCE (95% Confidence Interval)\n",
      "======================================================================\n",
      "\n",
      " Threshold |  Signals |   Accuracy |               95% CI |    Reliable?\n",
      "---------------------------------------------------------------------------\n",
      "      70%+ |     1948 |      58.9% |      [56.7% - 61.0%] |   ‚úÖ RELIABLE\n",
      "      75%+ |      627 |      67.6% |      [63.9% - 71.2%] |   ‚úÖ RELIABLE\n",
      "      80%+ |      229 |      65.1% |      [58.7% - 70.9%] |   ‚úÖ RELIABLE\n",
      "      85%+ |       48 |      93.8% |      [83.2% - 97.9%] |   ‚ö° MODERATE\n",
      "      90%+ |        8 |     100.0% |     [67.6% - 100.0%] |   ‚ö†Ô∏è TOO FEW\n",
      "\n",
      "======================================================================\n",
      "üìã RECOMMENDATION:\n",
      "   - ‚úÖ RELIABLE: Use this threshold confidently\n",
      "   - ‚ö° MODERATE: Use with caution, monitor performance\n",
      "   - ‚ö†Ô∏è TOO FEW: Need more data to confirm\n",
      "   - ‚ùå NOT RELIABLE: Don't use, may be random chance\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# STATISTICAL SIGNIFICANCE CHECK\n",
    "# ================================================================\n",
    "from scipy import stats\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä STATISTICAL SIGNIFICANCE (95% Confidence Interval)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def confidence_interval(n_success, n_total, confidence=0.95):\n",
    "    \"\"\"Wilson score interval for proportion\"\"\"\n",
    "    if n_total == 0:\n",
    "        return 0, 0, 0\n",
    "    p = n_success / n_total\n",
    "    z = stats.norm.ppf((1 + confidence) / 2)\n",
    "    \n",
    "    denominator = 1 + z**2 / n_total\n",
    "    center = (p + z**2 / (2 * n_total)) / denominator\n",
    "    spread = z * np.sqrt((p * (1 - p) + z**2 / (4 * n_total)) / n_total) / denominator\n",
    "    \n",
    "    lower = max(0, center - spread)\n",
    "    upper = min(1, center + spread)\n",
    "    return p, lower, upper\n",
    "\n",
    "print(f\"\\n{'Threshold':>10} | {'Signals':>8} | {'Accuracy':>10} | {'95% CI':>20} | {'Reliable?':>12}\")\n",
    "print(\"-\"*75)\n",
    "\n",
    "for conf in [70, 75, 80, 85, 90]:\n",
    "    mask = confidence >= conf\n",
    "    n_total = mask.sum()\n",
    "    n_success = y_test[mask].sum() if n_total > 0 else 0\n",
    "    \n",
    "    acc, lower, upper = confidence_interval(n_success, n_total)\n",
    "    ci_str = f\"[{lower*100:.1f}% - {upper*100:.1f}%]\"\n",
    "    \n",
    "    # Reliability check\n",
    "    if n_total >= 100 and lower > 0.55:\n",
    "        reliable = \"‚úÖ RELIABLE\"\n",
    "    elif n_total >= 30 and lower > 0.50:\n",
    "        reliable = \"‚ö° MODERATE\"\n",
    "    elif n_total < 30:\n",
    "        reliable = \"‚ö†Ô∏è TOO FEW\"\n",
    "    else:\n",
    "        reliable = \"‚ùå NOT RELIABLE\"\n",
    "    \n",
    "    print(f\"{conf:>8}%+ | {n_total:>8} | {acc*100:>9.1f}% | {ci_str:>20} | {reliable:>12}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã RECOMMENDATION:\")\n",
    "print(\"   - ‚úÖ RELIABLE: Use this threshold confidently\")\n",
    "print(\"   - ‚ö° MODERATE: Use with caution, monitor performance\")\n",
    "print(\"   - ‚ö†Ô∏è TOO FEW: Need more data to confirm\")\n",
    "print(\"   - ‚ùå NOT RELIABLE: Don't use, may be random chance\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
