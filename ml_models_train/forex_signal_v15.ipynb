{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17927e14",
   "metadata": {},
   "source": [
    "# Forex Signal V15: Multi-Timeframe Analysis (1m, 5m, 15m, 30m, 1h)\n",
    "\n",
    "This notebook implements a multi-timeframe machine learning approach for Forex prediction.\n",
    "It aggregates features from 1-minute, 5-minute, 15-minute, 30-minute, and 1-hour data to predict price movements.\n",
    "\n",
    "**Key Features:**\n",
    "- **Multi-Timeframe Resampling:** Converts 1m data into 5m, 15m, 30m, 1h candles.\n",
    "- **MTF Indicators:** Calculates RSI, MACD, BB, EMA for *all* timeframes.\n",
    "- **Feature Merging:** Aligns higher timeframe features to the 1-minute timeline (using forward fill to avoid lookahead bias).\n",
    "- **Ensemble Model:** Trains XGBoost, LightGBM, and CatBoost on the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2855b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ FOREX SIGNAL V15: Multi-Timeframe Analysis\n",
      "âœ“ Model Directory: c:\\Users\\Acer\\Desktop\\Forex-Signal-App\\models\\signal_generator_v15\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import joblib\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(r'c:\\Users\\Acer\\Desktop\\Forex-Signal-App')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "MODEL_DIR = BASE_DIR / 'models' / 'signal_generator_v15'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Constants\n",
    "TIMEFRAMES = ['1min', '5min', '15min', '30min', '1h']\n",
    "TARGET_TIMEFRAME = '1min'  # We predict on 1-min data using MTF context\n",
    "FORWARD_BARS = 15          # Predict 15 minutes into the future\n",
    "TP_PIPS = 10\n",
    "SL_PIPS = 10\n",
    "\n",
    "print(f\"ðŸš€ FOREX SIGNAL V15: Multi-Timeframe Analysis\")\n",
    "print(f\"âœ“ Model Directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d253194d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loader defined.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data (Train & Test Split)\n",
    "def load_tf_data(timeframe, dataset_type='train'):\n",
    "    \"\"\"\n",
    "    Load specific timeframe data from train or test folder\n",
    "    \"\"\"\n",
    "    # File names are like EURUSD_m1.csv\n",
    "    filename = f'EURUSD_{timeframe}.csv'\n",
    "    file_path = DATA_DIR / dataset_type / filename\n",
    "        \n",
    "    if not file_path.exists():\n",
    "        print(f\"âš ï¸ Warning: {file_path} not found.\")\n",
    "        return None\n",
    "        \n",
    "    # print(f\"Loading {dataset_type} {timeframe} data...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Standardize columns\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    \n",
    "    # Ensure time column is datetime\n",
    "    if 'time' in df.columns:\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df.set_index('time', inplace=True)\n",
    "        df.sort_index(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Data loader defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10defdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TRAIN data (2015-2023)...\n",
      "  1min: 3,354,904 rows\n",
      "  5min: 671,581 rows\n",
      "  15min: 224,382 rows\n",
      "  30min: 112,194 rows\n",
      "  1h: 56,098 rows\n",
      "\n",
      "Loading TEST data (2024-2025)...\n",
      "  1min: 743,476 rows\n",
      "  5min: 148,502 rows\n",
      "  15min: 49,807 rows\n",
      "  30min: 24,907 rows\n",
      "  1h: 12,454 rows\n"
     ]
    }
   ],
   "source": [
    "# 2. Load Multi-Timeframe Data\n",
    "timeframes_map = {\n",
    "    '1min': 'm1',\n",
    "    '5min': 'm5',\n",
    "    '15min': 'm15',\n",
    "    '30min': 'm30',\n",
    "    '1h': 'h1'\n",
    "}\n",
    "\n",
    "dfs_train = {}\n",
    "dfs_test = {}\n",
    "\n",
    "print(\"Loading TRAIN data (2015-2023)...\")\n",
    "for tf_name, tf_code in timeframes_map.items():\n",
    "    dfs_train[tf_name] = load_tf_data(tf_code, 'train')\n",
    "    if dfs_train[tf_name] is not None:\n",
    "        print(f\"  {tf_name}: {len(dfs_train[tf_name]):,} rows\")\n",
    "\n",
    "print(\"\\nLoading TEST data (2024-2025)...\")\n",
    "for tf_name, tf_code in timeframes_map.items():\n",
    "    dfs_test[tf_name] = load_tf_data(tf_code, 'test')\n",
    "    if dfs_test[tf_name] is not None:\n",
    "        print(f\"  {tf_name}: {len(dfs_test[tf_name]):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "582b2919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TRAIN features...\n",
      "Processing TEST features...\n",
      "Feature engineering complete.\n"
     ]
    }
   ],
   "source": [
    "# 3. Feature Engineering\n",
    "def add_technical_indicators(df):\n",
    "    \"\"\"\n",
    "    Calculates technical indicators for a given dataframe.\n",
    "    \"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return None\n",
    "        \n",
    "    df = df.copy()\n",
    "    \n",
    "    # RSI (14)\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD (12, 26, 9)\n",
    "    ema12 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    ema26 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['macd'] = ema12 - ema26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # Bollinger Bands (20, 2)\n",
    "    df['bb_mid'] = df['close'].rolling(window=20).mean()\n",
    "    df['bb_std'] = df['close'].rolling(window=20).std()\n",
    "    df['bb_upper'] = df['bb_mid'] + (df['bb_std'] * 2)\n",
    "    df['bb_lower'] = df['bb_mid'] - (df['bb_std'] * 2)\n",
    "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_mid']\n",
    "    \n",
    "    # EMAs\n",
    "    for span in [9, 21, 50, 200]:\n",
    "        df[f'ema_{span}'] = df['close'].ewm(span=span, adjust=False).mean()\n",
    "        \n",
    "    # ATR (14)\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = (df['high'] - df['close'].shift()).abs()\n",
    "    low_close = (df['low'] - df['close'].shift()).abs()\n",
    "    ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
    "    true_range = ranges.max(axis=1)\n",
    "    df['atr'] = true_range.rolling(14).mean()\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "def process_all_timeframes(dfs_dict):\n",
    "    processed = {}\n",
    "    for tf, df in dfs_dict.items():\n",
    "        if df is not None:\n",
    "            # print(f\"Processing {tf}...\")\n",
    "            processed[tf] = add_technical_indicators(df)\n",
    "            \n",
    "            # Rename columns with prefix (except for 1min which is base)\n",
    "            if tf != '1min' and processed[tf] is not None:\n",
    "                prefix = f\"{tf}_\"\n",
    "                new_cols = {col: f\"{prefix}{col}\" for col in processed[tf].columns}\n",
    "                processed[tf].rename(columns=new_cols, inplace=True)\n",
    "    return processed\n",
    "\n",
    "print(\"Processing TRAIN features...\")\n",
    "processed_train = process_all_timeframes(dfs_train)\n",
    "\n",
    "print(\"Processing TEST features...\")\n",
    "processed_test = process_all_timeframes(dfs_test)\n",
    "\n",
    "print(\"Feature engineering complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42b6d631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging TRAIN data...\n",
      "Train Shape: (3353766, 95)\n",
      "Merging TEST data...\n",
      "Test Shape: (742341, 95)\n"
     ]
    }
   ],
   "source": [
    "# 4. Merge Data (Multi-Timeframe Alignment)\n",
    "def merge_timeframes(processed_dfs):\n",
    "    if '1min' not in processed_dfs or processed_dfs['1min'] is None:\n",
    "        return None\n",
    "        \n",
    "    master_df = processed_dfs['1min'].copy()\n",
    "\n",
    "    for tf in ['5min', '15min', '30min', '1h']:\n",
    "        if tf in processed_dfs and processed_dfs[tf] is not None:\n",
    "            # print(f\"Merging {tf} features...\")\n",
    "            resampled_df = processed_dfs[tf].sort_index()\n",
    "            \n",
    "            master_df = pd.merge_asof(\n",
    "                master_df,\n",
    "                resampled_df,\n",
    "                left_index=True,\n",
    "                right_index=True,\n",
    "                direction='backward'\n",
    "            )\n",
    "            \n",
    "    master_df.dropna(inplace=True)\n",
    "    return master_df\n",
    "\n",
    "print(\"Merging TRAIN data...\")\n",
    "master_train = merge_timeframes(processed_train)\n",
    "print(f\"Train Shape: {master_train.shape}\")\n",
    "\n",
    "print(\"Merging TEST data...\")\n",
    "master_test = merge_timeframes(processed_test)\n",
    "print(f\"Test Shape: {master_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1275f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating targets for TRAIN...\n",
      "Creating targets for TEST...\n",
      "Target Distribution (Train):\n",
      "target\n",
      "0    0.865145\n",
      "2    0.067941\n",
      "1    0.066913\n",
      "Name: proportion, dtype: float64\n",
      "Training Features: 69\n",
      "Train Set: 3,353,751 rows\n",
      "Test Set:  742,326 rows\n"
     ]
    }
   ],
   "source": [
    "# 5. Define Target\n",
    "def create_target(df, tp_pips=10, sl_pips=10, forward_bars=15):\n",
    "    targets = []\n",
    "    pip_size = 0.0001\n",
    "    tp = tp_pips * pip_size\n",
    "    sl = sl_pips * pip_size\n",
    "    \n",
    "    closes = df['close'].values\n",
    "    highs = df['high'].values\n",
    "    lows = df['low'].values\n",
    "    \n",
    "    for i in range(len(df) - forward_bars):\n",
    "        current_close = closes[i]\n",
    "        \n",
    "        # Look ahead window\n",
    "        future_highs = highs[i+1 : i+1+forward_bars]\n",
    "        future_lows = lows[i+1 : i+1+forward_bars]\n",
    "        \n",
    "        max_high = np.max(future_highs)\n",
    "        min_low = np.min(future_lows)\n",
    "        \n",
    "        # Check for Buy Signal\n",
    "        if (max_high - current_close) >= tp and (current_close - min_low) < sl:\n",
    "            targets.append(1) # BUY\n",
    "        # Check for Sell Signal\n",
    "        elif (current_close - min_low) >= tp and (max_high - current_close) < sl:\n",
    "            targets.append(2) # SELL\n",
    "        else:\n",
    "            targets.append(0) # HOLD\n",
    "            \n",
    "    # Pad the end\n",
    "    targets.extend([0] * forward_bars)\n",
    "    return np.array(targets)\n",
    "\n",
    "print(\"Creating targets for TRAIN...\")\n",
    "master_train['target'] = create_target(master_train, TP_PIPS, SL_PIPS, FORWARD_BARS)\n",
    "train_df = master_train.iloc[:-FORWARD_BARS] # Remove last rows\n",
    "\n",
    "print(\"Creating targets for TEST...\")\n",
    "master_test['target'] = create_target(master_test, TP_PIPS, SL_PIPS, FORWARD_BARS)\n",
    "test_df = master_test.iloc[:-FORWARD_BARS] # Remove last rows\n",
    "\n",
    "print(\"Target Distribution (Train):\")\n",
    "print(train_df['target'].value_counts(normalize=True))\n",
    "\n",
    "# Define feature columns\n",
    "feature_cols = [c for c in train_df.columns if c not in ['target', 'open', 'high', 'low', 'close', 'tick_volume', 'spread', 'volume']]\n",
    "# Also exclude raw OHLC of higher timeframes\n",
    "feature_cols = [c for c in feature_cols if not any(x in c for x in ['open', 'high', 'low', 'tick_volume', 'volume'])]\n",
    "\n",
    "print(f\"Training Features: {len(feature_cols)}\")\n",
    "print(f\"Train Set: {len(train_df):,} rows\")\n",
    "print(f\"Test Set:  {len(test_df):,} rows\")\n",
    "\n",
    "# Define paths for saving processed data\n",
    "train_path = DATA_DIR / 'train' / 'EURUSD_train_processed.csv'\n",
    "test_path = DATA_DIR / 'test' / 'EURUSD_test_processed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf6f517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Train data to c:\\Users\\Acer\\Desktop\\Forex-Signal-App\\data\\train\\EURUSD_train_processed.csv...\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# SAVE & RELOAD SPLIT DATA\n",
    "# ==========================================\n",
    "\n",
    "# Use absolute paths defined earlier\n",
    "train_path = DATA_DIR / 'train' / 'EURUSD_train_processed.csv'\n",
    "test_path = DATA_DIR / 'test' / 'EURUSD_test_processed.csv'\n",
    "\n",
    "print(f\"Saving Train data to {train_path}...\")\n",
    "train_df.to_csv(train_path)\n",
    "\n",
    "print(f\"Saving Test data to {test_path}...\")\n",
    "test_df.to_csv(test_path)\n",
    "\n",
    "print(\"Reloading data from files to ensure training uses the saved files...\")\n",
    "train_df = pd.read_csv(train_path, index_col=0, parse_dates=True)\n",
    "test_df = pd.read_csv(test_path, index_col=0, parse_dates=True)\n",
    "\n",
    "print(\"Data reloaded successfully.\")\n",
    "print(f\"Train Shape: {train_df.shape}\")\n",
    "print(f\"Test Shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe38f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Train Models (XGBoost & CatBoost)\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['target']\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['target']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# XGBoost\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='multi:softprob',\n",
    "    num_class=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# CatBoost\n",
    "print(\"Training CatBoost...\")\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    loss_function='MultiClass',\n",
    "    verbose=100,\n",
    "    random_seed=42,\n",
    "    task_type='CPU' # Use GPU if available\n",
    ")\n",
    "cat_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3c08fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Evaluation\n",
    "def evaluate_model(model, X, y, name=\"Model\"):\n",
    "    preds = model.predict(X)\n",
    "    if isinstance(preds[0], list) or (isinstance(preds, np.ndarray) and preds.ndim > 1):\n",
    "         preds = np.array([p[0] for p in preds]) # Handle CatBoost output shape if needed\n",
    "         \n",
    "    acc = accuracy_score(y, preds)\n",
    "    print(f\"--- {name} Performance ---\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y, preds))\n",
    "    return acc\n",
    "\n",
    "evaluate_model(xgb_model, X_test_scaled, y_test, \"XGBoost\")\n",
    "evaluate_model(cat_model, X_test_scaled, y_test, \"CatBoost\")\n",
    "\n",
    "# Feature Importance (XGBoost)\n",
    "plt.figure(figsize=(10, 8))\n",
    "xgb.plot_importance(xgb_model, max_num_features=20)\n",
    "plt.title(\"Top 20 Features (XGBoost)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d89a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Save Models\n",
    "print(\"Saving models...\")\n",
    "joblib.dump(xgb_model, MODEL_DIR / 'xgboost_v15.joblib')\n",
    "joblib.dump(cat_model, MODEL_DIR / 'catboost_v15.joblib')\n",
    "joblib.dump(scaler, MODEL_DIR / 'scaler_v15.joblib')\n",
    "joblib.dump(feature_cols, MODEL_DIR / 'feature_cols_v15.joblib')\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    'timeframes': TIMEFRAMES,\n",
    "    'tp_pips': TP_PIPS,\n",
    "    'sl_pips': SL_PIPS,\n",
    "    'forward_bars': FORWARD_BARS\n",
    "}\n",
    "joblib.dump(config, MODEL_DIR / 'config_v15.joblib')\n",
    "\n",
    "print(f\"Models saved to {MODEL_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
