{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a368060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸš€ FOREX SIGNAL GENERATOR V14\n",
      "   V10 Features + TP/SL Based Labeling\n",
      "======================================================================\n",
      "âœ“ GPU Available: True\n",
      "âœ“ TP: 15 pips, SL: 10 pips\n",
      "âœ“ Forward window: 60 bars\n",
      "âœ“ Model Directory: c:\\Users\\Acer\\Desktop\\Forex-Signal-App\\models\\signal_generator_v14\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import joblib\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(r'c:\\Users\\Acer\\Desktop\\Forex-Signal-App')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "MODEL_DIR = BASE_DIR / 'models' / 'signal_generator_v14'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# GPU Check\n",
    "import torch\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "# Trading Parameters\n",
    "TP_PIPS = 15\n",
    "SL_PIPS = 10\n",
    "FORWARD_BARS = 60\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸš€ FOREX SIGNAL GENERATOR V14\")\n",
    "print(\"   V10 Features + TP/SL Based Labeling\")\n",
    "print(\"=\"*70)\n",
    "print(f\"âœ“ GPU Available: {GPU_AVAILABLE}\")\n",
    "print(f\"âœ“ TP: {TP_PIPS} pips, SL: {SL_PIPS} pips\")\n",
    "print(f\"âœ“ Forward window: {FORWARD_BARS} bars\")\n",
    "print(f\"âœ“ Model Directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8198d774",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab934933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,859,492 rows\n",
      "Test: 296,778 rows\n",
      "Train period: 2019-12-31 16:00:00+00:00 to 2024-12-30 16:00:00+00:00\n",
      "Test period: 2024-12-31 16:00:00+00:00 to 2025-10-17 06:11:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "train_df = pd.read_csv(DATA_DIR / 'EUR_USD_1min.csv')\n",
    "test_df = pd.read_csv(DATA_DIR / 'EUR_USD_test.csv')\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    if 'timestamp' in df.columns:\n",
    "        df.rename(columns={'timestamp': 'time'}, inplace=True)\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows\")\n",
    "print(f\"Test: {len(test_df):,} rows\")\n",
    "print(f\"Train period: {train_df['time'].min()} to {train_df['time'].max()}\")\n",
    "print(f\"Test period: {test_df['time'].min()} to {test_df['time'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be8419",
   "metadata": {},
   "source": [
    "## 2. V10-Style Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65479545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding V14 features (V10-style)...\n",
      "âœ“ Features added. Total columns: 69\n"
     ]
    }
   ],
   "source": [
    "def add_features_v14(df):\n",
    "    \"\"\"\n",
    "    V14 Features: V10-Ð¸Ð¹Ð½ Ð±Ò¯Ñ… features + V8 composite features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ==================== CORE FEATURES ====================\n",
    "    # Time Features\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['day_of_week'] = df['time'].dt.dayofweek\n",
    "    df['is_london'] = ((df['hour'] >= 8) & (df['hour'] < 16)).astype(int)\n",
    "    df['is_ny'] = ((df['hour'] >= 13) & (df['hour'] < 21)).astype(int)\n",
    "    df['is_overlap'] = ((df['hour'] >= 13) & (df['hour'] < 16)).astype(int)\n",
    "    \n",
    "    # Moving Averages\n",
    "    for p in [5, 10, 20, 50, 200]:\n",
    "        df[f'sma_{p}'] = df['close'].rolling(p).mean()\n",
    "        df[f'ema_{p}'] = df['close'].ewm(span=p, adjust=False).mean()\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    ema12 = df['close'].ewm(span=12).mean()\n",
    "    ema26 = df['close'].ewm(span=26).mean()\n",
    "    df['macd'] = ema12 - ema26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    df['bb_mid'] = df['close'].rolling(20).mean()\n",
    "    df['bb_std'] = df['close'].rolling(20).std()\n",
    "    df['bb_upper'] = df['bb_mid'] + 2 * df['bb_std']\n",
    "    df['bb_lower'] = df['bb_mid'] - 2 * df['bb_std']\n",
    "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / (df['bb_mid'] + 1e-10)\n",
    "    \n",
    "    # ATR & ADX\n",
    "    df['tr0'] = abs(df['high'] - df['low'])\n",
    "    df['tr1'] = abs(df['high'] - df['close'].shift())\n",
    "    df['tr2'] = abs(df['low'] - df['close'].shift())\n",
    "    df['tr'] = df[['tr0', 'tr1', 'tr2']].max(axis=1)\n",
    "    \n",
    "    df['up_move'] = df['high'] - df['high'].shift()\n",
    "    df['down_move'] = df['low'].shift() - df['low']\n",
    "    \n",
    "    df['plus_dm'] = np.where((df['up_move'] > df['down_move']) & (df['up_move'] > 0), df['up_move'], 0)\n",
    "    df['minus_dm'] = np.where((df['down_move'] > df['up_move']) & (df['down_move'] > 0), df['down_move'], 0)\n",
    "    \n",
    "    period = 14\n",
    "    df['atr'] = df['tr'].rolling(period).mean()\n",
    "    df['plus_di'] = 100 * (df['plus_dm'].rolling(period).mean() / (df['atr'] + 1e-10))\n",
    "    df['minus_di'] = 100 * (df['minus_dm'].rolling(period).mean() / (df['atr'] + 1e-10))\n",
    "    df['dx'] = 100 * abs(df['plus_di'] - df['minus_di']) / (df['plus_di'] + df['minus_di'] + 1e-10)\n",
    "    df['adx'] = df['dx'].rolling(period).mean()\n",
    "    \n",
    "    # CCI\n",
    "    tp = (df['high'] + df['low'] + df['close']) / 3\n",
    "    sma_tp = tp.rolling(20).mean()\n",
    "    mad_tp = tp.rolling(20).apply(lambda x: np.abs(x - x.mean()).mean())\n",
    "    df['cci'] = (tp - sma_tp) / (0.015 * mad_tp + 1e-10)\n",
    "    \n",
    "    # Williams %R\n",
    "    hh = df['high'].rolling(14).max()\n",
    "    ll = df['low'].rolling(14).min()\n",
    "    df['williams_r'] = -100 * (hh - df['close']) / (hh - ll + 1e-10)\n",
    "    \n",
    "    # Volatility\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['volatility'] = df['returns'].rolling(20).std() * 100\n",
    "    \n",
    "    # ==================== COMPOSITE FEATURES ====================\n",
    "    df['rsi_x_adx'] = df['rsi'] * df['adx'] / 100\n",
    "    df['momentum_score'] = (\n",
    "        (df['rsi'] > 50).astype(int) + \n",
    "        (df['macd'] > df['macd_signal']).astype(int) + \n",
    "        (df['plus_di'] > df['minus_di']).astype(int)\n",
    "    )\n",
    "    df['price_position'] = (df['close'] - df['sma_50']) / (df['atr'] + 1e-10)\n",
    "    df['trend_score'] = (\n",
    "        (df['close'] > df['sma_20']).astype(int) +\n",
    "        (df['sma_20'] > df['sma_50']).astype(int) +\n",
    "        (df['sma_50'] > df['sma_200']).astype(int) +\n",
    "        (df['adx'] > 25).astype(int)\n",
    "    )\n",
    "    df['rsi_zone'] = pd.cut(df['rsi'], bins=[0, 30, 45, 55, 70, 100], labels=[0, 1, 2, 3, 4]).astype(float)\n",
    "    df['macd_momentum'] = df['macd_hist'] - df['macd_hist'].shift(3)\n",
    "    df['close_vs_high'] = (df['high'].rolling(20).max() - df['close']) / (df['atr'] + 1e-10)\n",
    "    df['close_vs_low'] = (df['close'] - df['low'].rolling(20).min()) / (df['atr'] + 1e-10)\n",
    "    \n",
    "    # ==================== V10 ENHANCED FEATURES ====================\n",
    "    # Trend Strength Score (0-5)\n",
    "    df['trend_strength'] = (\n",
    "        (df['close'] > df['ema_5']).astype(int) +\n",
    "        (df['ema_5'] > df['ema_10']).astype(int) +\n",
    "        (df['ema_10'] > df['ema_20']).astype(int) +\n",
    "        (df['ema_20'] > df['ema_50']).astype(int) +\n",
    "        (df['adx'] > 20).astype(int)\n",
    "    )\n",
    "    \n",
    "    # Momentum Alignment\n",
    "    df['momentum_alignment'] = (\n",
    "        (df['rsi'] > 55).astype(int) +\n",
    "        (df['macd_hist'] > 0).astype(int) +\n",
    "        (df['cci'] > 50).astype(int) +\n",
    "        (df['williams_r'] > -30).astype(int)\n",
    "    )\n",
    "    \n",
    "    # Volatility State\n",
    "    df['volatility_sma'] = df['volatility'].rolling(50).mean()\n",
    "    df['volatility_state'] = np.where(\n",
    "        df['volatility'] > df['volatility_sma'] * 1.5, 2,\n",
    "        np.where(df['volatility'] < df['volatility_sma'] * 0.5, 0, 1)\n",
    "    )\n",
    "    \n",
    "    # Price Action Patterns\n",
    "    df['body'] = df['close'] - df['open']\n",
    "    df['upper_wick'] = df['high'] - df[['open', 'close']].max(axis=1)\n",
    "    df['lower_wick'] = df[['open', 'close']].min(axis=1) - df['low']\n",
    "    df['body_ratio'] = abs(df['body']) / (df['high'] - df['low'] + 1e-10)\n",
    "    \n",
    "    # Bullish/Bearish patterns\n",
    "    df['is_bullish'] = (df['close'] > df['open']).astype(int)\n",
    "    df['bullish_streak'] = df['is_bullish'].rolling(5).sum()\n",
    "    \n",
    "    # Support/Resistance proximity\n",
    "    df['dist_to_high20'] = (df['high'].rolling(20).max() - df['close']) / (df['atr'] + 1e-10)\n",
    "    df['dist_to_low20'] = (df['close'] - df['low'].rolling(20).min()) / (df['atr'] + 1e-10)\n",
    "    \n",
    "    # Multi-timeframe RSI\n",
    "    df['rsi_5'] = df['rsi'].rolling(5).mean()\n",
    "    df['rsi_20'] = df['rsi'].rolling(20).mean()\n",
    "    df['rsi_trend'] = df['rsi_5'] - df['rsi_20']\n",
    "    \n",
    "    # Breakout Detection\n",
    "    df['above_bb_upper'] = (df['close'] > df['bb_upper']).astype(int)\n",
    "    df['below_bb_lower'] = (df['close'] < df['bb_lower']).astype(int)\n",
    "    df['bb_breakout'] = df['above_bb_upper'] - df['below_bb_lower']\n",
    "    \n",
    "    # Price Momentum\n",
    "    df['price_change_5'] = (df['close'] - df['close'].shift(5)) / (df['atr'] + 1e-10)\n",
    "    df['price_change_10'] = (df['close'] - df['close'].shift(10)) / (df['atr'] + 1e-10)\n",
    "    df['price_change_20'] = (df['close'] - df['close'].shift(20)) / (df['atr'] + 1e-10)\n",
    "    \n",
    "    # Session Quality\n",
    "    df['session_quality'] = df['is_london'].astype(int) + df['is_ny'].astype(int) + df['is_overlap'].astype(int) * 2\n",
    "    \n",
    "    # Cleanup temp columns\n",
    "    drop_cols = ['tr0', 'tr1', 'tr2', 'tr', 'up_move', 'down_move', 'plus_dm', 'minus_dm']\n",
    "    df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Adding V14 features (V10-style)...\")\n",
    "train_df = add_features_v14(train_df)\n",
    "test_df = add_features_v14(test_df)\n",
    "print(f\"âœ“ Features added. Total columns: {len(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64a1be",
   "metadata": {},
   "source": [
    "## 3. TP/SL Based Labeling (BUY-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "614b2595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TP/SL based labels...\n",
      "This may take ~20 minutes for large dataset...\n",
      "âœ“ Labels created in 1318.7s\n",
      "âœ“ Labels created in 1318.7s\n",
      "\n",
      "ðŸ“Š Label Distribution:\n",
      "Train: 601,304 samples\n",
      "  WIN (TP hit): 191,049 (31.8%)\n",
      "  LOSE (SL hit): 410,255 (68.2%)\n",
      "\n",
      "Test: 121,253 samples\n",
      "  WIN (TP hit): 40,811 (33.7%)\n",
      "  LOSE (SL hit): 80,442 (66.3%)\n",
      "\n",
      "ðŸ“Š Label Distribution:\n",
      "Train: 601,304 samples\n",
      "  WIN (TP hit): 191,049 (31.8%)\n",
      "  LOSE (SL hit): 410,255 (68.2%)\n",
      "\n",
      "Test: 121,253 samples\n",
      "  WIN (TP hit): 40,811 (33.7%)\n",
      "  LOSE (SL hit): 80,442 (66.3%)\n"
     ]
    }
   ],
   "source": [
    "def create_tpsl_labels(df, tp_pips=15, sl_pips=10, forward_bars=60):\n",
    "    \"\"\"\n",
    "    V14 TP/SL Based Labeling for BUY signals:\n",
    "    - Label = 1 (WIN): TP hit before SL\n",
    "    - Label = 0 (LOSE): SL hit before TP\n",
    "    - Label = -1 (NO_TRADE): Neither hit within forward_bars\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    tp_price = tp_pips * 0.0001\n",
    "    sl_price = sl_pips * 0.0001\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if i + forward_bars >= len(df):\n",
    "            labels.append(-1)\n",
    "            continue\n",
    "            \n",
    "        entry_price = df.iloc[i]['close']\n",
    "        tp_level = entry_price + tp_price\n",
    "        sl_level = entry_price - sl_price\n",
    "        \n",
    "        future = df.iloc[i+1:i+1+forward_bars]\n",
    "        \n",
    "        tp_hit_idx = None\n",
    "        sl_hit_idx = None\n",
    "        \n",
    "        tp_hits = future[future['high'] >= tp_level]\n",
    "        if len(tp_hits) > 0:\n",
    "            tp_hit_idx = tp_hits.index[0]\n",
    "        \n",
    "        sl_hits = future[future['low'] <= sl_level]\n",
    "        if len(sl_hits) > 0:\n",
    "            sl_hit_idx = sl_hits.index[0]\n",
    "        \n",
    "        if tp_hit_idx is not None and sl_hit_idx is not None:\n",
    "            if tp_hit_idx < sl_hit_idx:\n",
    "                labels.append(1)  # WIN\n",
    "            else:\n",
    "                labels.append(0)  # LOSE\n",
    "        elif tp_hit_idx is not None:\n",
    "            labels.append(1)  # WIN\n",
    "        elif sl_hit_idx is not None:\n",
    "            labels.append(0)  # LOSE\n",
    "        else:\n",
    "            labels.append(-1)  # NO_TRADE\n",
    "    \n",
    "    df['signal'] = labels\n",
    "    return df\n",
    "\n",
    "print(\"Creating TP/SL based labels...\")\n",
    "print(\"This may take ~20 minutes for large dataset...\")\n",
    "\n",
    "start = time.time()\n",
    "train_df = create_tpsl_labels(train_df, TP_PIPS, SL_PIPS, FORWARD_BARS)\n",
    "test_df = create_tpsl_labels(test_df, TP_PIPS, SL_PIPS, FORWARD_BARS)\n",
    "print(f\"âœ“ Labels created in {time.time()-start:.1f}s\")\n",
    "\n",
    "# Filter only WIN/LOSE\n",
    "train_labeled = train_df[train_df['signal'] != -1].copy()\n",
    "test_labeled = test_df[test_df['signal'] != -1].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š Label Distribution:\")\n",
    "print(f\"Train: {len(train_labeled):,} samples\")\n",
    "print(f\"  WIN (TP hit): {(train_labeled['signal']==1).sum():,} ({(train_labeled['signal']==1).mean()*100:.1f}%)\")\n",
    "print(f\"  LOSE (SL hit): {(train_labeled['signal']==0).sum():,} ({(train_labeled['signal']==0).mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest: {len(test_labeled):,} samples\")\n",
    "print(f\"  WIN (TP hit): {(test_labeled['signal']==1).sum():,} ({(test_labeled['signal']==1).mean()*100:.1f}%)\")\n",
    "print(f\"  LOSE (SL hit): {(test_labeled['signal']==0).sum():,} ({(test_labeled['signal']==0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6916aa2",
   "metadata": {},
   "source": [
    "## 4. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2604b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Features: 63\n",
      "âœ“ X_train: (601167, 63)\n",
      "âœ“ X_test: (121231, 63)\n",
      "âœ“ y_train WIN ratio: 31.8%\n",
      "âœ“ y_test WIN ratio: 33.7%\n",
      "âœ“ X_train: (601167, 63)\n",
      "âœ“ X_test: (121231, 63)\n",
      "âœ“ y_train WIN ratio: 31.8%\n",
      "âœ“ y_test WIN ratio: 33.7%\n"
     ]
    }
   ],
   "source": [
    "# Clean NaN\n",
    "train_labeled = train_labeled.dropna()\n",
    "test_labeled = test_labeled.dropna()\n",
    "\n",
    "# Define feature columns\n",
    "exclude_cols = ['time', 'signal', 'open', 'high', 'low', 'close', 'volume', 'tick_volume']\n",
    "feature_cols = [c for c in train_labeled.columns if c not in exclude_cols]\n",
    "feature_cols = [c for c in feature_cols if train_labeled[c].dtype in ['float64', 'int64', 'float32', 'int32']]\n",
    "\n",
    "print(f\"ðŸ“Š Features: {len(feature_cols)}\")\n",
    "\n",
    "X_train = train_labeled[feature_cols].values\n",
    "y_train = train_labeled['signal'].values\n",
    "X_test = test_labeled[feature_cols].values\n",
    "y_test = test_labeled['signal'].values\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"âœ“ X_train: {X_train_scaled.shape}\")\n",
    "print(f\"âœ“ X_test: {X_test_scaled.shape}\")\n",
    "print(f\"âœ“ y_train WIN ratio: {y_train.mean()*100:.1f}%\")\n",
    "print(f\"âœ“ y_test WIN ratio: {y_test.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afac20a",
   "metadata": {},
   "source": [
    "## 5. Train V14 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "771f006d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸš€ TRAINING V14 MODELS\n",
      "   V10 Features + TP/SL Labeling + Regularization\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Creating XGBoost models...\n",
      "ðŸ“Š Creating LightGBM models...\n",
      "ðŸ“Š Creating CatBoost models...\n",
      "\n",
      "âœ“ Created 7 models\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸš€ TRAINING V14 MODELS\")\n",
    "print(\"   V10 Features + TP/SL Labeling + Regularization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models = {}\n",
    "\n",
    "# XGBoost - Medium Regularization\n",
    "print(\"\\nðŸ“Š Creating XGBoost models...\")\n",
    "models['xgb1'] = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.02,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=2.0,\n",
    "    min_child_weight=5,\n",
    "    gamma=0.2,\n",
    "    tree_method='hist',\n",
    "    device='cuda',\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=50,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "models['xgb2'] = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_alpha=1.0,\n",
    "    reg_lambda=5.0,\n",
    "    min_child_weight=10,\n",
    "    gamma=0.5,\n",
    "    tree_method='hist',\n",
    "    device='cuda',\n",
    "    random_state=43,\n",
    "    early_stopping_rounds=50,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "models['xgb3'] = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.01,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.3,\n",
    "    reg_lambda=1.0,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.1,\n",
    "    tree_method='hist',\n",
    "    device='cuda',\n",
    "    random_state=44,\n",
    "    early_stopping_rounds=50,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# LightGBM\n",
    "print(\"ðŸ“Š Creating LightGBM models...\")\n",
    "models['lgb1'] = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.02,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=2.0,\n",
    "    min_child_samples=30,\n",
    "    device='gpu',\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "models['lgb2'] = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_alpha=1.0,\n",
    "    reg_lambda=5.0,\n",
    "    min_child_samples=50,\n",
    "    device='gpu',\n",
    "    random_state=43,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# CatBoost\n",
    "print(\"ðŸ“Š Creating CatBoost models...\")\n",
    "models['cat1'] = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    depth=5,\n",
    "    learning_rate=0.02,\n",
    "    l2_leaf_reg=5,\n",
    "    min_data_in_leaf=30,\n",
    "    task_type='GPU',\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "models['cat2'] = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    depth=4,\n",
    "    learning_rate=0.03,\n",
    "    l2_leaf_reg=10,\n",
    "    min_data_in_leaf=50,\n",
    "    task_type='GPU',\n",
    "    random_state=43,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Created {len(models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37825c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Training xgb1...\n",
      "   Train: 68.3%, Test: 66.3%, Gap: +1.9% âœ“\n",
      "   Time: 4.1s\n",
      "\n",
      "ðŸ”§ Training xgb2...\n",
      "   Train: 68.3%, Test: 66.3%, Gap: +1.9% âœ“\n",
      "   Time: 4.1s\n",
      "\n",
      "ðŸ”§ Training xgb2...\n",
      "   Train: 68.2%, Test: 66.3%, Gap: +1.9% âœ“\n",
      "   Time: 2.2s\n",
      "\n",
      "ðŸ”§ Training xgb3...\n",
      "   Train: 68.2%, Test: 66.3%, Gap: +1.9% âœ“\n",
      "   Time: 2.2s\n",
      "\n",
      "ðŸ”§ Training xgb3...\n",
      "   Train: 68.3%, Test: 66.3%, Gap: +1.9% âœ“\n",
      "   Time: 3.5s\n",
      "\n",
      "ðŸ”§ Training lgb1...\n",
      "   Train: 68.3%, Test: 66.3%, Gap: +1.9% âœ“\n",
      "   Time: 3.5s\n",
      "\n",
      "ðŸ”§ Training lgb1...\n",
      "   Train: 68.5%, Test: 66.4%, Gap: +2.1% âœ“\n",
      "   Time: 10.1s\n",
      "\n",
      "ðŸ”§ Training lgb2...\n",
      "   Train: 68.5%, Test: 66.4%, Gap: +2.1% âœ“\n",
      "   Time: 10.1s\n",
      "\n",
      "ðŸ”§ Training lgb2...\n",
      "   Train: 68.4%, Test: 66.4%, Gap: +2.0% âœ“\n",
      "   Time: 4.1s\n",
      "\n",
      "ðŸ”§ Training cat1...\n",
      "   Train: 68.4%, Test: 66.4%, Gap: +2.0% âœ“\n",
      "   Time: 4.1s\n",
      "\n",
      "ðŸ”§ Training cat1...\n",
      "   Train: 68.3%, Test: 66.3%, Gap: +1.9% âœ“\n",
      "   Time: 7.1s\n",
      "\n",
      "ðŸ”§ Training cat2...\n",
      "   Train: 68.3%, Test: 66.3%, Gap: +1.9% âœ“\n",
      "   Time: 7.1s\n",
      "\n",
      "ðŸ”§ Training cat2...\n",
      "   Train: 68.3%, Test: 66.3%, Gap: +1.9% âœ“\n",
      "   Time: 3.9s\n",
      "\n",
      "âœ“ Trained 7 models\n",
      "   Train: 68.3%, Test: 66.3%, Gap: +1.9% âœ“\n",
      "   Time: 3.9s\n",
      "\n",
      "âœ“ Trained 7 models\n"
     ]
    }
   ],
   "source": [
    "# Train models\n",
    "trained_models = {}\n",
    "accuracies = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nðŸ”§ Training {name}...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        if 'xgb' in name:\n",
    "            model.fit(\n",
    "                X_train_scaled, y_train,\n",
    "                eval_set=[(X_test_scaled, y_test)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        train_time = time.time() - start\n",
    "        \n",
    "        # Evaluate\n",
    "        train_pred = model.predict(X_train_scaled)\n",
    "        test_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        test_acc = accuracy_score(y_test, test_pred)\n",
    "        gap = train_acc - test_acc\n",
    "        \n",
    "        trained_models[name] = model\n",
    "        accuracies[name] = test_acc\n",
    "        \n",
    "        overfit = \"âš ï¸\" if gap > 0.05 else \"âœ“\"\n",
    "        print(f\"   Train: {train_acc*100:.1f}%, Test: {test_acc*100:.1f}%, Gap: {gap*100:+.1f}% {overfit}\")\n",
    "        print(f\"   Time: {train_time:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error: {e}\")\n",
    "\n",
    "print(f\"\\nâœ“ Trained {len(trained_models)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b4aa0",
   "metadata": {},
   "source": [
    "## 6. Ensemble & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d77ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ“Š ENSEMBLE EVALUATION\n",
      "======================================================================\n",
      "\n",
      "Model Weights:\n",
      "  lgb2: 0.143\n",
      "  lgb1: 0.143\n",
      "  xgb1: 0.143\n",
      "  xgb3: 0.143\n",
      "  xgb2: 0.143\n",
      "  cat1: 0.143\n",
      "  cat2: 0.143\n",
      "\n",
      "Confidence Distribution:\n",
      "  >= 90%: 0\n",
      "  >= 80%: 0\n",
      "  >= 70%: 0\n",
      "  >= 60%: 0\n",
      "\n",
      "Confidence Distribution:\n",
      "  >= 90%: 0\n",
      "  >= 80%: 0\n",
      "  >= 70%: 0\n",
      "  >= 60%: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ“Š ENSEMBLE EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate weights based on accuracy\n",
    "total_acc = sum(accuracies.values())\n",
    "weights = {name: acc/total_acc for name, acc in accuracies.items()}\n",
    "\n",
    "print(\"\\nModel Weights:\")\n",
    "for name, w in sorted(weights.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {name}: {w:.3f}\")\n",
    "\n",
    "# Get predictions\n",
    "proba_dict = {}\n",
    "preds_dict = {}\n",
    "for name, model in trained_models.items():\n",
    "    proba_dict[name] = model.predict_proba(X_test_scaled)\n",
    "    preds_dict[name] = model.predict(X_test_scaled)\n",
    "\n",
    "# Weighted ensemble probability\n",
    "ensemble_proba = np.zeros((len(X_test_scaled), 2))\n",
    "for name, proba in proba_dict.items():\n",
    "    ensemble_proba += weights[name] * proba\n",
    "\n",
    "# WIN probability\n",
    "win_prob = ensemble_proba[:, 1] * 100\n",
    "\n",
    "# Agreement bonus (V10 style)\n",
    "all_preds = np.array([preds_dict[name] for name in trained_models.keys()])\n",
    "win_votes = np.sum(all_preds == 1, axis=0)\n",
    "\n",
    "all_agree = win_votes == len(trained_models)\n",
    "strong_agree = win_votes >= (len(trained_models) - 1)\n",
    "majority = win_votes >= (len(trained_models) // 2 + 1)\n",
    "\n",
    "confidence = win_prob.copy()\n",
    "confidence[all_agree] = np.minimum(confidence[all_agree] + 7, 100)  # +7 for all agree\n",
    "confidence[strong_agree & ~all_agree] = np.minimum(confidence[strong_agree & ~all_agree] + 4, 100)\n",
    "confidence[majority & ~strong_agree] = np.minimum(confidence[majority & ~strong_agree] + 2, 100)\n",
    "\n",
    "print(f\"\\nConfidence Distribution:\")\n",
    "print(f\"  >= 90%: {(confidence >= 90).sum():,}\")\n",
    "print(f\"  >= 80%: {(confidence >= 80).sum():,}\")\n",
    "print(f\"  >= 70%: {(confidence >= 70).sum():,}\")\n",
    "print(f\"  >= 60%: {(confidence >= 60).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9b69b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ V14 Results by Threshold:\n",
      "--------------------------------------------------------------------------------\n",
      " Threshold |    Signals |       Wins |   Win Rate |         PF\n",
      "--------------------------------------------------------------------------------\n",
      "      30%+ |     84,598 |     31,403 |      37.1% |      0.89 \n",
      "      35%+ |     46,313 |     18,571 |      40.1% |      1.00 ðŸŽ¯\n",
      "      40%+ |      5,330 |      2,290 |      43.0% |      1.13 ðŸŽ¯\n",
      "      45%+ |        161 |        102 |      63.4% |      2.59 ðŸŽ¯\n",
      "      50%+ |         27 |         14 |      51.9% |      1.62 ðŸŽ¯\n",
      "      55%+ |          0 |          - |          - |          -\n",
      "      60%+ |          0 |          - |          - |          -\n",
      "\n",
      "ðŸ“Š Raw WIN Probability Distribution:\n",
      "  >= 50%: 23\n",
      "  >= 45%: 161\n",
      "  >= 40%: 5,330\n",
      "  >= 35%: 46,313\n",
      "  >= 30%: 84,598\n"
     ]
    }
   ],
   "source": [
    "# Results by threshold - starting from lower values\n",
    "print(\"\\nðŸ“ˆ V14 Results by Threshold:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Threshold':>10} | {'Signals':>10} | {'Wins':>10} | {'Win Rate':>10} | {'PF':>10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "v14_results = {}\n",
    "for threshold in [30, 35, 40, 45, 50, 55, 60]:\n",
    "    mask = confidence >= threshold\n",
    "    if mask.sum() > 0:\n",
    "        signals = mask.sum()\n",
    "        wins = y_test[mask].sum()\n",
    "        losses = signals - wins\n",
    "        win_rate = wins / signals * 100\n",
    "        pf = (wins * TP_PIPS) / max(1, losses * SL_PIPS)\n",
    "        \n",
    "        v14_results[threshold] = {\n",
    "            'signals': signals,\n",
    "            'wins': wins,\n",
    "            'losses': losses,\n",
    "            'win_rate': win_rate,\n",
    "            'pf': pf\n",
    "        }\n",
    "        \n",
    "        marker = \"ðŸŽ¯\" if win_rate >= 40 else \"\"\n",
    "        print(f\"{threshold:>8}%+ | {signals:>10,} | {wins:>10,} | {win_rate:>9.1f}% | {pf:>9.2f} {marker}\")\n",
    "    else:\n",
    "        print(f\"{threshold:>8}%+ | {'0':>10} | {'-':>10} | {'-':>10} | {'-':>10}\")\n",
    "\n",
    "# Also show raw probability distribution\n",
    "print(\"\\nðŸ“Š Raw WIN Probability Distribution:\")\n",
    "print(f\"  >= 50%: {(win_prob >= 50).sum():,}\")\n",
    "print(f\"  >= 45%: {(win_prob >= 45).sum():,}\")\n",
    "print(f\"  >= 40%: {(win_prob >= 40).sum():,}\")\n",
    "print(f\"  >= 35%: {(win_prob >= 35).sum():,}\")\n",
    "print(f\"  >= 30%: {(win_prob >= 30).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d678de33",
   "metadata": {},
   "source": [
    "## 7. Overfit Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc3c03f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ” OVERFIT CHECK: Train vs Test\n",
      "======================================================================\n",
      "\n",
      " Threshold |  Train Sig |   Train WR |   Test Sig |    Test WR |      Gap\n",
      "--------------------------------------------------------------------------------\n",
      "      30%+ |    392,873 |      38.2% |     84,598 |      37.1% |    +1.1% âœ“\n",
      "      35%+ |    212,981 |      43.6% |     46,313 |      40.1% |    +3.5% âœ“\n",
      "      40%+ |     37,019 |      57.8% |      5,330 |      43.0% |   +14.8% âš ï¸\n",
      "      45%+ |      1,699 |      82.0% |        161 |      63.4% |   +18.6% âš ï¸\n",
      "      50%+ |        231 |      97.4% |         27 |      51.9% |   +45.6% âš ï¸\n",
      "\n",
      " Threshold |  Train Sig |   Train WR |   Test Sig |    Test WR |      Gap\n",
      "--------------------------------------------------------------------------------\n",
      "      30%+ |    392,873 |      38.2% |     84,598 |      37.1% |    +1.1% âœ“\n",
      "      35%+ |    212,981 |      43.6% |     46,313 |      40.1% |    +3.5% âœ“\n",
      "      40%+ |     37,019 |      57.8% |      5,330 |      43.0% |   +14.8% âš ï¸\n",
      "      45%+ |      1,699 |      82.0% |        161 |      63.4% |   +18.6% âš ï¸\n",
      "      50%+ |        231 |      97.4% |         27 |      51.9% |   +45.6% âš ï¸\n"
     ]
    }
   ],
   "source": [
    "# Check train vs test\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ” OVERFIT CHECK: Train vs Test\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get train predictions\n",
    "train_proba_dict = {}\n",
    "train_preds_dict = {}\n",
    "for name, model in trained_models.items():\n",
    "    train_proba_dict[name] = model.predict_proba(X_train_scaled)\n",
    "    train_preds_dict[name] = model.predict(X_train_scaled)\n",
    "\n",
    "train_ensemble = np.zeros((len(X_train_scaled), 2))\n",
    "for name, proba in train_proba_dict.items():\n",
    "    train_ensemble += weights[name] * proba\n",
    "\n",
    "train_win_prob = train_ensemble[:, 1] * 100\n",
    "\n",
    "# Agreement\n",
    "train_all_preds = np.array([train_preds_dict[name] for name in trained_models.keys()])\n",
    "train_votes = np.sum(train_all_preds == 1, axis=0)\n",
    "train_all_agree = train_votes == len(trained_models)\n",
    "train_strong = train_votes >= (len(trained_models) - 1)\n",
    "train_majority = train_votes >= (len(trained_models) // 2 + 1)\n",
    "\n",
    "train_conf = train_win_prob.copy()\n",
    "train_conf[train_all_agree] = np.minimum(train_conf[train_all_agree] + 7, 100)\n",
    "train_conf[train_strong & ~train_all_agree] = np.minimum(train_conf[train_strong & ~train_all_agree] + 4, 100)\n",
    "train_conf[train_majority & ~train_strong] = np.minimum(train_conf[train_majority & ~train_strong] + 2, 100)\n",
    "\n",
    "print(f\"\\n{'Threshold':>10} | {'Train Sig':>10} | {'Train WR':>10} | {'Test Sig':>10} | {'Test WR':>10} | {'Gap':>8}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for threshold in [30, 35, 40, 45, 50]:\n",
    "    train_mask = train_conf >= threshold\n",
    "    test_mask = confidence >= threshold\n",
    "    \n",
    "    train_sig = train_mask.sum()\n",
    "    test_sig = test_mask.sum()\n",
    "    \n",
    "    train_wr = y_train[train_mask].mean() * 100 if train_sig > 0 else 0\n",
    "    test_wr = y_test[test_mask].mean() * 100 if test_sig > 0 else 0\n",
    "    \n",
    "    gap = train_wr - test_wr\n",
    "    status = \"âœ“\" if abs(gap) < 5 else (\"âš¡\" if abs(gap) < 10 else \"âš ï¸\")\n",
    "    \n",
    "    print(f\"{threshold:>8}%+ | {train_sig:>10,} | {train_wr:>9.1f}% | {test_sig:>10,} | {test_wr:>9.1f}% | {gap:>+7.1f}% {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42bd8ed",
   "metadata": {},
   "source": [
    "## 8. Save V14 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaabaf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ’¾ SAVING V14 MODELS\n",
      "======================================================================\n",
      "âœ“ xgb1_v14.joblib\n",
      "âœ“ xgb2_v14.joblib\n",
      "âœ“ xgb3_v14.joblib\n",
      "âœ“ lgb1_v14.joblib\n",
      "âœ“ lgb2_v14.joblib\n",
      "âœ“ cat1_v14.joblib\n",
      "âœ“ cat2_v14.joblib\n",
      "âœ“ scaler_v14.joblib\n",
      "âœ“ feature_cols_v14.joblib\n",
      "âœ“ weights_v14.joblib\n",
      "âœ“ config_v14.joblib\n",
      "\n",
      "âœ… All V14 models saved to: c:\\Users\\Acer\\Desktop\\Forex-Signal-App\\models\\signal_generator_v14\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ’¾ SAVING V14 MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save models\n",
    "for name, model in trained_models.items():\n",
    "    joblib.dump(model, MODEL_DIR / f'{name}_v14.joblib')\n",
    "    print(f\"âœ“ {name}_v14.joblib\")\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, MODEL_DIR / 'scaler_v14.joblib')\n",
    "print(f\"âœ“ scaler_v14.joblib\")\n",
    "\n",
    "# Save features\n",
    "joblib.dump(feature_cols, MODEL_DIR / 'feature_cols_v14.joblib')\n",
    "print(f\"âœ“ feature_cols_v14.joblib\")\n",
    "\n",
    "# Save weights\n",
    "joblib.dump(weights, MODEL_DIR / 'weights_v14.joblib')\n",
    "print(f\"âœ“ weights_v14.joblib\")\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    'version': 'v14',\n",
    "    'description': 'V10 Features + TP/SL Labeling',\n",
    "    'labeling': 'TP/SL Based',\n",
    "    'tp_pips': TP_PIPS,\n",
    "    'sl_pips': SL_PIPS,\n",
    "    'forward_bars': FORWARD_BARS,\n",
    "    'models': list(trained_models.keys()),\n",
    "    'features': len(feature_cols),\n",
    "    'accuracies': accuracies,\n",
    "    'results': v14_results\n",
    "}\n",
    "joblib.dump(config, MODEL_DIR / 'config_v14.joblib')\n",
    "print(f\"âœ“ config_v14.joblib\")\n",
    "\n",
    "print(f\"\\nâœ… All V14 models saved to: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe37f66",
   "metadata": {},
   "source": [
    "## 9. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aafb9382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸŽ¯ V14 FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š TRAINING SETUP:\n",
      "   Features: V10-style (63 features)\n",
      "   Labeling: TP/SL Based (TP=15 pips, SL=10 pips)\n",
      "   Models: 7 (xgb1, xgb2, xgb3, lgb1, lgb2, cat1, cat2)\n",
      "   Regularization: Medium (V10 style)\n",
      "\n",
      "ðŸ“ˆ BEST RESULT (Threshold 45%+):\n",
      "   Signals: 161\n",
      "   Wins: 102\n",
      "   Losses: 59\n",
      "   Win Rate: 63.4%\n",
      "   Profit Factor: 2.59\n",
      "\n",
      "ðŸ’° EXPECTED PROFIT (trades @ $1/pip):\n",
      "   Gross Profit: $1,530\n",
      "   Gross Loss: $590\n",
      "   Net Profit: $940\n",
      "\n",
      "================================================================================\n",
      "âœ… V14 TRAINING COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸŽ¯ V14 FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best threshold\n",
    "valid_results = {k: v for k, v in v14_results.items() if v['signals'] >= 100}\n",
    "if valid_results:\n",
    "    best_th = max(valid_results.keys(), key=lambda x: valid_results[x]['pf'])\n",
    "    best = v14_results[best_th]\n",
    "else:\n",
    "    best_th = min(v14_results.keys())\n",
    "    best = v14_results[best_th]\n",
    "\n",
    "print(f\"\"\"\n",
    "ðŸ“Š TRAINING SETUP:\n",
    "   Features: V10-style ({len(feature_cols)} features)\n",
    "   Labeling: TP/SL Based (TP={TP_PIPS} pips, SL={SL_PIPS} pips)\n",
    "   Models: {len(trained_models)} ({', '.join(trained_models.keys())})\n",
    "   Regularization: Medium (V10 style)\n",
    "\n",
    "ðŸ“ˆ BEST RESULT (Threshold {best_th}%+):\n",
    "   Signals: {best['signals']:,}\n",
    "   Wins: {best['wins']:,}\n",
    "   Losses: {best['losses']:,}\n",
    "   Win Rate: {best['win_rate']:.1f}%\n",
    "   Profit Factor: {best['pf']:.2f}\n",
    "\n",
    "ðŸ’° EXPECTED PROFIT (trades @ $1/pip):\n",
    "   Gross Profit: ${int(best['wins'] * TP_PIPS):,}\n",
    "   Gross Loss: ${int(best['losses'] * SL_PIPS):,}\n",
    "   Net Profit: ${int(best['wins'] * TP_PIPS - best['losses'] * SL_PIPS):,}\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"âœ… V14 TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
