{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8931b74c",
   "metadata": {},
   "source": [
    "# forex_signal_v28 — PF-first Top-down Multi‑TF Ensemble (4H/1H → 15m → 1m)\n",
    "Энэ notebook нь Profit Factor (PF)-ийг өсгөхөд чиглэсэн **шаталсан (hierarchical) ensemble** архитектуртай.\n",
    "\n",
    "**Гол зарчим:**\n",
    "- **Decision:** 15m candle close(t) дээр шийдвэр гаргана\n",
    "- **Execution:** 15m дараагийн candle open(t+1) дээр орно (lookahead хамгаалалт)\n",
    "- **Macro gate:** 4H/1H дээр зөвхөн *зөв орчинд* trade зөвшөөрнө\n",
    "- **Edge model:** 15m дээр XGB (эсвэл fallback)\n",
    "- **Entry filter:** 1m aggregated дээр Logistic Regression\n",
    "- **Backtest:** spread/slippage/commission + 1% risk sizing + TP/SL intrabar worst-case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfcdf846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAS_XGB: True\n",
      "Config loaded. CFG(train_dir='../data/train', test_dir='../data/test', master_tf='15m', entry_tf='1m', macro_tfs=('1h', '4h'), pip_size=0.0001, usd_per_pip_per_lot=10.0, starting_equity=10000.0, spread_pips=1.0, slippage_pips=0.2, commission_per_lot_usd=0.0, risk_per_trade=0.01, atr_period=14, tp_atr=2.0, sl_atr=1.5, max_hold_bars=16, edge_th=0.62, entry_th=0.55)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 0) Imports & Config =====\n",
    "import os, glob, math, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# XGBoost (requested). If not installed, we'll fallback to sklearn HistGradientBoosting.\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "    from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    # directories (stepped up from ml_models_train/)\n",
    "    train_dir: str = \"../data/train\"\n",
    "    test_dir: str  = \"../data/test\"\n",
    "\n",
    "    # timeframes (PF-first selection)\n",
    "    master_tf: str = \"15m\"      # trade decision timeframe\n",
    "    entry_tf: str  = \"1m\"       # entry quality timeframe\n",
    "    macro_tfs: tuple = (\"1h\",\"4h\")\n",
    "\n",
    "    # costs / execution\n",
    "    pip_size: float = 0.0001            # EURUSD\n",
    "    usd_per_pip_per_lot: float = 10.0   # 1.0 lot ~ $10/pip for EURUSD\n",
    "    starting_equity: float = 10_000.0\n",
    "\n",
    "    # realistic costs (tune to your broker)\n",
    "    spread_pips: float = 1.0\n",
    "    slippage_pips: float = 0.2\n",
    "    commission_per_lot_usd: float = 0.0 # set if your broker charges commission (round-trip below)\n",
    "\n",
    "    # risk & exits (we will tune TP/SL multipliers on validation)\n",
    "    risk_per_trade: float = 0.01\n",
    "    atr_period: int = 14\n",
    "    tp_atr: float = 2.0\n",
    "    sl_atr: float = 1.5\n",
    "    max_hold_bars: int = 16   # 16*15m = 4h\n",
    "\n",
    "    # thresholds (to tune)\n",
    "    edge_th: float = 0.62\n",
    "    entry_th: float = 0.55\n",
    "\n",
    "cfg = CFG()\n",
    "print(\"HAS_XGB:\", HAS_XGB)\n",
    "print(\"Config loaded.\", cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623697db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15m  rows=224,382  file=EURUSD_m15.csv\n",
      " 4h  rows=14,498  file=EURUSD_h4.csv\n",
      " 1m  rows=3,354,904  file=EURUSD_m1.csv\n",
      " 1h  rows=56,098  file=EURUSD_h1.csv\n",
      "15m  rows=49,807  file=EURUSD_m15.csv\n",
      " 4h  rows=3,220  file=EURUSD_h4.csv\n",
      " 1m  rows=743,476  file=EURUSD_m1.csv\n",
      " 1h  rows=12,454  file=EURUSD_h1.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 1) Robust OHLCV loader (handles filename/column variations) =====\n",
    "import re\n",
    "\n",
    "def _find_csv_by_tf(folder: str, tf: str) -> str:\n",
    "    # Handle m15 vs 15m\n",
    "    patterns = [\n",
    "        f\"*{tf}*.csv\",\n",
    "        f\"*{tf.upper()}*.csv\",\n",
    "        f\"*{tf.lower()}*.csv\",\n",
    "    ]\n",
    "    # If tf=\"15m\", also look for \"m15\"\n",
    "    if len(tf) > 1 and tf[0].isdigit() and tf[-1].isalpha():\n",
    "        rev = f\"{tf[-1]}{tf[:-1]}\" # 15m -> m15\n",
    "        patterns.append(f\"*{rev}*.csv\")\n",
    "        patterns.append(f\"*{rev.upper()}*.csv\")\n",
    "        patterns.append(f\"*{rev.lower()}*.csv\")\n",
    "        \n",
    "    files = []\n",
    "    for pat in patterns:\n",
    "        files.extend(glob.glob(os.path.join(folder, pat)))\n",
    "    files = sorted(set(files))\n",
    "    \n",
    "    if not files:\n",
    "        # Fallback: fuzzy match using regex if nothing found\n",
    "        # (e.g. if file is EURUSD-2020-2023-15m.csv)\n",
    "        all_csvs = glob.glob(os.path.join(folder, \"*.csv\"))\n",
    "        for f in all_csvs:\n",
    "            bn = os.path.basename(f).lower()\n",
    "            # check if both number and unit present (simple check)\n",
    "            if tf in bn:\n",
    "                files.append(f)\n",
    "                \n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSV found for tf='{tf}' in {folder}. Tried {patterns}\")\n",
    "        \n",
    "    # prefer shortest name (often cleanest)\n",
    "    return sorted(files, key=lambda x: len(os.path.basename(x)))[0]\n",
    "\n",
    "def load_ohlcv(folder: str, tf: str) -> pd.DataFrame:\n",
    "    path = _find_csv_by_tf(folder, tf)\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    def pick(cands):\n",
    "        for c in cands:\n",
    "            if c in cols: return cols[c]\n",
    "        return None\n",
    "\n",
    "    ts = pick([\"timestamp\",\"time\",\"date\",\"datetime\"])\n",
    "    o  = pick([\"open\",\"o\"])\n",
    "    h  = pick([\"high\",\"h\"])\n",
    "    l  = pick([\"low\",\"l\"])\n",
    "    c  = pick([\"close\",\"c\"])\n",
    "    v  = pick([\"volume\",\"vol\",\"tick_volume\"])\n",
    "\n",
    "    if any(x is None for x in [ts,o,h,l,c]):\n",
    "        # Fallback: if 'time' is missing but we have 5 cols, assume they are T,O,H,L,C\n",
    "        if len(df.columns) >= 5 and ts is None:\n",
    "            # blind guess\n",
    "            col_list = df.columns.tolist()\n",
    "            print(f\"Warning: guessing columns for {path}: {col_list[:5]}\")\n",
    "            ts, o, h, l, c = col_list[0], col_list[1], col_list[2], col_list[3], col_list[4]\n",
    "        else:\n",
    "            raise ValueError(f\"Missing required columns in {path}. Have: {df.columns.tolist()}\")\n",
    "\n",
    "    out = df[[ts,o,h,l,c] + ([v] if v else [])].copy()\n",
    "    out.columns = [\"timestamp\",\"open\",\"high\",\"low\",\"close\"] + ([\"volume\"] if v else [])\n",
    "    \n",
    "    # Auto-detect format if possible, else standard mixed\n",
    "    out[\"timestamp\"] = pd.to_datetime(out[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "    out = out.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    for col in [\"open\",\"high\",\"low\",\"close\"] + ([\"volume\"] if \"volume\" in out.columns else []):\n",
    "        out[col] = pd.to_numeric(out[col], errors=\"coerce\")\n",
    "    out = out.dropna(subset=[\"open\",\"high\",\"low\",\"close\"]).reset_index(drop=True)\n",
    "\n",
    "    out.attrs[\"source_path\"] = path\n",
    "    return out\n",
    "\n",
    "def load_bundle(folder: str):\n",
    "    bundle = {}\n",
    "    for tf in set([cfg.master_tf, cfg.entry_tf, *cfg.macro_tfs]):\n",
    "        try:\n",
    "            bundle[tf] = load_ohlcv(folder, tf)\n",
    "            print(f\"{tf:>3}  rows={len(bundle[tf]):,}  file={os.path.basename(bundle[tf].attrs['source_path'])}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: {tf} not found in {folder}. Strategy might fail if this is essential.\")\n",
    "    return bundle\n",
    "\n",
    "train_raw = load_bundle(cfg.train_dir)\n",
    "test_raw  = load_bundle(cfg.test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ab75e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== 2) Technical indicators (fast, dependency-free) =====\n",
    "def ema(s: pd.Series, span: int) -> pd.Series:\n",
    "    return s.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "def rsi(close: pd.Series, period: int = 14) -> pd.Series:\n",
    "    delta = close.diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    dn = (-delta).clip(lower=0)\n",
    "    rs = up.ewm(alpha=1/period, adjust=False).mean() / (dn.ewm(alpha=1/period, adjust=False).mean() + 1e-12)\n",
    "    return 100 - (100/(1+rs))\n",
    "\n",
    "def true_range(df: pd.DataFrame) -> pd.Series:\n",
    "    prev = df[\"close\"].shift(1)\n",
    "    tr = pd.concat([\n",
    "        (df[\"high\"] - df[\"low\"]),\n",
    "        (df[\"high\"] - prev).abs(),\n",
    "        (df[\"low\"] - prev).abs()\n",
    "    ], axis=1).max(axis=1)\n",
    "    return tr\n",
    "\n",
    "def atr(df: pd.DataFrame, period: int = 14) -> pd.Series:\n",
    "    return true_range(df).ewm(alpha=1/period, adjust=False).mean()\n",
    "\n",
    "def adx(df: pd.DataFrame, period: int = 14) -> pd.Series:\n",
    "    up = df[\"high\"].diff()\n",
    "    dn = -df[\"low\"].diff()\n",
    "\n",
    "    plus_dm = np.where((up > dn) & (up > 0), up, 0.0)\n",
    "    minus_dm = np.where((dn > up) & (dn > 0), dn, 0.0)\n",
    "\n",
    "    tr = true_range(df)\n",
    "    atrn = tr.ewm(alpha=1/period, adjust=False).mean()\n",
    "\n",
    "    plus_di = 100 * pd.Series(plus_dm, index=df.index).ewm(alpha=1/period, adjust=False).mean() / (atrn + 1e-12)\n",
    "    minus_di = 100 * pd.Series(minus_dm, index=df.index).ewm(alpha=1/period, adjust=False).mean() / (atrn + 1e-12)\n",
    "\n",
    "    dx = 100 * (plus_di - minus_di).abs() / (plus_di + minus_di + 1e-12)\n",
    "    return dx.ewm(alpha=1/period, adjust=False).mean()\n",
    "\n",
    "def macd_hist(close: pd.Series, fast=12, slow=26, signal=9) -> pd.Series:\n",
    "    macd = ema(close, fast) - ema(close, slow)\n",
    "    sig  = ema(macd, signal)\n",
    "    return macd - sig\n",
    "\n",
    "def bb_width(close: pd.Series, period=20, k=2.0) -> pd.Series:\n",
    "    m = close.rolling(period).mean()\n",
    "    s = close.rolling(period).std()\n",
    "    upper = m + k*s\n",
    "    lower = m - k*s\n",
    "    width = (upper - lower) / (m.abs() + 1e-12)\n",
    "    return width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bf798a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df: (224102, 48) test_df: (49511, 48)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>m15_ema200</th>\n",
       "      <th>m15_ema50</th>\n",
       "      <th>m15_ema20</th>\n",
       "      <th>m15_ema200_slope</th>\n",
       "      <th>...</th>\n",
       "      <th>4h_dist_ema200_atr</th>\n",
       "      <th>4h_dist_ema50_atr</th>\n",
       "      <th>m1_ret_mean</th>\n",
       "      <th>m1_ret_std</th>\n",
       "      <th>m1_rng_mean</th>\n",
       "      <th>m1_rng_max</th>\n",
       "      <th>m1_body_mean</th>\n",
       "      <th>m1_wick_down_mean</th>\n",
       "      <th>m1_wick_up_mean</th>\n",
       "      <th>m1_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-06 20:00:00+00:00</td>\n",
       "      <td>1.19129</td>\n",
       "      <td>1.19147</td>\n",
       "      <td>1.18956</td>\n",
       "      <td>1.18980</td>\n",
       "      <td>2881.09</td>\n",
       "      <td>1.195015</td>\n",
       "      <td>1.192249</td>\n",
       "      <td>1.192217</td>\n",
       "      <td>-0.000114</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.630483</td>\n",
       "      <td>-3.415172</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.00063</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.140234</td>\n",
       "      <td>0.231039</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-06 20:15:00+00:00</td>\n",
       "      <td>1.18980</td>\n",
       "      <td>1.19038</td>\n",
       "      <td>1.18887</td>\n",
       "      <td>1.19019</td>\n",
       "      <td>2310.13</td>\n",
       "      <td>1.194967</td>\n",
       "      <td>1.192168</td>\n",
       "      <td>1.192024</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.630483</td>\n",
       "      <td>-3.415172</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.00091</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.184871</td>\n",
       "      <td>0.193857</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-06 20:30:00+00:00</td>\n",
       "      <td>1.19019</td>\n",
       "      <td>1.19043</td>\n",
       "      <td>1.18960</td>\n",
       "      <td>1.18960</td>\n",
       "      <td>1716.93</td>\n",
       "      <td>1.194913</td>\n",
       "      <td>1.192067</td>\n",
       "      <td>1.191793</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.630483</td>\n",
       "      <td>-3.415172</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.00093</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.243291</td>\n",
       "      <td>0.187859</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp     open     high      low    close   volume  \\\n",
       "0 2015-01-06 20:00:00+00:00  1.19129  1.19147  1.18956  1.18980  2881.09   \n",
       "1 2015-01-06 20:15:00+00:00  1.18980  1.19038  1.18887  1.19019  2310.13   \n",
       "2 2015-01-06 20:30:00+00:00  1.19019  1.19043  1.18960  1.18960  1716.93   \n",
       "\n",
       "   m15_ema200  m15_ema50  m15_ema20  m15_ema200_slope  ...  \\\n",
       "0    1.195015   1.192249   1.192217         -0.000114  ...   \n",
       "1    1.194967   1.192168   1.192024         -0.000138  ...   \n",
       "2    1.194913   1.192067   1.191793         -0.000154  ...   \n",
       "\n",
       "   4h_dist_ema200_atr  4h_dist_ema50_atr  m1_ret_mean  m1_ret_std  \\\n",
       "0           -4.630483          -3.415172    -0.000083    0.000155   \n",
       "1           -4.630483          -3.415172    -0.000083    0.000198   \n",
       "2           -4.630483          -3.415172     0.000022    0.000215   \n",
       "\n",
       "   m1_rng_mean  m1_rng_max  m1_body_mean  m1_wick_down_mean  m1_wick_up_mean  \\\n",
       "0     0.000213     0.00063      0.000147           0.140234         0.231039   \n",
       "1     0.000292     0.00091      0.000192           0.184871         0.193857   \n",
       "2     0.000319     0.00093      0.000195           0.243291         0.187859   \n",
       "\n",
       "   m1_count  \n",
       "0      15.0  \n",
       "1      15.0  \n",
       "2      15.0  \n",
       "\n",
       "[3 rows x 48 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ===== 3) Feature building & multi‑TF alignment (NO lookahead) =====\n",
    "def add_tf_features(df: pd.DataFrame, prefix: str) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[f\"{prefix}_ema200\"] = ema(out[\"close\"], 200)\n",
    "    out[f\"{prefix}_ema50\"]  = ema(out[\"close\"], 50)\n",
    "    out[f\"{prefix}_ema20\"]  = ema(out[\"close\"], 20)\n",
    "    out[f\"{prefix}_ema200_slope\"] = out[f\"{prefix}_ema200\"].diff(3)\n",
    "    out[f\"{prefix}_rsi14\"] = rsi(out[\"close\"], 14)\n",
    "    out[f\"{prefix}_adx14\"] = adx(out, 14)\n",
    "    out[f\"{prefix}_atr14\"] = atr(out, cfg.atr_period)\n",
    "    out[f\"{prefix}_macd_h\"] = macd_hist(out[\"close\"])\n",
    "    out[f\"{prefix}_bb_width\"] = bb_width(out[\"close\"])\n",
    "\n",
    "    # ATR-scaled distances (robust across years)\n",
    "    atrv = out[f\"{prefix}_atr14\"].replace(0, np.nan)\n",
    "    out[f\"{prefix}_dist_ema200_atr\"] = (out[\"close\"] - out[f\"{prefix}_ema200\"]) / atrv\n",
    "    out[f\"{prefix}_dist_ema50_atr\"]  = (out[\"close\"] - out[f\"{prefix}_ema50\"]) / atrv\n",
    "    return out\n",
    "\n",
    "def merge_asof_backward(left: pd.DataFrame, right: pd.DataFrame, suffix: str) -> pd.DataFrame:\n",
    "    # assumes timestamps represent candle OPEN time; uses last known candle whose open <= left timestamp\n",
    "    L = left.sort_values(\"timestamp\").copy()\n",
    "    R = right.sort_values(\"timestamp\").copy()\n",
    "    m = pd.merge_asof(L, R, on=\"timestamp\", direction=\"backward\", suffixes=(\"\", f\"_{suffix}\"))\n",
    "    return m\n",
    "\n",
    "def agg_entry_1m_to_15m(one_m: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = one_m.copy()\n",
    "    d = d.sort_values(\"timestamp\").set_index(\"timestamp\")\n",
    "\n",
    "    ret1 = np.log(d[\"close\"]).diff()\n",
    "    rng = (d[\"high\"] - d[\"low\"])\n",
    "    body = (d[\"close\"] - d[\"open\"]).abs()\n",
    "    wick_down = (d[[\"open\",\"close\"]].min(axis=1) - d[\"low\"]).clip(lower=0)\n",
    "    wick_up   = (d[\"high\"] - d[[\"open\",\"close\"]].max(axis=1)).clip(lower=0)\n",
    "    rng_safe = rng.replace(0, np.nan)\n",
    "\n",
    "    agg = pd.DataFrame({\n",
    "        \"m1_ret_mean\": ret1.resample(\"15min\").mean(),\n",
    "        \"m1_ret_std\":  ret1.resample(\"15min\").std(),\n",
    "        \"m1_rng_mean\": rng.resample(\"15min\").mean(),\n",
    "        \"m1_rng_max\":  rng.resample(\"15min\").max(),\n",
    "        \"m1_body_mean\": body.resample(\"15min\").mean(),\n",
    "        \"m1_wick_down_mean\": (wick_down / rng_safe).resample(\"15min\").mean(),\n",
    "        \"m1_wick_up_mean\":   (wick_up   / rng_safe).resample(\"15min\").mean(),\n",
    "        \"m1_count\": rng.resample(\"15min\").count(),\n",
    "    }).reset_index()\n",
    "\n",
    "    # shift by 1 bucket: master row at time t uses fully completed previous 15m window\n",
    "    for c in agg.columns:\n",
    "        if c != \"timestamp\":\n",
    "            agg[c] = agg[c].shift(1)\n",
    "    return agg\n",
    "\n",
    "def build_master_table(bundle: dict) -> pd.DataFrame:\n",
    "    # master (15m) with its own features\n",
    "    master = bundle[cfg.master_tf].copy()\n",
    "    master = add_tf_features(master, \"m15\")\n",
    "    master[\"atr14\"] = master[\"m15_atr14\"]\n",
    "\n",
    "    # macro features (1h, 4h)\n",
    "    for tf in cfg.macro_tfs:\n",
    "        mac = add_tf_features(bundle[tf], tf)\n",
    "        keep = [\"timestamp\"] + [c for c in mac.columns if c.startswith(tf+\"_\")]\n",
    "        master = merge_asof_backward(master, mac[keep], suffix=tf)\n",
    "\n",
    "    # entry agg (1m -> 15m)\n",
    "    entry = agg_entry_1m_to_15m(bundle[cfg.entry_tf])\n",
    "    master = pd.merge(master, entry, on=\"timestamp\", how=\"left\")\n",
    "\n",
    "    # clean\n",
    "    master = master.replace([np.inf, -np.inf], np.nan).sort_values(\"timestamp\")\n",
    "    master = master.fillna(method=\"ffill\").dropna().reset_index(drop=True)\n",
    "    return master\n",
    "\n",
    "train_df = build_master_table(train_raw)\n",
    "test_df  = build_master_table(test_raw)\n",
    "\n",
    "print(\"train_df:\", train_df.shape, \"test_df:\", test_df.shape)\n",
    "train_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "112d6c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 163897, 1: 60205}, {0: 34811, 1: 14700})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ===== 4) Macro gate (PF-first) =====\n",
    "def macro_gate(df: pd.DataFrame) -> pd.Series:\n",
    "    # conservative default; we'll optionally tune later\n",
    "    ok = (\n",
    "        (df[\"4h_dist_ema200_atr\"] > 0.0) &\n",
    "        (df[\"4h_ema200_slope\"] > 0.0) &\n",
    "        (df[\"4h_adx14\"] > 18) &\n",
    "        (df[\"1h_dist_ema200_atr\"] > -0.25) &\n",
    "        (df[\"1h_adx14\"] > 16) &\n",
    "        (df[\"4h_bb_width\"] > 0.001)  # avoid ultra-compressed dead market\n",
    "    )\n",
    "    return ok.astype(int)\n",
    "\n",
    "train_df[\"macro_ok\"] = macro_gate(train_df)\n",
    "test_df[\"macro_ok\"]  = macro_gate(test_df)\n",
    "\n",
    "train_df[\"macro_ok\"].value_counts().to_dict(), test_df[\"macro_ok\"].value_counts().to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bdcb5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 152397, 1: 71705}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ===== 5) Triple‑barrier label on 15m (for EDGE model) =====\n",
    "def make_triple_barrier_label(df: pd.DataFrame, tp_atr: float, sl_atr: float, max_hold: int) -> pd.Series:\n",
    "    tp = df[\"open\"].shift(-1) + tp_atr * df[\"atr14\"]  # TP/SL set at decision time (t), entry at t+1 open\n",
    "    sl = df[\"open\"].shift(-1) - sl_atr * df[\"atr14\"]\n",
    "\n",
    "    y = np.zeros(len(df), dtype=int)\n",
    "    for i in range(len(df) - max_hold - 2):\n",
    "        entry_i = i + 1\n",
    "        if not np.isfinite(tp.iloc[i]) or not np.isfinite(sl.iloc[i]):\n",
    "            continue\n",
    "        hi = df[\"high\"].iloc[entry_i: entry_i + max_hold]\n",
    "        lo = df[\"low\"].iloc[entry_i: entry_i + max_hold]\n",
    "\n",
    "        # earliest barrier hit\n",
    "        hit_tp = np.where(hi.values >= tp.iloc[i])[0]\n",
    "        hit_sl = np.where(lo.values <= sl.iloc[i])[0]\n",
    "\n",
    "        t_tp = hit_tp[0] if len(hit_tp) else None\n",
    "        t_sl = hit_sl[0] if len(hit_sl) else None\n",
    "\n",
    "        if t_tp is not None and (t_sl is None or t_tp < t_sl):\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = 0\n",
    "\n",
    "    return pd.Series(y, index=df.index)\n",
    "\n",
    "# default label params (tuned later)\n",
    "train_df[\"y_edge\"] = make_triple_barrier_label(train_df, cfg.tp_atr, cfg.sl_atr, cfg.max_hold_bars)\n",
    "test_df[\"y_edge\"]  = make_triple_barrier_label(test_df,  cfg.tp_atr, cfg.sl_atr, cfg.max_hold_bars)\n",
    "\n",
    "train_df[\"y_edge\"].value_counts().to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcb92a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== 6) Entry label (1m aggregated): avoid immediate adverse move after entry =====\n",
    "# We label \"good entry\" if within next 3 master bars (45m) we do NOT dip more than X*ATR below entry.\n",
    "def make_entry_label(df: pd.DataFrame, adverse_atr: float = 0.5, horizon: int = 3) -> pd.Series:\n",
    "    y = np.zeros(len(df), dtype=int)\n",
    "    for i in range(len(df) - horizon - 2):\n",
    "        entry_i = i + 1\n",
    "        entry = df.loc[entry_i, \"open\"]\n",
    "        atr_i = df.loc[i, \"atr14\"]\n",
    "        if not np.isfinite(entry) or not np.isfinite(atr_i) or atr_i <= 0:\n",
    "            continue\n",
    "        floor = entry - adverse_atr * atr_i\n",
    "        lo = df[\"low\"].iloc[entry_i:entry_i+horizon].min()\n",
    "        y[i] = 1 if lo > floor else 0\n",
    "    return pd.Series(y, index=df.index)\n",
    "\n",
    "train_df[\"y_entry\"] = make_entry_label(train_df, adverse_atr=0.5, horizon=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fb45549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core: (174243, 52) val: (49859, 52)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 7) Train/Validation split inside train (2015–2023) =====\n",
    "# We split by time using timestamp years if available; fallback to last 20% as validation.\n",
    "train_df[\"year\"] = train_df[\"timestamp\"].dt.year\n",
    "\n",
    "core = train_df[train_df[\"year\"] <= 2021].copy()\n",
    "val  = train_df[(train_df[\"year\"] >= 2022) & (train_df[\"year\"] <= 2023)].copy()\n",
    "\n",
    "if len(val) < 1000:\n",
    "    cut = int(len(train_df)*0.8)\n",
    "    core = train_df.iloc[:cut].copy()\n",
    "    val  = train_df.iloc[cut:].copy()\n",
    "\n",
    "print(\"core:\", core.shape, \"val:\", val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5b79c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing_edge: []\n",
      "missing_entry: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 8) Feature sets =====\n",
    "# Edge model features: 15m + macro context + entry agg (but NOT future)\n",
    "EDGE_FEATURES = [\n",
    "    # master 15m\n",
    "    \"m15_dist_ema200_atr\",\"m15_dist_ema50_atr\",\"m15_ema200_slope\",\"m15_rsi14\",\"m15_adx14\",\"m15_macd_h\",\"m15_bb_width\",\n",
    "    \"m15_atr14\",\n",
    "    # macro context\n",
    "    \"1h_dist_ema200_atr\",\"1h_ema200_slope\",\"1h_adx14\",\"1h_bb_width\",\n",
    "    \"4h_dist_ema200_atr\",\"4h_ema200_slope\",\"4h_adx14\",\"4h_bb_width\",\n",
    "]\n",
    "\n",
    "# Entry features (from 1m aggregation) + a couple of local context\n",
    "ENTRY_FEATURES = [\n",
    "    \"m1_ret_mean\",\"m1_ret_std\",\"m1_rng_mean\",\"m1_rng_max\",\"m1_body_mean\",\"m1_wick_down_mean\",\"m1_wick_up_mean\",\"m1_count\",\n",
    "    \"m15_atr14\",\"m15_rsi14\"\n",
    "]\n",
    "\n",
    "# sanity\n",
    "missing_edge = [c for c in EDGE_FEATURES if c not in train_df.columns]\n",
    "missing_entry = [c for c in ENTRY_FEATURES if c not in train_df.columns]\n",
    "print(\"missing_edge:\", missing_edge)\n",
    "print(\"missing_entry:\", missing_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eedf8307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge AUC (val, gated): 0.6288569294141332\n",
      "Entry AUC (val, gated): 0.6251427957062523\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 9) Models =====\n",
    "def make_edge_model():\n",
    "    if HAS_XGB:\n",
    "        return XGBClassifier(\n",
    "            n_estimators=600,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=2.0,\n",
    "            min_child_weight=2,\n",
    "            random_state=42,\n",
    "            eval_metric=\"logloss\",\n",
    "            n_jobs=4\n",
    "        )\n",
    "    else:\n",
    "        return HistGradientBoostingClassifier(\n",
    "            max_depth=4,\n",
    "            learning_rate=0.05,\n",
    "            max_iter=600,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "edge_model = make_edge_model()\n",
    "\n",
    "# Calibrate probabilities for stable thresholding (PF tuning)\n",
    "edge_clf = CalibratedClassifierCV(edge_model, method=\"sigmoid\", cv=3)\n",
    "\n",
    "entry_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"lr\", LogisticRegression(max_iter=2000))\n",
    "])\n",
    "\n",
    "# train edge on macro_ok==1 only (PF-first selectivity)\n",
    "core_edge = core[core[\"macro_ok\"]==1].copy()\n",
    "val_edge  = val[val[\"macro_ok\"]==1].copy()\n",
    "\n",
    "X_core_edge = core_edge[EDGE_FEATURES]\n",
    "y_core_edge = core_edge[\"y_edge\"]\n",
    "\n",
    "X_val_edge  = val_edge[EDGE_FEATURES]\n",
    "y_val_edge  = val_edge[\"y_edge\"]\n",
    "\n",
    "edge_clf.fit(X_core_edge, y_core_edge)\n",
    "\n",
    "p_val_edge = edge_clf.predict_proba(X_val_edge)[:,1]\n",
    "print(\"Edge AUC (val, gated):\", roc_auc_score(y_val_edge, p_val_edge))\n",
    "\n",
    "# train entry filter on all macro_ok==1 rows (still ok)\n",
    "core_entry = core[core[\"macro_ok\"]==1].copy()\n",
    "val_entry  = val[val[\"macro_ok\"]==1].copy()\n",
    "\n",
    "X_core_entry = core_entry[ENTRY_FEATURES]\n",
    "y_core_entry = core_entry[\"y_entry\"]\n",
    "\n",
    "X_val_entry  = val_entry[ENTRY_FEATURES]\n",
    "y_val_entry  = val_entry[\"y_entry\"]\n",
    "\n",
    "entry_clf.fit(X_core_entry, y_core_entry)\n",
    "p_val_entry = entry_clf.predict_proba(X_val_entry)[:,1]\n",
    "print(\"Entry AUC (val, gated):\", roc_auc_score(y_val_entry, p_val_entry))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8fc5910",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== 10) Backtest engine (truthful): next-open, costs, 1% risk, intrabar worst-case =====\n",
    "@dataclass\n",
    "class Trade:\n",
    "    entry_time: pd.Timestamp\n",
    "    exit_time: pd.Timestamp\n",
    "    entry_price: float\n",
    "    exit_price: float\n",
    "    sl: float\n",
    "    tp: float\n",
    "    lots: float\n",
    "    pnl_usd: float\n",
    "    pnl_pips: float\n",
    "    outcome: str\n",
    "\n",
    "def pip_to_price(pips: float) -> float:\n",
    "    return pips * cfg.pip_size\n",
    "\n",
    "def price_to_pips(price_delta: float) -> float:\n",
    "    return price_delta / cfg.pip_size\n",
    "\n",
    "def calc_lots_for_risk(equity: float, stop_pips: float) -> float:\n",
    "    if stop_pips <= 0:\n",
    "        return 0.0\n",
    "    risk_usd = equity * cfg.risk_per_trade\n",
    "    lots = risk_usd / (stop_pips * cfg.usd_per_pip_per_lot + 1e-12)\n",
    "    return max(0.0, float(lots))\n",
    "\n",
    "def run_backtest(df: pd.DataFrame, p_edge: np.ndarray, p_entry: np.ndarray,\n",
    "                edge_th: float, entry_th: float, tp_atr: float, sl_atr: float, max_hold: int):\n",
    "    df = df.reset_index(drop=True).copy()\n",
    "    spread = pip_to_price(cfg.spread_pips)\n",
    "    slip   = pip_to_price(cfg.slippage_pips)\n",
    "\n",
    "    equity = cfg.starting_equity\n",
    "    eq_curve = []\n",
    "    trades = []\n",
    "    in_pos = False\n",
    "    entry_i = None\n",
    "    entry_price = sl = tp = lots = None\n",
    "    bars_in_trade = 0\n",
    "\n",
    "    for i in range(len(df)-2):\n",
    "        t = df.loc[i, \"timestamp\"]\n",
    "        eq_curve.append((t, equity))\n",
    "\n",
    "        if in_pos:\n",
    "            bars_in_trade += 1\n",
    "            hi = df.loc[i, \"high\"]\n",
    "            lo = df.loc[i, \"low\"]\n",
    "            hit_sl = lo <= sl\n",
    "            hit_tp = hi >= tp\n",
    "\n",
    "            if hit_sl or hit_tp or bars_in_trade >= max_hold:\n",
    "                if hit_sl and hit_tp:\n",
    "                    exit_price = sl   # worst-case for long\n",
    "                    outcome = \"SL(amb)\"\n",
    "                elif hit_sl:\n",
    "                    exit_price = sl\n",
    "                    outcome = \"SL\"\n",
    "                elif hit_tp:\n",
    "                    exit_price = tp\n",
    "                    outcome = \"TP\"\n",
    "                else:\n",
    "                    # time exit at close(i) on bid\n",
    "                    exit_price = df.loc[i, \"close\"]\n",
    "                    outcome = \"TIME\"\n",
    "\n",
    "                # long exit on bid (worse): -spread/2 - slip\n",
    "                exit_price = exit_price - spread/2 - slip\n",
    "\n",
    "                pnl_price = exit_price - entry_price\n",
    "                pnl_pips  = price_to_pips(pnl_price)\n",
    "                pnl_usd   = pnl_pips * cfg.usd_per_pip_per_lot * lots\n",
    "                pnl_usd  -= cfg.commission_per_lot_usd * lots  # round-trip\n",
    "\n",
    "                equity += pnl_usd\n",
    "                trades.append(Trade(\n",
    "                    entry_time=df.loc[entry_i, \"timestamp\"],\n",
    "                    exit_time=t,\n",
    "                    entry_price=float(entry_price),\n",
    "                    exit_price=float(exit_price),\n",
    "                    sl=float(sl), tp=float(tp),\n",
    "                    lots=float(lots),\n",
    "                    pnl_usd=float(pnl_usd),\n",
    "                    pnl_pips=float(pnl_pips),\n",
    "                    outcome=outcome\n",
    "                ))\n",
    "                in_pos = False\n",
    "                entry_i = None\n",
    "                bars_in_trade = 0\n",
    "            continue\n",
    "\n",
    "        # no position: decide at close(i), enter at open(i+1)\n",
    "        if df.loc[i, \"macro_ok\"] != 1:\n",
    "            continue\n",
    "        if not (p_edge[i] > edge_th and p_entry[i] > entry_th):\n",
    "            continue\n",
    "\n",
    "        entry_i = i + 1\n",
    "        raw_entry = df.loc[entry_i, \"open\"]\n",
    "        entry_price = raw_entry + spread/2 + slip  # long entry on ask\n",
    "\n",
    "        atr_i = df.loc[i, \"atr14\"]\n",
    "        if not np.isfinite(atr_i) or atr_i <= 0:\n",
    "            entry_i = None\n",
    "            continue\n",
    "\n",
    "        tp = entry_price + tp_atr * atr_i\n",
    "        sl = entry_price - sl_atr * atr_i\n",
    "\n",
    "        stop_pips = price_to_pips(entry_price - sl)\n",
    "        lots = calc_lots_for_risk(equity, stop_pips)\n",
    "        if lots <= 0:\n",
    "            entry_i = None\n",
    "            continue\n",
    "\n",
    "        in_pos = True\n",
    "        bars_in_trade = 0\n",
    "\n",
    "    eq = pd.DataFrame(eq_curve, columns=[\"timestamp\",\"equity\"]).drop_duplicates(\"timestamp\")\n",
    "    eq[\"peak\"] = eq[\"equity\"].cummax()\n",
    "    eq[\"dd\"] = (eq[\"equity\"] - eq[\"peak\"]) / (eq[\"peak\"] + 1e-12)\n",
    "    return trades, eq\n",
    "\n",
    "def summarize(trades, eq):\n",
    "    if len(trades) == 0:\n",
    "        return {\"trades\":0, \"profit_factor\":0.0, \"max_drawdown\": float(eq[\"dd\"].min()) if len(eq) else 0.0}\n",
    "    pnl = np.array([t.pnl_usd for t in trades])\n",
    "    gross_profit = pnl[pnl>0].sum()\n",
    "    gross_loss = -pnl[pnl<0].sum()\n",
    "    pf = gross_profit / gross_loss if gross_loss > 0 else float(\"inf\")\n",
    "    return {\n",
    "        \"trades\": int(len(trades)),\n",
    "        \"profit_factor\": float(pf),\n",
    "        \"net_profit_usd\": float(pnl.sum()),\n",
    "        \"winrate\": float((pnl>0).mean()),\n",
    "        \"avg_trade_usd\": float(pnl.mean()),\n",
    "        \"max_drawdown\": float(eq[\"dd\"].min()) if len(eq) else 0.0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42164544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search done in 18.6s. Candidates: 0\n",
      "No config passed constraints.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 11) Validation tuning (PF-first): thresholds + TP/SL grid =====\n",
    "def compute_probs(df: pd.DataFrame):\n",
    "    df_use = df.copy()\n",
    "    X_edge = df_use[EDGE_FEATURES]\n",
    "    X_ent  = df_use[ENTRY_FEATURES]\n",
    "    p_edge = edge_clf.predict_proba(X_edge)[:,1]\n",
    "    p_ent  = entry_clf.predict_proba(X_ent)[:,1]\n",
    "    return p_edge, p_ent\n",
    "\n",
    "p_val_edge_all, p_val_entry_all = compute_probs(val)\n",
    "\n",
    "# Narrower grid to save time, but broader scope for thresholds\n",
    "grid_edge_th  = [0.55, 0.60, 0.65]\n",
    "grid_entry_th = [0.50, 0.53, 0.55]\n",
    "grid_tp = [2.0, 2.5]\n",
    "grid_sl = [1.5, 2.0]\n",
    "\n",
    "best = None\n",
    "rows = []\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "for et in grid_edge_th:\n",
    "    for it in grid_entry_th:\n",
    "        for tp_atr in grid_tp:\n",
    "            for sl_atr in grid_sl:\n",
    "                trades, eq = run_backtest(val, p_val_edge_all, p_val_entry_all, et, it, tp_atr, sl_atr, cfg.max_hold_bars)\n",
    "                st = summarize(trades, eq)\n",
    "                \n",
    "                # constraints (PF-first but keep it reasonable)\n",
    "                # Soften min trades to 10 just to find SOMETHING, in real life you want 50+\n",
    "                if st[\"trades\"] < 10:\n",
    "                    continue\n",
    "                if st[\"max_drawdown\"] < -0.30: # relax DD\n",
    "                    continue\n",
    "                    \n",
    "                score = st[\"profit_factor\"]  # primary objective\n",
    "                \n",
    "                rows.append((score, et, it, tp_atr, sl_atr, st[\"trades\"], st[\"max_drawdown\"], st[\"net_profit_usd\"]))\n",
    "                \n",
    "                # track best\n",
    "                if best is None or score > best[0]:\n",
    "                    best = (score, et, it, tp_atr, sl_atr, st)\n",
    "\n",
    "print(f\"Grid search done in {time.time()-t0:.1f}s. Candidates: {len(rows)}\")\n",
    "if best:\n",
    "    print(\"Best PF:\", best[0])\n",
    "else:\n",
    "    print(\"No config passed constraints.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fb30b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PF</th>\n",
       "      <th>edge_th</th>\n",
       "      <th>entry_th</th>\n",
       "      <th>tp_atr</th>\n",
       "      <th>sl_atr</th>\n",
       "      <th>trades</th>\n",
       "      <th>max_dd</th>\n",
       "      <th>net_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [PF, edge_th, entry_th, tp_atr, sl_atr, trades, max_dd, net_usd]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Show top validation configs\n",
    "top = sorted(rows, key=lambda x: x[0], reverse=True)[:20]\n",
    "pd.DataFrame(top, columns=[\"PF\",\"edge_th\",\"entry_th\",\"tp_atr\",\"sl_atr\",\"trades\",\"max_dd\",\"net_usd\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89117750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: No valid config found during tuning. Using defaults.\n",
      "BEST on VAL: {}\n",
      "Chosen: {'EDGE_TH': 0.62, 'ENTRY_TH': 0.55, 'TP_ATR': 2.0, 'SL_ATR': 1.5}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply best config\n",
    "if best is None:\n",
    "    print(\"WARNING: No valid config found during tuning. Using defaults.\")\n",
    "    # Default fallback\n",
    "    PF_best = 0.0\n",
    "    EDGE_TH = cfg.edge_th\n",
    "    ENTRY_TH = cfg.entry_th\n",
    "    TP_ATR = cfg.tp_atr\n",
    "    SL_ATR = cfg.sl_atr\n",
    "    st_best = {}\n",
    "else:\n",
    "    PF_best, EDGE_TH, ENTRY_TH, TP_ATR, SL_ATR, st_best = best\n",
    "    \n",
    "print(\"BEST on VAL:\", st_best)\n",
    "print(\"Chosen:\", {\"EDGE_TH\":EDGE_TH,\"ENTRY_TH\":ENTRY_TH,\"TP_ATR\":TP_ATR,\"SL_ATR\":SL_ATR})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6029804c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trades': 0, 'profit_factor': 0.0, 'max_drawdown': 0.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ===== 12) Final TEST backtest (2024–2025) with tuned params =====\n",
    "p_test_edge, p_test_entry = compute_probs(test_df)\n",
    "\n",
    "trades_test, eq_test = run_backtest(test_df, p_test_edge, p_test_entry, EDGE_TH, ENTRY_TH, TP_ATR, SL_ATR, cfg.max_hold_bars)\n",
    "stats_test = summarize(trades_test, eq_test)\n",
    "stats_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5e0400d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Quick peek: outcomes\n",
    "from collections import Counter\n",
    "Counter([t.outcome for t in trades_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d13fd292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>equity</th>\n",
       "      <th>peak</th>\n",
       "      <th>dd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49504</th>\n",
       "      <td>2025-12-30 22:15:00+00:00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49505</th>\n",
       "      <td>2025-12-30 22:30:00+00:00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49506</th>\n",
       "      <td>2025-12-30 22:45:00+00:00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49507</th>\n",
       "      <td>2025-12-30 23:00:00+00:00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49508</th>\n",
       "      <td>2025-12-30 23:15:00+00:00</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      timestamp   equity     peak   dd\n",
       "49504 2025-12-30 22:15:00+00:00  10000.0  10000.0  0.0\n",
       "49505 2025-12-30 22:30:00+00:00  10000.0  10000.0  0.0\n",
       "49506 2025-12-30 22:45:00+00:00  10000.0  10000.0  0.0\n",
       "49507 2025-12-30 23:00:00+00:00  10000.0  10000.0  0.0\n",
       "49508 2025-12-30 23:15:00+00:00  10000.0  10000.0  0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Equity curve preview\n",
    "eq_test.tail()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
