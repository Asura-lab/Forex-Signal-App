{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efc38538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸš€ FOREX SIGNAL GENERATOR V13\n",
      "   TP/SL Based Labeling + Strong Regularization\n",
      "======================================================================\n",
      "âœ“ GPU Available: True\n",
      "âœ“ TP: 15 pips, SL: 10 pips\n",
      "âœ“ Forward window: 60 bars\n",
      "âœ“ Model Directory: c:\\Users\\Acer\\Desktop\\Forex-Signal-App\\models\\signal_generator_v13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import joblib\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(r'c:\\Users\\Acer\\Desktop\\Forex-Signal-App')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "MODEL_DIR = BASE_DIR / 'models' / 'signal_generator_v13'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# GPU Check\n",
    "import torch\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "# Trading Parameters\n",
    "TP_PIPS = 15\n",
    "SL_PIPS = 10\n",
    "FORWARD_BARS = 60\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸš€ FOREX SIGNAL GENERATOR V13\")\n",
    "print(\"   TP/SL Based Labeling + Strong Regularization\")\n",
    "print(\"=\"*70)\n",
    "print(f\"âœ“ GPU Available: {GPU_AVAILABLE}\")\n",
    "print(f\"âœ“ TP: {TP_PIPS} pips, SL: {SL_PIPS} pips\")\n",
    "print(f\"âœ“ Forward window: {FORWARD_BARS} bars\")\n",
    "print(f\"âœ“ Model Directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc65d7d",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c412eb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,859,492 rows\n",
      "Test: 296,778 rows\n",
      "Train period: 2019-12-31 16:00:00+00:00 to 2024-12-30 16:00:00+00:00\n",
      "Test period: 2024-12-31 16:00:00+00:00 to 2025-10-17 06:11:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "train_df = pd.read_csv(DATA_DIR / 'EUR_USD_1min.csv')\n",
    "test_df = pd.read_csv(DATA_DIR / 'EUR_USD_test.csv')\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    if 'timestamp' in df.columns:\n",
    "        df.rename(columns={'timestamp': 'time'}, inplace=True)\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows\")\n",
    "print(f\"Test: {len(test_df):,} rows\")\n",
    "print(f\"Train period: {train_df['time'].min()} to {train_df['time'].max()}\")\n",
    "print(f\"Test period: {test_df['time'].min()} to {test_df['time'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db530340",
   "metadata": {},
   "source": [
    "## 2. TP/SL Based Labeling (BUY-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42492b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TP/SL based labels...\n",
      "This may take a few minutes...\n",
      "âœ“ Labels created in 1312.9s\n",
      "\n",
      "ðŸ“Š Label Distribution:\n",
      "Train: 601,304 samples\n",
      "  WIN (TP hit): 191,049 (31.8%)\n",
      "  LOSE (SL hit): 410,255 (68.2%)\n",
      "\n",
      "Test: 121,253 samples\n",
      "  WIN (TP hit): 40,811 (33.7%)\n",
      "  LOSE (SL hit): 80,442 (66.3%)\n"
     ]
    }
   ],
   "source": [
    "def create_tpsl_labels(df, tp_pips=15, sl_pips=10, forward_bars=60):\n",
    "    \"\"\"\n",
    "    V13 TP/SL Based Labeling for BUY signals:\n",
    "    - Label = 1 (WIN): TP hit before SL\n",
    "    - Label = 0 (LOSE): SL hit before TP\n",
    "    - Label = -1 (NO_TRADE): Neither hit within forward_bars\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    tp_price = tp_pips * 0.0001\n",
    "    sl_price = sl_pips * 0.0001\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if i + forward_bars >= len(df):\n",
    "            labels.append(-1)  # Not enough future data\n",
    "            continue\n",
    "            \n",
    "        entry_price = df.iloc[i]['close']\n",
    "        tp_level = entry_price + tp_price\n",
    "        sl_level = entry_price - sl_price\n",
    "        \n",
    "        # Look forward\n",
    "        future = df.iloc[i+1:i+1+forward_bars]\n",
    "        \n",
    "        tp_hit_idx = None\n",
    "        sl_hit_idx = None\n",
    "        \n",
    "        # Find first TP hit\n",
    "        tp_hits = future[future['high'] >= tp_level]\n",
    "        if len(tp_hits) > 0:\n",
    "            tp_hit_idx = tp_hits.index[0]\n",
    "        \n",
    "        # Find first SL hit\n",
    "        sl_hits = future[future['low'] <= sl_level]\n",
    "        if len(sl_hits) > 0:\n",
    "            sl_hit_idx = sl_hits.index[0]\n",
    "        \n",
    "        # Determine label\n",
    "        if tp_hit_idx is not None and sl_hit_idx is not None:\n",
    "            if tp_hit_idx < sl_hit_idx:\n",
    "                labels.append(1)  # WIN - TP first\n",
    "            else:\n",
    "                labels.append(0)  # LOSE - SL first\n",
    "        elif tp_hit_idx is not None:\n",
    "            labels.append(1)  # WIN - only TP hit\n",
    "        elif sl_hit_idx is not None:\n",
    "            labels.append(0)  # LOSE - only SL hit\n",
    "        else:\n",
    "            labels.append(-1)  # NO_TRADE - neither hit\n",
    "    \n",
    "    df['signal'] = labels\n",
    "    return df\n",
    "\n",
    "print(\"Creating TP/SL based labels...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "start = time.time()\n",
    "train_df = create_tpsl_labels(train_df, TP_PIPS, SL_PIPS, FORWARD_BARS)\n",
    "test_df = create_tpsl_labels(test_df, TP_PIPS, SL_PIPS, FORWARD_BARS)\n",
    "print(f\"âœ“ Labels created in {time.time()-start:.1f}s\")\n",
    "\n",
    "# Filter only WIN/LOSE (remove NO_TRADE)\n",
    "train_labeled = train_df[train_df['signal'] != -1].copy()\n",
    "test_labeled = test_df[test_df['signal'] != -1].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š Label Distribution:\")\n",
    "print(f\"Train: {len(train_labeled):,} samples\")\n",
    "print(f\"  WIN (TP hit): {(train_labeled['signal']==1).sum():,} ({(train_labeled['signal']==1).mean()*100:.1f}%)\")\n",
    "print(f\"  LOSE (SL hit): {(train_labeled['signal']==0).sum():,} ({(train_labeled['signal']==0).mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest: {len(test_labeled):,} samples\")\n",
    "print(f\"  WIN (TP hit): {(test_labeled['signal']==1).sum():,} ({(test_labeled['signal']==1).mean()*100:.1f}%)\")\n",
    "print(f\"  LOSE (SL hit): {(test_labeled['signal']==0).sum():,} ({(test_labeled['signal']==0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8380d0e",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e2f804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding features...\n",
      "âœ“ Train: 601,105 samples\n",
      "âœ“ Test: 121,054 samples\n",
      "âœ“ Train: 601,105 samples\n",
      "âœ“ Test: 121,054 samples\n"
     ]
    }
   ],
   "source": [
    "def add_features_v13(df):\n",
    "    \"\"\"V13 Feature Engineering - Same proven features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Time Features\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['day_of_week'] = df['time'].dt.dayofweek\n",
    "    df['is_london'] = ((df['hour'] >= 8) & (df['hour'] < 16)).astype(int)\n",
    "    df['is_ny'] = ((df['hour'] >= 13) & (df['hour'] < 21)).astype(int)\n",
    "    df['is_overlap'] = ((df['hour'] >= 13) & (df['hour'] < 16)).astype(int)\n",
    "    \n",
    "    # Moving Averages\n",
    "    for p in [5, 10, 20, 50, 200]:\n",
    "        df[f'sma_{p}'] = df['close'].rolling(p).mean()\n",
    "        df[f'ema_{p}'] = df['close'].ewm(span=p, adjust=False).mean()\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    ema12 = df['close'].ewm(span=12).mean()\n",
    "    ema26 = df['close'].ewm(span=26).mean()\n",
    "    df['macd'] = ema12 - ema26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    df['bb_mid'] = df['close'].rolling(20).mean()\n",
    "    bb_std = df['close'].rolling(20).std()\n",
    "    df['bb_upper'] = df['bb_mid'] + 2 * bb_std\n",
    "    df['bb_lower'] = df['bb_mid'] - 2 * bb_std\n",
    "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / (df['bb_mid'] + 1e-10)\n",
    "    df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'] + 1e-10)\n",
    "    \n",
    "    # ATR\n",
    "    tr = pd.concat([\n",
    "        abs(df['high'] - df['low']),\n",
    "        abs(df['high'] - df['close'].shift()),\n",
    "        abs(df['low'] - df['close'].shift())\n",
    "    ], axis=1).max(axis=1)\n",
    "    df['atr'] = tr.rolling(14).mean()\n",
    "    df['atr_ratio'] = df['atr'] / (df['close'] + 1e-10)\n",
    "    \n",
    "    # Stochastic\n",
    "    low_14 = df['low'].rolling(14).min()\n",
    "    high_14 = df['high'].rolling(14).max()\n",
    "    df['stoch_k'] = 100 * (df['close'] - low_14) / (high_14 - low_14 + 1e-10)\n",
    "    df['stoch_d'] = df['stoch_k'].rolling(3).mean()\n",
    "    \n",
    "    # Price patterns\n",
    "    df['body'] = df['close'] - df['open']\n",
    "    df['body_pct'] = df['body'] / (df['open'] + 1e-10)\n",
    "    df['upper_wick'] = df['high'] - df[['open', 'close']].max(axis=1)\n",
    "    df['lower_wick'] = df[['open', 'close']].min(axis=1) - df['low']\n",
    "    df['range'] = df['high'] - df['low']\n",
    "    df['range_atr'] = df['range'] / (df['atr'] + 1e-10)\n",
    "    \n",
    "    # Momentum\n",
    "    for p in [5, 10, 20]:\n",
    "        df[f'momentum_{p}'] = df['close'] - df['close'].shift(p)\n",
    "        df[f'roc_{p}'] = df['close'].pct_change(p) * 100\n",
    "    \n",
    "    # Trend\n",
    "    df['trend_5'] = (df['close'] > df['sma_5']).astype(int)\n",
    "    df['trend_20'] = (df['close'] > df['sma_20']).astype(int)\n",
    "    df['trend_50'] = (df['close'] > df['sma_50']).astype(int)\n",
    "    df['trend_strength'] = df['trend_5'] + df['trend_20'] + df['trend_50']\n",
    "    \n",
    "    # MA distances\n",
    "    df['dist_sma20'] = (df['close'] - df['sma_20']) / (df['sma_20'] + 1e-10) * 100\n",
    "    df['dist_sma50'] = (df['close'] - df['sma_50']) / (df['sma_50'] + 1e-10) * 100\n",
    "    \n",
    "    # Volatility\n",
    "    df['volatility_10'] = df['close'].rolling(10).std()\n",
    "    df['volatility_20'] = df['close'].rolling(20).std()\n",
    "    \n",
    "    # Volume\n",
    "    if 'volume' in df.columns:\n",
    "        df['volume_sma'] = df['volume'].rolling(20).mean()\n",
    "        df['volume_ratio'] = df['volume'] / (df['volume_sma'] + 1e-10)\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "print(\"Adding features...\")\n",
    "train_labeled = add_features_v13(train_labeled)\n",
    "test_labeled = add_features_v13(test_labeled)\n",
    "\n",
    "print(f\"âœ“ Train: {len(train_labeled):,} samples\")\n",
    "print(f\"âœ“ Test: {len(test_labeled):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed69a95",
   "metadata": {},
   "source": [
    "## 4. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c680573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Features: 50\n",
      "âœ“ X_train: (601105, 50)\n",
      "âœ“ X_test: (121054, 50)\n",
      "âœ“ y_train WIN ratio: 31.8%\n",
      "âœ“ y_test WIN ratio: 33.7%\n",
      "âœ“ X_train: (601105, 50)\n",
      "âœ“ X_test: (121054, 50)\n",
      "âœ“ y_train WIN ratio: 31.8%\n",
      "âœ“ y_test WIN ratio: 33.7%\n"
     ]
    }
   ],
   "source": [
    "# Define feature columns\n",
    "exclude_cols = ['time', 'signal', 'open', 'high', 'low', 'close', 'volume', 'tick_volume']\n",
    "feature_cols = [c for c in train_labeled.columns if c not in exclude_cols]\n",
    "\n",
    "# Only numeric columns\n",
    "feature_cols = [c for c in feature_cols if train_labeled[c].dtype in ['float64', 'int64', 'float32', 'int32']]\n",
    "\n",
    "print(f\"ðŸ“Š Features: {len(feature_cols)}\")\n",
    "\n",
    "X_train = train_labeled[feature_cols].values\n",
    "y_train = train_labeled['signal'].values\n",
    "X_test = test_labeled[feature_cols].values\n",
    "y_test = test_labeled['signal'].values\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"âœ“ X_train: {X_train_scaled.shape}\")\n",
    "print(f\"âœ“ X_test: {X_test_scaled.shape}\")\n",
    "print(f\"âœ“ y_train WIN ratio: {y_train.mean()*100:.1f}%\")\n",
    "print(f\"âœ“ y_test WIN ratio: {y_test.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2cedc2",
   "metadata": {},
   "source": [
    "## 5. Train V13 Models (Strong Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a052f889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸš€ TRAINING V13 MODELS\n",
      "   With Strong Regularization to Prevent Overfitting\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Training XGBoost...\n",
      "ðŸ“Š Training LightGBM...\n",
      "ðŸ“Š Training CatBoost...\n",
      "\n",
      "âœ“ Created 6 models\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸš€ TRAINING V13 MODELS\")\n",
    "print(\"   With Strong Regularization to Prevent Overfitting\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models = {}\n",
    "\n",
    "# XGBoost - Strong Regularization\n",
    "print(\"\\nðŸ“Š Training XGBoost...\")\n",
    "models['xgb1'] = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=4,           # Shallow trees\n",
    "    learning_rate=0.01,    # Low learning rate\n",
    "    subsample=0.6,         # Strong subsampling\n",
    "    colsample_bytree=0.6,\n",
    "    reg_alpha=1.0,         # Strong L1\n",
    "    reg_lambda=5.0,        # Strong L2\n",
    "    min_child_weight=10,   # More samples per leaf\n",
    "    gamma=0.5,             # Pruning\n",
    "    tree_method='hist',\n",
    "    device='cuda',\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=50,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "models['xgb2'] = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.02,\n",
    "    subsample=0.5,\n",
    "    colsample_bytree=0.5,\n",
    "    reg_alpha=2.0,\n",
    "    reg_lambda=10.0,\n",
    "    min_child_weight=20,\n",
    "    gamma=1.0,\n",
    "    tree_method='hist',\n",
    "    device='cuda',\n",
    "    random_state=43,\n",
    "    early_stopping_rounds=50,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# LightGBM - Strong Regularization\n",
    "print(\"ðŸ“Š Training LightGBM...\")\n",
    "models['lgb1'] = lgb.LGBMClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.01,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_alpha=1.0,\n",
    "    reg_lambda=5.0,\n",
    "    min_child_samples=50,\n",
    "    device='gpu',\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "models['lgb2'] = lgb.LGBMClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.02,\n",
    "    subsample=0.5,\n",
    "    colsample_bytree=0.5,\n",
    "    reg_alpha=2.0,\n",
    "    reg_lambda=10.0,\n",
    "    min_child_samples=100,\n",
    "    device='gpu',\n",
    "    random_state=43,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# CatBoost - Strong Regularization\n",
    "print(\"ðŸ“Š Training CatBoost...\")\n",
    "models['cat1'] = CatBoostClassifier(\n",
    "    iterations=300,\n",
    "    depth=4,\n",
    "    learning_rate=0.01,\n",
    "    l2_leaf_reg=10,\n",
    "    min_data_in_leaf=50,\n",
    "    task_type='GPU',\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "models['cat2'] = CatBoostClassifier(\n",
    "    iterations=300,\n",
    "    depth=3,\n",
    "    learning_rate=0.02,\n",
    "    l2_leaf_reg=20,\n",
    "    min_data_in_leaf=100,\n",
    "    task_type='GPU',\n",
    "    random_state=43,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Created {len(models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43e91fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Training xgb1...\n",
      "   Train: 69.9%, Test: 68.3%, Gap: +1.6% âœ“\n",
      "   Time: 24.1s\n",
      "\n",
      "ðŸ”§ Training xgb2...\n",
      "   Train: 69.9%, Test: 68.3%, Gap: +1.6% âœ“\n",
      "   Time: 24.1s\n",
      "\n",
      "ðŸ”§ Training xgb2...\n",
      "   Train: 70.0%, Test: 68.4%, Gap: +1.6% âœ“\n",
      "   Time: 7.7s\n",
      "\n",
      "ðŸ”§ Training lgb1...\n",
      "   Train: 70.0%, Test: 68.4%, Gap: +1.6% âœ“\n",
      "   Time: 7.7s\n",
      "\n",
      "ðŸ”§ Training lgb1...\n",
      "   Train: 69.9%, Test: 68.3%, Gap: +1.6% âœ“\n",
      "   Time: 10.9s\n",
      "\n",
      "ðŸ”§ Training lgb2...\n",
      "   Train: 69.9%, Test: 68.3%, Gap: +1.6% âœ“\n",
      "   Time: 10.9s\n",
      "\n",
      "ðŸ”§ Training lgb2...\n",
      "   Train: 70.0%, Test: 68.3%, Gap: +1.7% âœ“\n",
      "   Time: 6.5s\n",
      "\n",
      "ðŸ”§ Training cat1...\n",
      "   Train: 70.0%, Test: 68.3%, Gap: +1.7% âœ“\n",
      "   Time: 6.5s\n",
      "\n",
      "ðŸ”§ Training cat1...\n",
      "   Train: 69.6%, Test: 67.9%, Gap: +1.6% âœ“\n",
      "   Time: 34.9s\n",
      "\n",
      "ðŸ”§ Training cat2...\n",
      "   Train: 69.6%, Test: 67.9%, Gap: +1.6% âœ“\n",
      "   Time: 34.9s\n",
      "\n",
      "ðŸ”§ Training cat2...\n",
      "   Train: 69.8%, Test: 68.2%, Gap: +1.6% âœ“\n",
      "   Time: 91.0s\n",
      "\n",
      "âœ“ Trained 6 models\n",
      "   Train: 69.8%, Test: 68.2%, Gap: +1.6% âœ“\n",
      "   Time: 91.0s\n",
      "\n",
      "âœ“ Trained 6 models\n"
     ]
    }
   ],
   "source": [
    "# Train models\n",
    "trained_models = {}\n",
    "accuracies = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nðŸ”§ Training {name}...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        if 'xgb' in name:\n",
    "            # XGBoost with eval set for early stopping\n",
    "            model.fit(\n",
    "                X_train_scaled, y_train,\n",
    "                eval_set=[(X_test_scaled, y_test)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        train_time = time.time() - start\n",
    "        \n",
    "        # Evaluate\n",
    "        train_pred = model.predict(X_train_scaled)\n",
    "        test_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        test_acc = accuracy_score(y_test, test_pred)\n",
    "        gap = train_acc - test_acc\n",
    "        \n",
    "        trained_models[name] = model\n",
    "        accuracies[name] = test_acc\n",
    "        \n",
    "        overfit = \"âš ï¸\" if gap > 0.05 else \"âœ“\"\n",
    "        print(f\"   Train: {train_acc*100:.1f}%, Test: {test_acc*100:.1f}%, Gap: {gap*100:+.1f}% {overfit}\")\n",
    "        print(f\"   Time: {train_time:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error: {e}\")\n",
    "\n",
    "print(f\"\\nâœ“ Trained {len(trained_models)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134fb702",
   "metadata": {},
   "source": [
    "## 6. Ensemble & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce3c15f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ“Š ENSEMBLE EVALUATION\n",
      "======================================================================\n",
      "\n",
      "Model Weights:\n",
      "  xgb2: 0.167\n",
      "  xgb1: 0.167\n",
      "  lgb2: 0.167\n",
      "  lgb1: 0.167\n",
      "  cat2: 0.167\n",
      "  cat1: 0.166\n",
      "\n",
      "Confidence Distribution:\n",
      "  >= 90%: 0\n",
      "  >= 80%: 0\n",
      "  >= 70%: 26\n",
      "  >= 60%: 1,595\n",
      "\n",
      "Confidence Distribution:\n",
      "  >= 90%: 0\n",
      "  >= 80%: 0\n",
      "  >= 70%: 26\n",
      "  >= 60%: 1,595\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ“Š ENSEMBLE EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate weights based on accuracy\n",
    "total_acc = sum(accuracies.values())\n",
    "weights = {name: acc/total_acc for name, acc in accuracies.items()}\n",
    "\n",
    "print(\"\\nModel Weights:\")\n",
    "for name, w in sorted(weights.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {name}: {w:.3f}\")\n",
    "\n",
    "# Get predictions\n",
    "proba_dict = {}\n",
    "preds_dict = {}\n",
    "for name, model in trained_models.items():\n",
    "    proba_dict[name] = model.predict_proba(X_test_scaled)\n",
    "    preds_dict[name] = model.predict(X_test_scaled)\n",
    "\n",
    "# Weighted ensemble probability\n",
    "ensemble_proba = np.zeros((len(X_test_scaled), 2))\n",
    "for name, proba in proba_dict.items():\n",
    "    ensemble_proba += weights[name] * proba\n",
    "\n",
    "# WIN probability\n",
    "win_prob = ensemble_proba[:, 1] * 100\n",
    "\n",
    "# Agreement bonus\n",
    "all_preds = np.array([preds_dict[name] for name in trained_models.keys()])\n",
    "win_votes = np.sum(all_preds == 1, axis=0)\n",
    "\n",
    "all_agree = win_votes == len(trained_models)\n",
    "strong_agree = win_votes >= (len(trained_models) - 1)\n",
    "majority = win_votes >= (len(trained_models) // 2 + 1)\n",
    "\n",
    "confidence = win_prob.copy()\n",
    "confidence[all_agree] = np.minimum(confidence[all_agree] + 5, 100)\n",
    "confidence[strong_agree & ~all_agree] = np.minimum(confidence[strong_agree & ~all_agree] + 3, 100)\n",
    "confidence[majority & ~strong_agree] = np.minimum(confidence[majority & ~strong_agree] + 1, 100)\n",
    "\n",
    "print(f\"\\nConfidence Distribution:\")\n",
    "print(f\"  >= 90%: {(confidence >= 90).sum():,}\")\n",
    "print(f\"  >= 80%: {(confidence >= 80).sum():,}\")\n",
    "print(f\"  >= 70%: {(confidence >= 70).sum():,}\")\n",
    "print(f\"  >= 60%: {(confidence >= 60).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b529579c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ V13 Results by Threshold:\n",
      "--------------------------------------------------------------------------------\n",
      " Threshold |    Signals |       Wins |   Win Rate |         PF\n",
      "--------------------------------------------------------------------------------\n",
      "      50%+ |      7,432 |      4,912 |      66.1% |      2.92 ðŸŽ¯\n",
      "      55%+ |      5,111 |      3,509 |      68.7% |      3.29 ðŸŽ¯\n",
      "      60%+ |      1,595 |      1,247 |      78.2% |      5.38 ðŸŽ¯\n",
      "      65%+ |        107 |         99 |      92.5% |     18.56 ðŸŽ¯\n",
      "      70%+ |         26 |         25 |      96.2% |     37.50 ðŸŽ¯\n",
      "      75%+ |          0 |          - |          - |          -\n",
      "      80%+ |          0 |          - |          - |          -\n",
      "      85%+ |          0 |          - |          - |          -\n",
      "      90%+ |          0 |          - |          - |          -\n"
     ]
    }
   ],
   "source": [
    "# Results by threshold\n",
    "print(\"\\nðŸ“ˆ V13 Results by Threshold:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Threshold':>10} | {'Signals':>10} | {'Wins':>10} | {'Win Rate':>10} | {'PF':>10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "v13_results = {}\n",
    "for threshold in [50, 55, 60, 65, 70, 75, 80, 85, 90]:\n",
    "    mask = confidence >= threshold\n",
    "    if mask.sum() > 0:\n",
    "        signals = mask.sum()\n",
    "        wins = y_test[mask].sum()\n",
    "        losses = signals - wins\n",
    "        win_rate = wins / signals * 100\n",
    "        pf = (wins * TP_PIPS) / max(1, losses * SL_PIPS)\n",
    "        \n",
    "        v13_results[threshold] = {\n",
    "            'signals': signals,\n",
    "            'wins': wins,\n",
    "            'losses': losses,\n",
    "            'win_rate': win_rate,\n",
    "            'pf': pf\n",
    "        }\n",
    "        \n",
    "        marker = \"ðŸŽ¯\" if win_rate >= 50 else \"\"\n",
    "        print(f\"{threshold:>8}%+ | {signals:>10,} | {wins:>10,} | {win_rate:>9.1f}% | {pf:>9.2f} {marker}\")\n",
    "    else:\n",
    "        print(f\"{threshold:>8}%+ | {'0':>10} | {'-':>10} | {'-':>10} | {'-':>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06358749",
   "metadata": {},
   "source": [
    "## 7. Overfit Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8198c298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ” OVERFIT CHECK: Train vs Test\n",
      "======================================================================\n",
      "\n",
      " Threshold |  Train Sig |   Train WR |   Test Sig |    Test WR |      Gap\n",
      "--------------------------------------------------------------------------------\n",
      "      50%+ |     38,017 |      63.4% |      7,432 |      66.1% |    -2.7% âœ“\n",
      "      60%+ |      8,495 |      72.5% |      1,595 |      78.2% |    -5.7% âš¡\n",
      "      70%+ |        142 |      96.5% |         26 |      96.2% |    +0.3% âœ“\n",
      "      80%+ |          0 |       0.0% |          0 |       0.0% |    +0.0% âœ“\n",
      "      90%+ |          0 |       0.0% |          0 |       0.0% |    +0.0% âœ“\n",
      "\n",
      " Threshold |  Train Sig |   Train WR |   Test Sig |    Test WR |      Gap\n",
      "--------------------------------------------------------------------------------\n",
      "      50%+ |     38,017 |      63.4% |      7,432 |      66.1% |    -2.7% âœ“\n",
      "      60%+ |      8,495 |      72.5% |      1,595 |      78.2% |    -5.7% âš¡\n",
      "      70%+ |        142 |      96.5% |         26 |      96.2% |    +0.3% âœ“\n",
      "      80%+ |          0 |       0.0% |          0 |       0.0% |    +0.0% âœ“\n",
      "      90%+ |          0 |       0.0% |          0 |       0.0% |    +0.0% âœ“\n"
     ]
    }
   ],
   "source": [
    "# Check train vs test\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ” OVERFIT CHECK: Train vs Test\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get train predictions\n",
    "train_proba_dict = {}\n",
    "train_preds_dict = {}\n",
    "for name, model in trained_models.items():\n",
    "    train_proba_dict[name] = model.predict_proba(X_train_scaled)\n",
    "    train_preds_dict[name] = model.predict(X_train_scaled)\n",
    "\n",
    "train_ensemble = np.zeros((len(X_train_scaled), 2))\n",
    "for name, proba in train_proba_dict.items():\n",
    "    train_ensemble += weights[name] * proba\n",
    "\n",
    "train_win_prob = train_ensemble[:, 1] * 100\n",
    "\n",
    "# Agreement\n",
    "train_all_preds = np.array([train_preds_dict[name] for name in trained_models.keys()])\n",
    "train_votes = np.sum(train_all_preds == 1, axis=0)\n",
    "train_all_agree = train_votes == len(trained_models)\n",
    "train_strong = train_votes >= (len(trained_models) - 1)\n",
    "train_majority = train_votes >= (len(trained_models) // 2 + 1)\n",
    "\n",
    "train_conf = train_win_prob.copy()\n",
    "train_conf[train_all_agree] = np.minimum(train_conf[train_all_agree] + 5, 100)\n",
    "train_conf[train_strong & ~train_all_agree] = np.minimum(train_conf[train_strong & ~train_all_agree] + 3, 100)\n",
    "train_conf[train_majority & ~train_strong] = np.minimum(train_conf[train_majority & ~train_strong] + 1, 100)\n",
    "\n",
    "print(f\"\\n{'Threshold':>10} | {'Train Sig':>10} | {'Train WR':>10} | {'Test Sig':>10} | {'Test WR':>10} | {'Gap':>8}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for threshold in [50, 60, 70, 80, 90]:\n",
    "    train_mask = train_conf >= threshold\n",
    "    test_mask = confidence >= threshold\n",
    "    \n",
    "    train_sig = train_mask.sum()\n",
    "    test_sig = test_mask.sum()\n",
    "    \n",
    "    train_wr = y_train[train_mask].mean() * 100 if train_sig > 0 else 0\n",
    "    test_wr = y_test[test_mask].mean() * 100 if test_sig > 0 else 0\n",
    "    \n",
    "    gap = train_wr - test_wr\n",
    "    status = \"âœ“\" if abs(gap) < 5 else (\"âš¡\" if abs(gap) < 10 else \"âš ï¸\")\n",
    "    \n",
    "    print(f\"{threshold:>8}%+ | {train_sig:>10,} | {train_wr:>9.1f}% | {test_sig:>10,} | {test_wr:>9.1f}% | {gap:>+7.1f}% {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b59375",
   "metadata": {},
   "source": [
    "## 8. Save V13 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b88904d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ’¾ SAVING V13 MODELS\n",
      "======================================================================\n",
      "âœ“ xgb1_v13.joblib\n",
      "âœ“ xgb2_v13.joblib\n",
      "âœ“ lgb1_v13.joblib\n",
      "âœ“ lgb2_v13.joblib\n",
      "âœ“ cat1_v13.joblib\n",
      "âœ“ cat2_v13.joblib\n",
      "âœ“ scaler_v13.joblib\n",
      "âœ“ feature_cols_v13.joblib\n",
      "âœ“ weights_v13.joblib\n",
      "âœ“ config_v13.joblib\n",
      "\n",
      "âœ… All V13 models saved to: c:\\Users\\Acer\\Desktop\\Forex-Signal-App\\models\\signal_generator_v13\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ’¾ SAVING V13 MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save models\n",
    "for name, model in trained_models.items():\n",
    "    joblib.dump(model, MODEL_DIR / f'{name}_v13.joblib')\n",
    "    print(f\"âœ“ {name}_v13.joblib\")\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, MODEL_DIR / 'scaler_v13.joblib')\n",
    "print(f\"âœ“ scaler_v13.joblib\")\n",
    "\n",
    "# Save features\n",
    "joblib.dump(feature_cols, MODEL_DIR / 'feature_cols_v13.joblib')\n",
    "print(f\"âœ“ feature_cols_v13.joblib\")\n",
    "\n",
    "# Save weights\n",
    "joblib.dump(weights, MODEL_DIR / 'weights_v13.joblib')\n",
    "print(f\"âœ“ weights_v13.joblib\")\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    'version': 'v13',\n",
    "    'labeling': 'TP/SL Based',\n",
    "    'tp_pips': TP_PIPS,\n",
    "    'sl_pips': SL_PIPS,\n",
    "    'forward_bars': FORWARD_BARS,\n",
    "    'models': list(trained_models.keys()),\n",
    "    'features': len(feature_cols),\n",
    "    'accuracies': accuracies,\n",
    "    'results': v13_results\n",
    "}\n",
    "joblib.dump(config, MODEL_DIR / 'config_v13.joblib')\n",
    "print(f\"âœ“ config_v13.joblib\")\n",
    "\n",
    "print(f\"\\nâœ… All V13 models saved to: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c99320c",
   "metadata": {},
   "source": [
    "## 9. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd5661b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸŽ¯ V13 FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š TRAINING SETUP:\n",
      "   Labeling: TP/SL Based (TP=15 pips, SL=10 pips)\n",
      "   Models: 6 (xgb1, xgb2, lgb1, lgb2, cat1, cat2)\n",
      "   Features: 50\n",
      "   Regularization: Strong (max_depth=3-4, reg_alpha/lambda high)\n",
      "\n",
      "ðŸ“ˆ BEST RESULT (Threshold 65%+):\n",
      "   Signals: 107\n",
      "   Wins: 99\n",
      "   Losses: 8\n",
      "   Win Rate: 92.5%\n",
      "   Profit Factor: 18.56\n",
      "\n",
      "ðŸ’° EXPECTED PROFIT (1000 trades @ $1/pip):\n",
      "   Gross Profit: $1,485\n",
      "   Gross Loss: $80\n",
      "   Net Profit: $1,405\n",
      "\n",
      "================================================================================\n",
      "âœ… V13 TRAINING COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸŽ¯ V13 FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best threshold\n",
    "best_th = max(v13_results.keys(), key=lambda x: v13_results[x]['pf'] if v13_results[x]['signals'] >= 100 else 0)\n",
    "best = v13_results[best_th]\n",
    "\n",
    "print(f\"\"\"\n",
    "ðŸ“Š TRAINING SETUP:\n",
    "   Labeling: TP/SL Based (TP={TP_PIPS} pips, SL={SL_PIPS} pips)\n",
    "   Models: {len(trained_models)} ({', '.join(trained_models.keys())})\n",
    "   Features: {len(feature_cols)}\n",
    "   Regularization: Strong (max_depth=3-4, reg_alpha/lambda high)\n",
    "\n",
    "ðŸ“ˆ BEST RESULT (Threshold {best_th}%+):\n",
    "   Signals: {best['signals']:,}\n",
    "   Wins: {best['wins']:,}\n",
    "   Losses: {best['losses']:,}\n",
    "   Win Rate: {best['win_rate']:.1f}%\n",
    "   Profit Factor: {best['pf']:.2f}\n",
    "\n",
    "ðŸ’° EXPECTED PROFIT (1000 trades @ $1/pip):\n",
    "   Gross Profit: ${best['wins'] * TP_PIPS:,}\n",
    "   Gross Loss: ${best['losses'] * SL_PIPS:,}\n",
    "   Net Profit: ${best['wins'] * TP_PIPS - best['losses'] * SL_PIPS:,}\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"âœ… V13 TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
