{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469e9799",
   "metadata": {},
   "source": [
    "# forex_signal_v26 — PF-first Multi-Timeframe Hierarchical Ensemble\n",
    "Энэ notebook нь:\n",
    "- **4H + 1H Macro Gate** (LONG зөвшөөрөх эсэх)\n",
    "- **15m Edge Model (XGBoost)** (TP хүрэх магадлал)\n",
    "- **1m Entry Quality Model (LogReg)** (муу entry-г шүүх)\n",
    "- **Next-open execution**, spread/slippage, 1% risk sizing, triple-barrier label\n",
    "\n",
    "Дата бүтэц:\n",
    "- `data/train/` → 2015–2023\n",
    "- `data/test/` → 2024–2025\n",
    "\n",
    "Timeframe файлуудыг `*1m*.csv`, `*15m*.csv`, `*1h*.csv`, `*4h*.csv` гэх мэт нэрээр байрлуулсан гэж таамаглана.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1727ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(train_dir='../data/train', test_dir='../data/test', trade_tf='15m', macro_tfs=('1h', '4h'), entry_tf='1m', train_core_end='2021-12-31', val_start='2022-01-01', val_end='2023-12-31', atr_period=14, tp_atr=2.0, sl_atr=1.5, max_hold_bars=16, starting_equity=10000.0, risk_per_trade=0.01, spread_pips=1.2, slippage_pips=0.2, commission_per_lot_usd=0.0, edge_th=0.6, entry_th=0.55, pip_size=0.0001, lot_size=100000, usd_per_pip_per_lot=10.0)\n"
     ]
    }
   ],
   "source": [
    "# ===== 0) Imports & Global Config =====\n",
    "import os, re, glob, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Adjusted paths to step up from ml_models_train/ to root\n",
    "    train_dir: str = \"../data/train\"\n",
    "    test_dir: str = \"../data/test\"\n",
    "\n",
    "    trade_tf: str = \"15m\"\n",
    "    macro_tfs: tuple = (\"1h\", \"4h\")\n",
    "    entry_tf: str = \"1m\"\n",
    "\n",
    "    # Validation split inside train\n",
    "    train_core_end: str = \"2021-12-31\"\n",
    "    val_start: str = \"2022-01-01\"\n",
    "    val_end: str = \"2023-12-31\"\n",
    "\n",
    "    # Triple barrier defaults (tune on validation)\n",
    "    atr_period: int = 14\n",
    "    tp_atr: float = 2.0\n",
    "    sl_atr: float = 1.5\n",
    "    max_hold_bars: int = 16  # 16 * 15m = 4h\n",
    "\n",
    "    # Execution / costs\n",
    "    starting_equity: float = 10_000.0\n",
    "    risk_per_trade: float = 0.01  # 1%\n",
    "    spread_pips: float = 1.2      # change to your broker typical\n",
    "    slippage_pips: float = 0.2\n",
    "    commission_per_lot_usd: float = 0.0  # set if needed\n",
    "\n",
    "    # Decision thresholds (tune on validation)\n",
    "    edge_th: float = 0.60\n",
    "    entry_th: float = 0.55\n",
    "\n",
    "    # Forex pip conventions (EURUSD)\n",
    "    pip_size: float = 0.0001\n",
    "    lot_size: float = 100_000  # 1.0 lot = 100k base\n",
    "    usd_per_pip_per_lot: float = 10.0  # approx for EURUSD (USD quote)\n",
    "\n",
    "CFG = Config()\n",
    "print(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bdca7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1m from ../data/train\\EURUSD_m1.csv rows=3,354,904\n",
      "Loaded 4h from ../data/train\\EURUSD_h4.csv rows=14,498\n",
      "Loaded 15m from ../data/train\\EURUSD_m15.csv rows=224,382\n",
      "Loaded 1h from ../data/train\\EURUSD_h1.csv rows=56,098\n",
      "Loaded 1m from ../data/test\\EURUSD_m1.csv rows=743,476\n",
      "Loaded 4h from ../data/test\\EURUSD_h4.csv rows=3,220\n",
      "Loaded 15m from ../data/test\\EURUSD_m15.csv rows=49,807\n",
      "Loaded 1h from ../data/test\\EURUSD_h1.csv rows=12,454\n"
     ]
    }
   ],
   "source": [
    "# ===== 1) Data Loading (robust to filenames) =====\n",
    "def _find_csv_by_tf(folder: str, tf: str) -> str:\n",
    "    # Accept patterns like *15m*.csv, *15min*.csv, etc.\n",
    "    patterns = [\n",
    "        f\"*{tf}*.csv\",\n",
    "        f\"*{tf.replace('m','min')}*.csv\",\n",
    "        f\"*{tf.upper()}*.csv\",\n",
    "        f\"*{tf.lower()}*.csv\",\n",
    "    ]\n",
    "    \n",
    "    # Add reversed pattern for filenames like EURUSD_m15.csv (tf=\"15m\")\n",
    "    if len(tf) > 1 and tf[-1].isalpha() and tf[:-1].isdigit():\n",
    "        rev = f\"{tf[-1]}{tf[:-1]}\" # 15m -> m15\n",
    "        patterns.append(f\"*{rev}*.csv\")\n",
    "        patterns.append(f\"*{rev.upper()}*.csv\")\n",
    "        patterns.append(f\"*{rev.lower()}*.csv\")\n",
    "        \n",
    "    files = []\n",
    "    for p in patterns:\n",
    "        files.extend(glob.glob(os.path.join(folder, p)))\n",
    "    files = sorted(set(files))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSV found for tf='{tf}' in {folder}. Looked for patterns: {patterns}\")\n",
    "    # Prefer the shortest name (often the intended one)\n",
    "    files = sorted(files, key=lambda x: len(os.path.basename(x)))\n",
    "    return files[0]\n",
    "\n",
    "def load_ohlcv(folder: str, tf: str) -> pd.DataFrame:\n",
    "    path = _find_csv_by_tf(folder, tf)\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Heuristic column mapping\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    # timestamp column\n",
    "    ts_col = None\n",
    "    for k in [\"timestamp\", \"time\", \"date\", \"datetime\"]:\n",
    "        if k in cols:\n",
    "            ts_col = cols[k]\n",
    "            break\n",
    "    if ts_col is None:\n",
    "        raise ValueError(f\"Could not find timestamp column in {path}. Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # OHLC\n",
    "    def pick(names):\n",
    "        for n in names:\n",
    "            if n in cols:\n",
    "                return cols[n]\n",
    "        return None\n",
    "\n",
    "    o = pick([\"open\",\"o\"])\n",
    "    h = pick([\"high\",\"h\"])\n",
    "    l = pick([\"low\",\"l\"])\n",
    "    c = pick([\"close\",\"c\"])\n",
    "    v = pick([\"volume\",\"vol\",\"tick_volume\"])\n",
    "    required = [o,h,l,c]\n",
    "    if any(x is None for x in required):\n",
    "        raise ValueError(f\"Missing OHLC in {path}. Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    out = df[[ts_col, o,h,l,c] + ([v] if v is not None else [])].copy()\n",
    "    out.columns = [\"timestamp\",\"open\",\"high\",\"low\",\"close\"] + ([\"volume\"] if v is not None else [])\n",
    "    out[\"timestamp\"] = pd.to_datetime(out[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "    out = out.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    # Make sure numeric\n",
    "    for col in [\"open\",\"high\",\"low\",\"close\"] + ([\"volume\"] if \"volume\" in out.columns else []):\n",
    "        out[col] = pd.to_numeric(out[col], errors=\"coerce\")\n",
    "    out = out.dropna(subset=[\"open\",\"high\",\"low\",\"close\"]).reset_index(drop=True)\n",
    "\n",
    "    out.attrs[\"source_path\"] = path\n",
    "    return out\n",
    "\n",
    "def load_all_timeframes(base_dir: str) -> dict:\n",
    "    data = {}\n",
    "    # We load only what this architecture uses; extend if you want.\n",
    "    for tf in set([CFG.trade_tf, CFG.entry_tf, *CFG.macro_tfs]):\n",
    "        data[tf] = load_ohlcv(base_dir, tf)\n",
    "        print(f\"Loaded {tf} from {data[tf].attrs.get('source_path')} rows={len(data[tf]):,}\")\n",
    "    return data\n",
    "\n",
    "train_raw = load_all_timeframes(CFG.train_dir)\n",
    "test_raw  = load_all_timeframes(CFG.test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c599ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 2) Technical Indicators (pure pandas, no TA-Lib) =====\n",
    "def ema(s: pd.Series, span: int) -> pd.Series:\n",
    "    return s.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "def rsi(close: pd.Series, period: int = 14) -> pd.Series:\n",
    "    delta = close.diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -delta.clip(upper=0)\n",
    "    ma_up = up.ewm(alpha=1/period, adjust=False).mean()\n",
    "    ma_down = down.ewm(alpha=1/period, adjust=False).mean()\n",
    "    rs = ma_up / (ma_down.replace(0, np.nan))\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def true_range(df: pd.DataFrame) -> pd.Series:\n",
    "    prev_close = df[\"close\"].shift(1)\n",
    "    tr = pd.concat([\n",
    "        (df[\"high\"] - df[\"low\"]),\n",
    "        (df[\"high\"] - prev_close).abs(),\n",
    "        (df[\"low\"]  - prev_close).abs(),\n",
    "    ], axis=1).max(axis=1)\n",
    "    return tr\n",
    "\n",
    "def atr(df: pd.DataFrame, period: int = 14) -> pd.Series:\n",
    "    tr = true_range(df)\n",
    "    return tr.ewm(alpha=1/period, adjust=False).mean()\n",
    "\n",
    "def adx(df: pd.DataFrame, period: int = 14) -> pd.Series:\n",
    "    high, low, close = df[\"high\"], df[\"low\"], df[\"close\"]\n",
    "    up_move = high.diff()\n",
    "    down_move = -low.diff()\n",
    "    plus_dm = np.where((up_move > down_move) & (up_move > 0), up_move, 0.0)\n",
    "    minus_dm = np.where((down_move > up_move) & (down_move > 0), down_move, 0.0)\n",
    "    tr = true_range(df)\n",
    "    atr_ = tr.ewm(alpha=1/period, adjust=False).mean()\n",
    "    plus_di = 100 * pd.Series(plus_dm, index=df.index).ewm(alpha=1/period, adjust=False).mean() / (atr_.replace(0, np.nan))\n",
    "    minus_di = 100 * pd.Series(minus_dm, index=df.index).ewm(alpha=1/period, adjust=False).mean() / (atr_.replace(0, np.nan))\n",
    "    dx = (100 * (plus_di - minus_di).abs() / ((plus_di + minus_di).replace(0, np.nan)))\n",
    "    return dx.ewm(alpha=1/period, adjust=False).mean()\n",
    "\n",
    "def macd_hist(close: pd.Series, fast: int=12, slow: int=26, signal: int=9) -> pd.Series:\n",
    "    macd = ema(close, fast) - ema(close, slow)\n",
    "    sig = ema(macd, signal)\n",
    "    return macd - sig\n",
    "\n",
    "def bb_width(close: pd.Series, period: int=20, n_std: float=2.0) -> pd.Series:\n",
    "    ma = close.rolling(period).mean()\n",
    "    sd = close.rolling(period).std()\n",
    "    upper = ma + n_std * sd\n",
    "    lower = ma - n_std * sd\n",
    "    width = (upper - lower) / (ma.replace(0, np.nan))\n",
    "    return width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ca171b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df (224382, 42) test_df (49807, 42)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ret1</th>\n",
       "      <th>ret3</th>\n",
       "      <th>ret6</th>\n",
       "      <th>ret12</th>\n",
       "      <th>ema20</th>\n",
       "      <th>ema50</th>\n",
       "      <th>ema200</th>\n",
       "      <th>ema50_slope</th>\n",
       "      <th>rsi14</th>\n",
       "      <th>macd_hist</th>\n",
       "      <th>atr14</th>\n",
       "      <th>bb_width</th>\n",
       "      <th>dist_ema50_atr</th>\n",
       "      <th>dist_ema200_atr</th>\n",
       "      <th>range_atr</th>\n",
       "      <th>body_atr</th>\n",
       "      <th>1h_ema200</th>\n",
       "      <th>1h_ema50</th>\n",
       "      <th>1h_ema200_slope</th>\n",
       "      <th>1h_adx14</th>\n",
       "      <th>1h_atr14</th>\n",
       "      <th>1h_bb_width</th>\n",
       "      <th>1h_dist_ema200_atr</th>\n",
       "      <th>4h_ema200</th>\n",
       "      <th>4h_ema50</th>\n",
       "      <th>4h_ema200_slope</th>\n",
       "      <th>4h_adx14</th>\n",
       "      <th>4h_atr14</th>\n",
       "      <th>4h_bb_width</th>\n",
       "      <th>4h_dist_ema200_atr</th>\n",
       "      <th>m1_ret_mean</th>\n",
       "      <th>m1_ret_std</th>\n",
       "      <th>m1_range_mean</th>\n",
       "      <th>m1_range_max</th>\n",
       "      <th>m1_wick_down_mean</th>\n",
       "      <th>m1_wick_up_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 22:00:00+00:00</td>\n",
       "      <td>1.21038</td>\n",
       "      <td>1.21064</td>\n",
       "      <td>1.21025</td>\n",
       "      <td>1.21035</td>\n",
       "      <td>244.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.210350</td>\n",
       "      <td>1.210350</td>\n",
       "      <td>1.210350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>1.21014</td>\n",
       "      <td>1.21014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00074</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.20869</td>\n",
       "      <td>1.20869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 22:15:00+00:00</td>\n",
       "      <td>1.21038</td>\n",
       "      <td>1.21073</td>\n",
       "      <td>1.20999</td>\n",
       "      <td>1.21016</td>\n",
       "      <td>360.07</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.210332</td>\n",
       "      <td>1.210343</td>\n",
       "      <td>1.210348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.439877</td>\n",
       "      <td>-0.453276</td>\n",
       "      <td>1.783133</td>\n",
       "      <td>0.530120</td>\n",
       "      <td>1.21014</td>\n",
       "      <td>1.21014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00074</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.20869</td>\n",
       "      <td>1.20869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.00029</td>\n",
       "      <td>0.200998</td>\n",
       "      <td>0.195416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 22:30:00+00:00</td>\n",
       "      <td>1.21020</td>\n",
       "      <td>1.21039</td>\n",
       "      <td>1.21010</td>\n",
       "      <td>1.21039</td>\n",
       "      <td>238.65</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.210337</td>\n",
       "      <td>1.210344</td>\n",
       "      <td>1.210349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.518519</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.112271</td>\n",
       "      <td>0.102134</td>\n",
       "      <td>0.714160</td>\n",
       "      <td>0.467898</td>\n",
       "      <td>1.21014</td>\n",
       "      <td>1.21014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00074</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.20869</td>\n",
       "      <td>1.20869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.00040</td>\n",
       "      <td>0.151082</td>\n",
       "      <td>0.188161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp     open     high      low    close  volume  \\\n",
       "0 2015-01-01 22:00:00+00:00  1.21038  1.21064  1.21025  1.21035  244.30   \n",
       "1 2015-01-01 22:15:00+00:00  1.21038  1.21073  1.20999  1.21016  360.07   \n",
       "2 2015-01-01 22:30:00+00:00  1.21020  1.21039  1.21010  1.21039  238.65   \n",
       "\n",
       "       ret1  ret3  ret6  ret12     ema20     ema50    ema200  ema50_slope  \\\n",
       "0       NaN   NaN   NaN    NaN  1.210350  1.210350  1.210350          NaN   \n",
       "1 -0.000157   NaN   NaN    NaN  1.210332  1.210343  1.210348          NaN   \n",
       "2  0.000190   NaN   NaN    NaN  1.210337  1.210344  1.210349          NaN   \n",
       "\n",
       "      rsi14  macd_hist     atr14  bb_width  dist_ema50_atr  dist_ema200_atr  \\\n",
       "0       NaN   0.000000  0.000390       NaN        0.000000         0.000000   \n",
       "1  0.000000  -0.000012  0.000415       NaN       -0.439877        -0.453276   \n",
       "2  8.518519  -0.000004  0.000406       NaN        0.112271         0.102134   \n",
       "\n",
       "   range_atr  body_atr  1h_ema200  1h_ema50  1h_ema200_slope  1h_adx14  \\\n",
       "0   1.000000  0.076923    1.21014   1.21014              NaN       NaN   \n",
       "1   1.783133  0.530120    1.21014   1.21014              NaN       NaN   \n",
       "2   0.714160  0.467898    1.21014   1.21014              NaN       NaN   \n",
       "\n",
       "   1h_atr14  1h_bb_width  1h_dist_ema200_atr  4h_ema200  4h_ema50  \\\n",
       "0   0.00074          NaN                 0.0    1.20869   1.20869   \n",
       "1   0.00074          NaN                 0.0    1.20869   1.20869   \n",
       "2   0.00074          NaN                 0.0    1.20869   1.20869   \n",
       "\n",
       "   4h_ema200_slope  4h_adx14  4h_atr14  4h_bb_width  4h_dist_ema200_atr  \\\n",
       "0              NaN       NaN    0.0029          NaN                 0.0   \n",
       "1              NaN       NaN    0.0029          NaN                 0.0   \n",
       "2              NaN       NaN    0.0029          NaN                 0.0   \n",
       "\n",
       "   m1_ret_mean  m1_ret_std  m1_range_mean  m1_range_max  m1_wick_down_mean  \\\n",
       "0          NaN         NaN            NaN           NaN                NaN   \n",
       "1    -0.000001    0.000045       0.000113       0.00029           0.200998   \n",
       "2    -0.000010    0.000060       0.000109       0.00040           0.151082   \n",
       "\n",
       "   m1_wick_up_mean  \n",
       "0              NaN  \n",
       "1         0.195416  \n",
       "2         0.188161  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== 3) Feature Builders =====\n",
    "def add_macro_features(df: pd.DataFrame, prefix: str) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[f\"{prefix}_ema200\"] = ema(out[\"close\"], 200)\n",
    "    out[f\"{prefix}_ema50\"]  = ema(out[\"close\"], 50)\n",
    "    out[f\"{prefix}_ema200_slope\"] = out[f\"{prefix}_ema200\"].diff(3)\n",
    "    out[f\"{prefix}_adx14\"] = adx(out, 14)\n",
    "    out[f\"{prefix}_atr14\"] = atr(out, CFG.atr_period)\n",
    "    out[f\"{prefix}_bb_width\"] = bb_width(out[\"close\"], 20, 2.0)\n",
    "    # Normalize distances by ATR\n",
    "    out[f\"{prefix}_dist_ema200_atr\"] = (out[\"close\"] - out[f\"{prefix}_ema200\"]) / (out[f\"{prefix}_atr14\"].replace(0, np.nan))\n",
    "    return out\n",
    "\n",
    "def add_trade_features_15m(df15: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df15.copy()\n",
    "    out[\"ret1\"] = np.log(out[\"close\"]).diff()\n",
    "    out[\"ret3\"] = out[\"ret1\"].rolling(3).sum()\n",
    "    out[\"ret6\"] = out[\"ret1\"].rolling(6).sum()\n",
    "    out[\"ret12\"] = out[\"ret1\"].rolling(12).sum()\n",
    "\n",
    "    out[\"ema20\"] = ema(out[\"close\"], 20)\n",
    "    out[\"ema50\"] = ema(out[\"close\"], 50)\n",
    "    out[\"ema200\"] = ema(out[\"close\"], 200)\n",
    "    out[\"ema50_slope\"] = out[\"ema50\"].diff(3)\n",
    "    out[\"rsi14\"] = rsi(out[\"close\"], 14)\n",
    "    out[\"macd_hist\"] = macd_hist(out[\"close\"], 12, 26, 9)\n",
    "    out[\"atr14\"] = atr(out, CFG.atr_period)\n",
    "    out[\"bb_width\"] = bb_width(out[\"close\"], 20, 2.0)\n",
    "\n",
    "    out[\"dist_ema50_atr\"] = (out[\"close\"] - out[\"ema50\"]) / (out[\"atr14\"].replace(0, np.nan))\n",
    "    out[\"dist_ema200_atr\"] = (out[\"close\"] - out[\"ema200\"]) / (out[\"atr14\"].replace(0, np.nan))\n",
    "    out[\"range_atr\"] = (out[\"high\"] - out[\"low\"]) / (out[\"atr14\"].replace(0, np.nan))\n",
    "    out[\"body_atr\"] = (out[\"close\"] - out[\"open\"]).abs() / (out[\"atr14\"].replace(0, np.nan))\n",
    "    return out\n",
    "\n",
    "def build_entry_agg_1m(df1m: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Create 15m buckets based on timestamp; assume timestamp is candle open.\n",
    "    d = df1m.copy().set_index(\"timestamp\")\n",
    "    # Aggregate within each 15m interval\n",
    "    agg = pd.DataFrame({\n",
    "        \"m1_ret_mean\": np.log(d[\"close\"]).diff().resample(\"15min\").mean(),\n",
    "        \"m1_ret_std\":  np.log(d[\"close\"]).diff().resample(\"15min\").std(),\n",
    "        \"m1_range_mean\": (d[\"high\"]-d[\"low\"]).resample(\"15min\").mean(),\n",
    "        \"m1_range_max\":  (d[\"high\"]-d[\"low\"]).resample(\"15min\").max(),\n",
    "        \"m1_wick_down_mean\": ((d[[\"open\",\"close\"]].min(axis=1) - d[\"low\"]) / (d[\"high\"]-d[\"low\"]).replace(0,np.nan)).resample(\"15min\").mean(),\n",
    "        \"m1_wick_up_mean\":   ((d[\"high\"] - d[[\"open\",\"close\"]].max(axis=1)) / (d[\"high\"]-d[\"low\"]).replace(0,np.nan)).resample(\"15min\").mean(),\n",
    "    }).reset_index()\n",
    "    # IMPORTANT: shift by 1 bucket so 15m row at time t only uses completed previous 15m window\n",
    "    for col in agg.columns:\n",
    "        if col != \"timestamp\":\n",
    "            agg[col] = agg[col].shift(1)\n",
    "    return agg\n",
    "\n",
    "def asof_merge_left(base: pd.DataFrame, other: pd.DataFrame, suffix: str, on: str=\"timestamp\") -> pd.DataFrame:\n",
    "    # Merge last known macro candle that started <= base timestamp\n",
    "    left = base.sort_values(on).copy()\n",
    "    right = other.sort_values(on).copy()\n",
    "    merged = pd.merge_asof(left, right, on=on, direction=\"backward\", suffixes=(\"\", suffix))\n",
    "    return merged\n",
    "\n",
    "def build_dataset(raw: dict) -> pd.DataFrame:\n",
    "    df15 = raw[CFG.trade_tf].copy()\n",
    "    df15 = add_trade_features_15m(df15)\n",
    "\n",
    "    # Macro merges\n",
    "    for tf in CFG.macro_tfs:\n",
    "        m = add_macro_features(raw[tf], prefix=tf)\n",
    "        # Keep only macro feature columns + timestamp\n",
    "        keep = [\"timestamp\"] + [c for c in m.columns if c.startswith(tf+\"_\")]\n",
    "        df15 = asof_merge_left(df15, m[keep], suffix=f\"_{tf}\")\n",
    "\n",
    "    # Entry aggregation\n",
    "    entry_agg = build_entry_agg_1m(raw[CFG.entry_tf])\n",
    "    df15 = pd.merge(df15, entry_agg, on=\"timestamp\", how=\"left\")\n",
    "\n",
    "    # Basic cleanup\n",
    "    df15 = df15.replace([np.inf, -np.inf], np.nan)\n",
    "    return df15\n",
    "\n",
    "train_df = build_dataset(train_raw)\n",
    "test_df  = build_dataset(test_raw)\n",
    "\n",
    "print(\"train_df\", train_df.shape, \"test_df\", test_df.shape)\n",
    "train_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f37112a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>macro_ok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>224377</th>\n",
       "      <td>2023-12-29 20:45:00+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224378</th>\n",
       "      <td>2023-12-29 21:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224379</th>\n",
       "      <td>2023-12-29 21:15:00+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224380</th>\n",
       "      <td>2023-12-29 21:30:00+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224381</th>\n",
       "      <td>2023-12-29 21:45:00+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       timestamp  macro_ok\n",
       "224377 2023-12-29 20:45:00+00:00         1\n",
       "224378 2023-12-29 21:00:00+00:00         1\n",
       "224379 2023-12-29 21:15:00+00:00         1\n",
       "224380 2023-12-29 21:30:00+00:00         1\n",
       "224381 2023-12-29 21:45:00+00:00         1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== 4) Macro Gate (Rule-based, PF-first) =====\n",
    "def macro_gate(row) -> int:\n",
    "    # Use 4H and 1H context; conservative defaults.\n",
    "    ok = True\n",
    "\n",
    "    # 4h trend & strength\n",
    "    if not (row.get(\"4h_dist_ema200_atr\") is not None):\n",
    "        return 0\n",
    "\n",
    "    ok &= (row[\"4h_dist_ema200_atr\"] > 0)  # price above EMA200\n",
    "    ok &= (row[\"4h_ema200_slope\"] > 0)\n",
    "    ok &= (row[\"4h_adx14\"] > 18)\n",
    "\n",
    "    # 1h confirmation\n",
    "    ok &= (row[\"1h_dist_ema200_atr\"] > 0)\n",
    "    ok &= (row[\"1h_ema200_slope\"] > 0)\n",
    "\n",
    "    # Volatility sanity (avoid extreme squeeze/chaos)\n",
    "    # ATR percentile proxy: use BB width + range_atr (15m)\n",
    "    ok &= (row[\"bb_width\"] > 0)  # avoid NaN early rows\n",
    "    return int(bool(ok))\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    df[\"macro_ok\"] = df.apply(macro_gate, axis=1)\n",
    "\n",
    "train_df[[\"timestamp\",\"macro_ok\"]].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3daba2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>y_edge</th>\n",
       "      <th>macro_ok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 22:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 22:15:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 22:30:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 22:45:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 23:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp  y_edge  macro_ok\n",
       "0 2015-01-01 22:00:00+00:00     0.0         0\n",
       "1 2015-01-01 22:15:00+00:00     0.0         0\n",
       "2 2015-01-01 22:30:00+00:00     0.0         0\n",
       "3 2015-01-01 22:45:00+00:00     0.0         0\n",
       "4 2015-01-01 23:00:00+00:00     0.0         0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== 5) Triple-Barrier Label on 15m (entry at next open) =====\n",
    "def triple_barrier_labels(df: pd.DataFrame) -> pd.Series:\n",
    "    # Label computed on the SAME dataframe order (15m)\n",
    "    n = len(df)\n",
    "    label = np.full(n, np.nan)\n",
    "\n",
    "    atr_col = \"atr14\"\n",
    "    for i in range(n - (CFG.max_hold_bars + 2)):\n",
    "        if np.isnan(df.loc[i, atr_col]):\n",
    "            continue\n",
    "\n",
    "        entry_idx = i + 1\n",
    "        entry = df.loc[entry_idx, \"open\"]\n",
    "        atr_i = df.loc[i, atr_col]\n",
    "        tp = entry + CFG.tp_atr * atr_i\n",
    "        sl = entry - CFG.sl_atr * atr_i\n",
    "\n",
    "        end = min(n-1, entry_idx + CFG.max_hold_bars)\n",
    "        hit = 0  # default loss/time-out\n",
    "\n",
    "        for j in range(entry_idx, end+1):\n",
    "            hi = df.loc[j, \"high\"]\n",
    "            lo = df.loc[j, \"low\"]\n",
    "            # If both hit in same bar, assume worst-case for long (SL first)\n",
    "            if (lo <= sl) and (hi >= tp):\n",
    "                hit = 0\n",
    "                break\n",
    "            if lo <= sl:\n",
    "                hit = 0\n",
    "                break\n",
    "            if hi >= tp:\n",
    "                hit = 1\n",
    "                break\n",
    "        label[i] = hit\n",
    "\n",
    "    return pd.Series(label, index=df.index, name=\"y_edge\")\n",
    "\n",
    "# Compute labels on train only (no leaking test)\n",
    "train_df[\"y_edge\"] = triple_barrier_labels(train_df)\n",
    "train_df[[\"timestamp\",\"y_edge\",\"macro_ok\"]].dropna().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "959d0081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>y_entry_good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 22:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 22:15:00+00:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 22:30:00+00:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 22:45:00+00:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 23:00:00+00:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp  y_entry_good\n",
       "0 2015-01-01 22:00:00+00:00           0.0\n",
       "1 2015-01-01 22:15:00+00:00           1.0\n",
       "2 2015-01-01 22:30:00+00:00           1.0\n",
       "3 2015-01-01 22:45:00+00:00           1.0\n",
       "4 2015-01-01 23:00:00+00:00           1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== 6) Entry Quality Labels (1m based) =====\n",
    "# Label: after entry (next 15m open), within next 5 minutes,\n",
    "# does price move against us more than X pips? (bad entry = 1)\n",
    "# We'll model p_entry_good (good entry) => y_entry_good in {0,1}\n",
    "\n",
    "def build_entry_quality_labels(df15: pd.DataFrame, df1m: pd.DataFrame, adverse_pips: float = 3.0, window_min: int = 5) -> pd.Series:\n",
    "    d1 = df1m.copy().set_index(\"timestamp\").sort_index()\n",
    "    adverse = adverse_pips * CFG.pip_size\n",
    "\n",
    "    y = np.full(len(df15), np.nan)\n",
    "    for i in range(len(df15)-2):\n",
    "        entry_time = df15.loc[i+1, \"timestamp\"]\n",
    "        entry_price = df15.loc[i+1, \"open\"]\n",
    "        t0 = entry_time\n",
    "        t1 = entry_time + pd.Timedelta(minutes=window_min)\n",
    "        window = d1.loc[t0:t1]\n",
    "        if len(window) < 2:\n",
    "            continue\n",
    "        # worst adverse move for long: min low - entry\n",
    "        min_low = window[\"low\"].min()\n",
    "        bad = (entry_price - min_low) >= adverse\n",
    "        y[i] = 0 if bad else 1  # 1=good entry\n",
    "    return pd.Series(y, index=df15.index, name=\"y_entry_good\")\n",
    "\n",
    "train_df[\"y_entry_good\"] = build_entry_quality_labels(train_df, train_raw[CFG.entry_tf])\n",
    "train_df[[\"timestamp\",\"y_entry_good\"]].dropna().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baf55138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_core (174400, 46) val (49812, 46)\n",
      "2015-01-01 22:00:00+00:00 2021-12-31 00:00:00+00:00\n",
      "2022-01-02 22:00:00+00:00 2023-12-29 17:15:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# ===== 7) Train/Validation Split (time-based) =====\n",
    "def split_train_val(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    df = df.dropna(subset=[\"y_edge\",\"y_entry_good\"]).reset_index(drop=True)\n",
    "    df[\"date\"] = df[\"timestamp\"].dt.date\n",
    "\n",
    "    train_core = df[df[\"timestamp\"] <= pd.Timestamp(CFG.train_core_end, tz=\"UTC\")].copy()\n",
    "    val = df[(df[\"timestamp\"] >= pd.Timestamp(CFG.val_start, tz=\"UTC\")) & (df[\"timestamp\"] <= pd.Timestamp(CFG.val_end, tz=\"UTC\"))].copy()\n",
    "    return train_core, val\n",
    "\n",
    "train_core, val = split_train_val(train_df)\n",
    "print(\"train_core\", train_core.shape, \"val\", val.shape)\n",
    "print(train_core[\"timestamp\"].min(), train_core[\"timestamp\"].max())\n",
    "print(val[\"timestamp\"].min(), val[\"timestamp\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f2f1c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174400, 23) (174400, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_17588\\3047488170.py:27: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  X = X.fillna(method=\"ffill\").fillna(0)\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_17588\\3047488170.py:27: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  X = X.fillna(method=\"ffill\").fillna(0)\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_17588\\3047488170.py:27: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  X = X.fillna(method=\"ffill\").fillna(0)\n",
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_17588\\3047488170.py:27: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  X = X.fillna(method=\"ffill\").fillna(0)\n"
     ]
    }
   ],
   "source": [
    "# ===== 8) Feature Sets =====\n",
    "# We keep a compact but diverse set (PF-first, avoid redundancy).\n",
    "EDGE_FEATURES = [\n",
    "    # 15m core\n",
    "    \"ret1\",\"ret3\",\"ret6\",\"ret12\",\n",
    "    \"ema50_slope\",\"rsi14\",\"macd_hist\",\"bb_width\",\n",
    "    \"dist_ema50_atr\",\"dist_ema200_atr\",\"range_atr\",\"body_atr\",\n",
    "    # macro context (1h/4h)\n",
    "    \"1h_dist_ema200_atr\",\"1h_ema200_slope\",\"1h_adx14\",\"1h_bb_width\",\n",
    "    \"4h_dist_ema200_atr\",\"4h_ema200_slope\",\"4h_adx14\",\"4h_bb_width\",\n",
    "    # costs proxies / micro info\n",
    "    \"m1_ret_std\",\"m1_range_max\",\n",
    "    \"macro_ok\",  # gating info as feature too\n",
    "]\n",
    "\n",
    "ENTRY_FEATURES = [\n",
    "    \"m1_ret_mean\",\"m1_ret_std\",\"m1_range_mean\",\"m1_range_max\",\n",
    "    \"m1_wick_down_mean\",\"m1_wick_up_mean\",\n",
    "    \"range_atr\",\"bb_width\",\n",
    "]\n",
    "\n",
    "def make_xy(df: pd.DataFrame, features: list, ycol: str):\n",
    "    X = df[features].copy()\n",
    "    y = df[ycol].astype(int).copy()\n",
    "    # Replace inf/nan\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    X = X.fillna(method=\"ffill\").fillna(0)\n",
    "    return X, y\n",
    "\n",
    "Xtr_edge, ytr_edge = make_xy(train_core, EDGE_FEATURES, \"y_edge\")\n",
    "Xva_edge, yva_edge = make_xy(val, EDGE_FEATURES, \"y_edge\")\n",
    "\n",
    "Xtr_entry, ytr_entry = make_xy(train_core, ENTRY_FEATURES, \"y_entry_good\")\n",
    "Xva_entry, yva_entry = make_xy(val, ENTRY_FEATURES, \"y_entry_good\")\n",
    "\n",
    "print(Xtr_edge.shape, Xtr_entry.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "407965f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge model AUC (val): 0.6658\n",
      "Entry model AUC (val): 0.7488\n"
     ]
    }
   ],
   "source": [
    "# ===== 9) Train Models =====\n",
    "# 9.1 Edge model (XGBoost)\n",
    "edge_model = xgb.XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    min_child_weight=4,\n",
    "    gamma=0.0,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    eval_metric=\"logloss\",\n",
    ")\n",
    "edge_model.fit(Xtr_edge, ytr_edge)\n",
    "\n",
    "pva_edge = edge_model.predict_proba(Xva_edge)[:,1]\n",
    "auc_edge = roc_auc_score(yva_edge, pva_edge)\n",
    "print(\"Edge model AUC (val):\", round(auc_edge, 4))\n",
    "\n",
    "# 9.2 Entry model (Logistic Regression with scaling)\n",
    "entry_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"clf\", LogisticRegression(max_iter=500, random_state=SEED))\n",
    "])\n",
    "entry_model.fit(Xtr_entry, ytr_entry)\n",
    "pva_entry = entry_model.predict_proba(Xva_entry)[:,1]\n",
    "auc_entry = roc_auc_score(yva_entry, pva_entry)\n",
    "print(\"Entry model AUC (val):\", round(auc_entry, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3d7b446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best (proxy) thresholds on val: (np.float64(0.62), np.float64(0.65), np.float64(3.111111111111111))\n"
     ]
    }
   ],
   "source": [
    "# ===== 10) Threshold Tuning on Validation (PF-first) =====\n",
    "# We grid-search thresholds to maximize PF subject to a DD constraint (simple approximation on validation).\n",
    "# NOTE: Validation backtest is simplified; final backtest engine is used on test set.\n",
    "\n",
    "def compute_trade_signals(df: pd.DataFrame, edge_probs: np.ndarray, entry_probs: np.ndarray, edge_th: float, entry_th: float) -> pd.Series:\n",
    "    # signal at row i means we decide at close(i) and enter at open(i+1)\n",
    "    sig = (df[\"macro_ok\"].values == 1) & (edge_probs > edge_th) & (entry_probs > entry_th)\n",
    "    return pd.Series(sig.astype(int), index=df.index, name=\"signal\")\n",
    "\n",
    "def quick_pf_proxy(df: pd.DataFrame, signal: pd.Series) -> float:\n",
    "    # Proxy PF using 1R TP/SL outcomes from y_edge (already triple-barrier based)\n",
    "    # Only evaluates where signal==1 and y_edge is available.\n",
    "    mask = (signal == 1) & df[\"y_edge\"].notna()\n",
    "    if mask.sum() < 50:\n",
    "        return 0.0\n",
    "    y = df.loc[mask, \"y_edge\"].astype(int)\n",
    "    # wins=1, losses=0; with TP:SL ratio approx tp_atr/sl_atr\n",
    "    R_win = CFG.tp_atr / CFG.sl_atr\n",
    "    wins = (y==1).sum()\n",
    "    losses = (y==0).sum()\n",
    "    gross_profit = wins * R_win\n",
    "    gross_loss = losses * 1.0\n",
    "    if gross_loss == 0:\n",
    "        return float(\"inf\")\n",
    "    return gross_profit / gross_loss\n",
    "\n",
    "# Prepare probs for validation set\n",
    "pva_edge = edge_model.predict_proba(Xva_edge)[:,1]\n",
    "pva_entry = entry_model.predict_proba(Xva_entry)[:,1]\n",
    "\n",
    "edge_grid = np.round(np.linspace(0.55, 0.75, 9), 2)\n",
    "entry_grid = np.round(np.linspace(0.50, 0.70, 9), 2)\n",
    "\n",
    "best = (CFG.edge_th, CFG.entry_th, -1)\n",
    "for eth in edge_grid:\n",
    "    for ith in entry_grid:\n",
    "        sig = compute_trade_signals(val, pva_edge, pva_entry, eth, ith)\n",
    "        pf = quick_pf_proxy(val, sig)\n",
    "        if pf > best[2]:\n",
    "            best = (eth, ith, pf)\n",
    "print(\"Best (proxy) thresholds on val:\", best)\n",
    "\n",
    "CFG.edge_th, CFG.entry_th = best[0], best[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfdacd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 11) Full Backtest Engine (next-open execution, spread/slippage, 1% risk sizing) =====\n",
    "@dataclass\n",
    "class Trade:\n",
    "    entry_time: pd.Timestamp\n",
    "    exit_time: pd.Timestamp\n",
    "    entry_price: float\n",
    "    exit_price: float\n",
    "    sl: float\n",
    "    tp: float\n",
    "    lots: float\n",
    "    pnl_usd: float\n",
    "    pnl_pips: float\n",
    "    outcome: str\n",
    "\n",
    "def pip_to_price(pips: float) -> float:\n",
    "    return pips * CFG.pip_size\n",
    "\n",
    "def price_to_pips(delta_price: float) -> float:\n",
    "    return delta_price / CFG.pip_size\n",
    "\n",
    "def calc_lots_for_risk(equity: float, stop_pips: float) -> float:\n",
    "    if stop_pips <= 0:\n",
    "        return 0.0\n",
    "    risk_usd = equity * CFG.risk_per_trade\n",
    "    usd_per_pip = CFG.usd_per_pip_per_lot\n",
    "    lots = risk_usd / (stop_pips * usd_per_pip)\n",
    "    # avoid absurd leverage from tiny stops\n",
    "    return float(max(0.0, min(lots, 20.0)))\n",
    "\n",
    "def backtest(df: pd.DataFrame, edge_probs: np.ndarray, entry_probs: np.ndarray) -> tuple[list, pd.DataFrame]:\n",
    "    df = df.reset_index(drop=True).copy()\n",
    "    spread = pip_to_price(CFG.spread_pips)\n",
    "    slip = pip_to_price(CFG.slippage_pips)\n",
    "\n",
    "    in_pos = False\n",
    "    trades: list[Trade] = []\n",
    "    equity = CFG.starting_equity\n",
    "    equity_curve = []\n",
    "\n",
    "    entry_i = None\n",
    "    entry_price = sl = tp = lots = None\n",
    "\n",
    "    for i in range(len(df)-2):\n",
    "        t = df.loc[i, \"timestamp\"]\n",
    "        equity_curve.append((t, equity))\n",
    "\n",
    "        # manage open position\n",
    "        if in_pos:\n",
    "            hi = df.loc[i, \"high\"]\n",
    "            lo = df.loc[i, \"low\"]\n",
    "\n",
    "            hit_sl = lo <= sl\n",
    "            hit_tp = hi >= tp\n",
    "\n",
    "            if hit_sl or hit_tp:\n",
    "                # worst-case for long if both in same bar\n",
    "                if hit_sl and hit_tp:\n",
    "                    exit_price = sl\n",
    "                    outcome = \"SL(ambiguous)\"\n",
    "                elif hit_sl:\n",
    "                    exit_price = sl\n",
    "                    outcome = \"SL\"\n",
    "                else:\n",
    "                    exit_price = tp\n",
    "                    outcome = \"TP\"\n",
    "\n",
    "                # exit on bid (for long). Model bid = mid - spread/2, but we used prices as mid.\n",
    "                # Approx: subtract half spread from exit (worse)\n",
    "                exit_price = exit_price - spread/2 - slip\n",
    "\n",
    "                pnl_price = exit_price - entry_price\n",
    "                pnl_pips = price_to_pips(pnl_price)\n",
    "                pnl_usd = pnl_pips * CFG.usd_per_pip_per_lot * lots\n",
    "\n",
    "                # commission (round-trip)\n",
    "                pnl_usd -= CFG.commission_per_lot_usd * lots\n",
    "\n",
    "                equity += pnl_usd\n",
    "                trades.append(Trade(\n",
    "                    entry_time=df.loc[entry_i, \"timestamp\"],\n",
    "                    exit_time=t,\n",
    "                    entry_price=entry_price,\n",
    "                    exit_price=exit_price,\n",
    "                    sl=sl, tp=tp,\n",
    "                    lots=lots,\n",
    "                    pnl_usd=pnl_usd,\n",
    "                    pnl_pips=pnl_pips,\n",
    "                    outcome=outcome\n",
    "                ))\n",
    "                in_pos = False\n",
    "                entry_i = None\n",
    "            continue\n",
    "\n",
    "        # no position: decide at close(i), enter at open(i+1)\n",
    "        sig = (df.loc[i, \"macro_ok\"] == 1) and (edge_probs[i] > CFG.edge_th) and (entry_probs[i] > CFG.entry_th)\n",
    "        if not sig:\n",
    "            continue\n",
    "\n",
    "        # compute entry next bar open with ask+slippage\n",
    "        entry_i = i + 1\n",
    "        raw_entry = df.loc[entry_i, \"open\"]\n",
    "        entry_price = raw_entry + spread/2 + slip\n",
    "\n",
    "        atr_i = df.loc[i, \"atr14\"]\n",
    "        if not np.isfinite(atr_i) or atr_i <= 0:\n",
    "            continue\n",
    "\n",
    "        tp = entry_price + CFG.tp_atr * atr_i\n",
    "        sl = entry_price - CFG.sl_atr * atr_i\n",
    "\n",
    "        stop_pips = price_to_pips(entry_price - sl)\n",
    "        lots = calc_lots_for_risk(equity, stop_pips)\n",
    "        if lots <= 0:\n",
    "            continue\n",
    "\n",
    "        in_pos = True\n",
    "\n",
    "    # final equity point\n",
    "    if len(df) > 0:\n",
    "        equity_curve.append((df.loc[len(df)-1, \"timestamp\"], equity))\n",
    "\n",
    "    eq = pd.DataFrame(equity_curve, columns=[\"timestamp\",\"equity\"]).drop_duplicates(\"timestamp\")\n",
    "    eq[\"peak\"] = eq[\"equity\"].cummax()\n",
    "    eq[\"dd\"] = (eq[\"equity\"] - eq[\"peak\"]) / eq[\"peak\"]\n",
    "    return trades, eq\n",
    "\n",
    "def perf_stats(trades: list[Trade], eq: pd.DataFrame) -> dict:\n",
    "    if len(trades) == 0:\n",
    "        return {\"trades\":0}\n",
    "    pnl = np.array([t.pnl_usd for t in trades])\n",
    "    gross_profit = pnl[pnl>0].sum()\n",
    "    gross_loss = -pnl[pnl<0].sum()\n",
    "    pf = gross_profit / gross_loss if gross_loss > 0 else float(\"inf\")\n",
    "    winrate = (pnl>0).mean()\n",
    "    max_dd = float(eq[\"dd\"].min()) if len(eq) else 0.0\n",
    "    return {\n",
    "        \"trades\": len(trades),\n",
    "        \"profit_factor\": float(pf),\n",
    "        \"winrate\": float(winrate),\n",
    "        \"net_profit_usd\": float(pnl.sum()),\n",
    "        \"max_drawdown\": float(max_dd),\n",
    "        \"avg_trade_usd\": float(pnl.mean()),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04101e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_17588\\931550590.py:4: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_use = test_use.replace([np.inf, -np.inf], np.nan).fillna(method=\"ffill\").fillna(0).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'trades': 59,\n",
       " 'profit_factor': 1.4817615163267073,\n",
       " 'winrate': 0.5932203389830508,\n",
       " 'net_profit_usd': 1467.6881177890225,\n",
       " 'max_drawdown': -0.05799982894702798,\n",
       " 'avg_trade_usd': 24.87606979303428}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== 12) Run Final Backtest on TEST (2024–2025) =====\n",
    "# Prepare test features & probabilities\n",
    "test_use = test_df.copy()\n",
    "test_use = test_use.replace([np.inf, -np.inf], np.nan).fillna(method=\"ffill\").fillna(0).reset_index(drop=True)\n",
    "\n",
    "Xte_edge = test_use[EDGE_FEATURES].copy()\n",
    "Xte_entry = test_use[ENTRY_FEATURES].copy()\n",
    "\n",
    "pte_edge = edge_model.predict_proba(Xte_edge)[:,1]\n",
    "pte_entry = entry_model.predict_proba(Xte_entry)[:,1]\n",
    "\n",
    "trades, eq = backtest(test_use, pte_edge, pte_entry)\n",
    "stats = perf_stats(trades, eq)\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad17a9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 entry_time                 exit_time  entry_price  \\\n",
      "0 2024-02-26 08:15:00+00:00 2024-02-26 08:30:00+00:00      1.08286   \n",
      "1 2024-04-04 07:00:00+00:00 2024-04-04 07:15:00+00:00      1.08407   \n",
      "2 2024-04-09 19:15:00+00:00 2024-04-09 20:00:00+00:00      1.08500   \n",
      "3 2024-05-09 00:15:00+00:00 2024-05-09 01:30:00+00:00      1.07440   \n",
      "4 2024-05-13 08:30:00+00:00 2024-05-13 09:00:00+00:00      1.07777   \n",
      "\n",
      "   exit_price        sl        tp      lots     pnl_usd  pnl_pips outcome  \n",
      "0    1.083447  1.082360  1.083527  2.000381  117.330282  5.865396      TP  \n",
      "1    1.084608  1.083606  1.084688  2.181057  117.449283  5.384971      TP  \n",
      "2    1.085782  1.084353  1.085862  1.582403  123.804506  7.823831      TP  \n",
      "3    1.074766  1.074066  1.074846  3.097325  113.335851  3.659152      TP  \n",
      "4    1.078398  1.077239  1.078478  1.971838  123.850891  6.280986      TP  \n",
      "\n",
      "Last trades:\n",
      "                  entry_time                 exit_time  entry_price  \\\n",
      "54 2025-04-22 11:30:00+00:00 2025-04-22 12:15:00+00:00      1.14873   \n",
      "55 2025-11-17 06:15:00+00:00 2025-11-17 06:30:00+00:00      1.15977   \n",
      "56 2025-12-26 08:15:00+00:00 2025-12-26 10:15:00+00:00      1.17683   \n",
      "57 2025-12-29 06:15:00+00:00 2025-12-29 06:45:00+00:00      1.17564   \n",
      "58 2025-12-30 13:15:00+00:00 2025-12-30 15:15:00+00:00      1.17591   \n",
      "\n",
      "    exit_price        sl        tp      lots     pnl_usd   pnl_pips outcome  \n",
      "54    1.146790  1.146870  1.151210  0.607846 -117.911082 -19.398194      SL  \n",
      "55    1.160363  1.159265  1.160443  2.215202  131.437323   5.933423      TP  \n",
      "56    1.177762  1.176071  1.177842  1.491801  138.977028   9.316055      TP  \n",
      "57    1.176438  1.174982  1.176518  1.740386  138.841380   7.977621      TP  \n",
      "58    1.175089  1.175169  1.176897  1.565803 -128.488183  -8.205899      SL  \n",
      "\n",
      "Equity tail:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>equity</th>\n",
       "      <th>peak</th>\n",
       "      <th>dd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49796</th>\n",
       "      <td>2025-12-30 21:15:00+00:00</td>\n",
       "      <td>11467.688118</td>\n",
       "      <td>11596.176301</td>\n",
       "      <td>-0.01108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49797</th>\n",
       "      <td>2025-12-30 21:30:00+00:00</td>\n",
       "      <td>11467.688118</td>\n",
       "      <td>11596.176301</td>\n",
       "      <td>-0.01108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49798</th>\n",
       "      <td>2025-12-30 21:45:00+00:00</td>\n",
       "      <td>11467.688118</td>\n",
       "      <td>11596.176301</td>\n",
       "      <td>-0.01108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49799</th>\n",
       "      <td>2025-12-30 22:00:00+00:00</td>\n",
       "      <td>11467.688118</td>\n",
       "      <td>11596.176301</td>\n",
       "      <td>-0.01108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49800</th>\n",
       "      <td>2025-12-30 22:15:00+00:00</td>\n",
       "      <td>11467.688118</td>\n",
       "      <td>11596.176301</td>\n",
       "      <td>-0.01108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49801</th>\n",
       "      <td>2025-12-30 22:30:00+00:00</td>\n",
       "      <td>11467.688118</td>\n",
       "      <td>11596.176301</td>\n",
       "      <td>-0.01108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49802</th>\n",
       "      <td>2025-12-30 22:45:00+00:00</td>\n",
       "      <td>11467.688118</td>\n",
       "      <td>11596.176301</td>\n",
       "      <td>-0.01108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49803</th>\n",
       "      <td>2025-12-30 23:00:00+00:00</td>\n",
       "      <td>11467.688118</td>\n",
       "      <td>11596.176301</td>\n",
       "      <td>-0.01108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49804</th>\n",
       "      <td>2025-12-30 23:15:00+00:00</td>\n",
       "      <td>11467.688118</td>\n",
       "      <td>11596.176301</td>\n",
       "      <td>-0.01108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49805</th>\n",
       "      <td>2025-12-30 23:45:00+00:00</td>\n",
       "      <td>11467.688118</td>\n",
       "      <td>11596.176301</td>\n",
       "      <td>-0.01108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      timestamp        equity          peak       dd\n",
       "49796 2025-12-30 21:15:00+00:00  11467.688118  11596.176301 -0.01108\n",
       "49797 2025-12-30 21:30:00+00:00  11467.688118  11596.176301 -0.01108\n",
       "49798 2025-12-30 21:45:00+00:00  11467.688118  11596.176301 -0.01108\n",
       "49799 2025-12-30 22:00:00+00:00  11467.688118  11596.176301 -0.01108\n",
       "49800 2025-12-30 22:15:00+00:00  11467.688118  11596.176301 -0.01108\n",
       "49801 2025-12-30 22:30:00+00:00  11467.688118  11596.176301 -0.01108\n",
       "49802 2025-12-30 22:45:00+00:00  11467.688118  11596.176301 -0.01108\n",
       "49803 2025-12-30 23:00:00+00:00  11467.688118  11596.176301 -0.01108\n",
       "49804 2025-12-30 23:15:00+00:00  11467.688118  11596.176301 -0.01108\n",
       "49805 2025-12-30 23:45:00+00:00  11467.688118  11596.176301 -0.01108"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats:\n",
      "trades: 59\n",
      "profit_factor: 1.4817615163267073\n",
      "winrate: 0.5932203389830508\n",
      "net_profit_usd: 1467.6881177890225\n",
      "max_drawdown: -0.05799982894702798\n",
      "avg_trade_usd: 24.87606979303428\n"
     ]
    }
   ],
   "source": [
    "# ===== 13) Inspect Trades & Equity Curve (basic) =====\n",
    "trades_df = pd.DataFrame([t.__dict__ for t in trades]) if trades else pd.DataFrame()\n",
    "print(trades_df.head(5))\n",
    "print(\"\\nLast trades:\")\n",
    "print(trades_df.tail(5))\n",
    "\n",
    "print(\"\\nEquity tail:\")\n",
    "display(eq.tail(10))\n",
    "\n",
    "print(\"\\nStats:\")\n",
    "for k,v in stats.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd0b7261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry agg shift check (should have NaN early, shifted by 1):\n",
      "                  timestamp   m1_ret_mean  m1_ret_std\n",
      "0 2024-01-01 22:00:00+00:00  0.000000e+00    0.000000\n",
      "1 2024-01-01 22:15:00+00:00  1.293644e-06    0.000013\n",
      "2 2024-01-01 22:30:00+00:00 -6.468192e-07    0.000030\n",
      "3 2024-01-01 22:45:00+00:00  2.263847e-06    0.000008\n",
      "4 2024-01-01 23:00:00+00:00  3.018353e-06    0.000033\n",
      "5 2024-01-01 23:15:00+00:00 -3.320940e-05    0.000053\n",
      "6 2024-01-01 23:30:00+00:00 -4.227838e-06    0.000030\n",
      "7 2024-01-01 23:45:00+00:00 -5.436186e-06    0.000040\n",
      "8 2024-01-02 00:00:00+00:00 -1.294394e-06    0.000041\n",
      "9 2024-01-02 00:15:00+00:00  3.624239e-06    0.000036\n",
      "\n",
      "Macro merge check (1h/4h features should be forward-filled, not future):\n",
      "                  timestamp  1h_ema200_slope  4h_ema200_slope\n",
      "0 2024-01-01 22:00:00+00:00              0.0              0.0\n",
      "1 2024-01-01 22:15:00+00:00              0.0              0.0\n",
      "2 2024-01-01 22:30:00+00:00              0.0              0.0\n",
      "3 2024-01-01 22:45:00+00:00              0.0              0.0\n",
      "4 2024-01-01 23:00:00+00:00              0.0              0.0\n",
      "5 2024-01-01 23:15:00+00:00              0.0              0.0\n",
      "6 2024-01-01 23:30:00+00:00              0.0              0.0\n",
      "7 2024-01-01 23:45:00+00:00              0.0              0.0\n",
      "8 2024-01-02 00:00:00+00:00              0.0              0.0\n",
      "9 2024-01-02 00:15:00+00:00              0.0              0.0\n",
      "\n",
      "Trades per month (rough):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_17588\\1232720361.py:14: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  trades_df[\"month\"] = trades_df[\"entry_time\"].dt.to_period(\"M\").astype(str)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "month\n",
       "2024-02     1\n",
       "2024-04     2\n",
       "2024-05     7\n",
       "2024-06     4\n",
       "2024-07    10\n",
       "2024-08    19\n",
       "2025-03     8\n",
       "2025-04     4\n",
       "2025-11     1\n",
       "2025-12     3\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== 14) Sanity Checks (anti-leakage + realism) =====\n",
    "# 1) Ensure decision uses only past info: entry agg shifted, macro asof backward, entry at next open.\n",
    "# 2) Costs included via spread/slippage.\n",
    "# 3) Worst-case TP/SL ambiguity handling.\n",
    "\n",
    "print(\"Entry agg shift check (should have NaN early, shifted by 1):\")\n",
    "print(test_use[[\"timestamp\",\"m1_ret_mean\",\"m1_ret_std\"]].head(10))\n",
    "\n",
    "print(\"\\nMacro merge check (1h/4h features should be forward-filled, not future):\")\n",
    "print(test_use[[\"timestamp\",\"1h_ema200_slope\",\"4h_ema200_slope\"]].head(10))\n",
    "\n",
    "print(\"\\nTrades per month (rough):\")\n",
    "if len(trades_df):\n",
    "    trades_df[\"month\"] = trades_df[\"entry_time\"].dt.to_period(\"M\").astype(str)\n",
    "    display(trades_df.groupby(\"month\").size().tail(24))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daede90a",
   "metadata": {},
   "source": [
    "## Дараагийн алхамууд\n",
    "- `CFG.spread_pips`, `CFG.slippage_pips`, `CFG.commission_per_lot_usd`-ийг өөрийн брокерын бодит утгаар тохируул.\n",
    "- `tp_atr`, `sl_atr`, `max_hold_bars`, threshold-уудыг 2022–2023 validation дээр илүү нарийвчлан grid/random search хийж болно.\n",
    "- Хэрвээ trade хэт цөөн гарч байвал macro gate-г бага зэрэг сулруул (ж: ADX босго, EMA200 slope).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
