{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# forex_signal_v29\n",
        "\n",
        "PF-first Top-down Multi-Timeframe Ensemble (robust loader + resampling fallbacks)\n",
        "\n",
        "**Trade TF:** 15m (decision at close(t), entry at open(t+1))\n",
        "\n",
        "**Macro TF:** 1H & 4H (from files if present; otherwise resampled from 15m)\n",
        "\n",
        "**Entry TF:** 1m (from file if present; otherwise approximated from 5m/15m)\n",
        "\n",
        "This notebook is designed to **never silently produce 0 trades** without telling you why. It prints diagnostics at each stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HAS_XGB: True\n",
            "CFG: {'commission_per_lot_usd': 0.0, 'core_end': '2021-12-31', 'edge_th': 0.6, 'entry_th': 0.55, 'max_hold_bars': 16, 'pair_hint': 'EURUSD', 'risk_per_trade': 0.01, 'sl_atr': 1.5, 'slippage_pips': 0.2, 'spread_pips': 1.0, 'test_dir': '../data/test', 'tp_atr': 2.0, 'train_dir': '../data/train', 'val_end': '2023-12-31'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ===== 0) Imports & Config =====\n",
        "import os, glob, math, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Optional models\n",
        "HAS_XGB = False\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    HAS_XGB = True\n",
        "except Exception as e:\n",
        "    HAS_XGB = False\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "class CFG:\n",
        "    # Update these paths to match your machine\n",
        "    # Stepped up from ml_models_train/ to root\n",
        "    train_dir = \"../data/train\"\n",
        "    test_dir  = \"../data/test\"\n",
        "    pair_hint = \"EURUSD\"  # used to auto-pick files\n",
        "\n",
        "    # execution costs (set realistically!)\n",
        "    spread_pips: float = 1.0\n",
        "    slippage_pips: float = 0.2\n",
        "    commission_per_lot_usd: float = 0.0   # round-trip commission\n",
        "\n",
        "    # risk\n",
        "    risk_per_trade: float = 0.01\n",
        "\n",
        "    # triple-barrier defaults (tuned on validation later)\n",
        "    tp_atr: float = 2.0\n",
        "    sl_atr: float = 1.5\n",
        "    max_hold_bars: int = 16  # on 15m => 4h\n",
        "\n",
        "    # initial thresholds (will be tuned)\n",
        "    edge_th: float = 0.60\n",
        "    entry_th: float = 0.55\n",
        "\n",
        "    # validation split inside train\n",
        "    core_end = \"2021-12-31\"\n",
        "    val_end  = \"2023-12-31\"\n",
        "\n",
        "cfg = CFG()\n",
        "print(\"HAS_XGB:\", HAS_XGB)\n",
        "print(\"CFG:\", {k:getattr(cfg,k) for k in dir(cfg) if not k.startswith('_') and k.islower()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1m: EURUSD_m1.csv  rows=3354904\n",
            "Loaded 5m: EURUSD_m5.csv  rows=671581\n",
            "Loaded 15m: EURUSD_m15.csv  rows=224382\n",
            "Loaded 30m: EURUSD_m30.csv  rows=112194\n",
            "Loaded 1h: EURUSD_h1.csv  rows=56098\n",
            "Loaded 4h: EURUSD_h4.csv  rows=14498\n",
            "Loaded 1m: EURUSD_m1.csv  rows=743476\n",
            "Loaded 5m: EURUSD_m5.csv  rows=148502\n",
            "Loaded 15m: EURUSD_m15.csv  rows=49807\n",
            "Loaded 30m: EURUSD_m30.csv  rows=24907\n",
            "Loaded 1h: EURUSD_h1.csv  rows=12454\n",
            "Loaded 4h: EURUSD_h4.csv  rows=3220\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ===== 1) Robust CSV loader =====\n",
        "import re\n",
        "\n",
        "def _guess_tf_from_name(name: str) -> str:\n",
        "    n = name.lower()\n",
        "    \n",
        "    # Check for specific patterns with boundaries or clear delimiters\n",
        "    # e.g. _15m, m15.csv, -15m-\n",
        "    \n",
        "    # Strict regex checks first\n",
        "    # 15m, 1m etc.\n",
        "    if re.search(r\"(?:_|-|\\b)15m(?:_|-|\\b|\\.)\", n) or re.search(r\"(?:_|-|\\b)m15(?:_|-|\\b|\\.)\", n):\n",
        "        return \"15m\"\n",
        "    if re.search(r\"(?:_|-|\\b)1m(?:_|-|\\b|\\.)\", n) or re.search(r\"(?:_|-|\\b)m1(?:_|-|\\b|\\.)\", n):\n",
        "        return \"1m\"\n",
        "    if re.search(r\"(?:_|-|\\b)5m(?:_|-|\\b|\\.)\", n) or re.search(r\"(?:_|-|\\b)m5(?:_|-|\\b|\\.)\", n):\n",
        "        return \"5m\"\n",
        "    if re.search(r\"(?:_|-|\\b)30m(?:_|-|\\b|\\.)\", n) or re.search(r\"(?:_|-|\\b)m30(?:_|-|\\b|\\.)\", n):\n",
        "        return \"30m\"\n",
        "    if re.search(r\"(?:_|-|\\b)1h(?:_|-|\\b|\\.)\", n) or re.search(r\"(?:_|-|\\b)h1(?:_|-|\\b|\\.)\", n):\n",
        "        return \"1h\"\n",
        "    if re.search(r\"(?:_|-|\\b)4h(?:_|-|\\b|\\.)\", n) or re.search(r\"(?:_|-|\\b)h4(?:_|-|\\b|\\.)\", n):\n",
        "        return \"4h\"\n",
        "    if re.search(r\"(?:_|-|\\b)1d(?:_|-|\\b|\\.)\", n) or re.search(r\"(?:_|-|\\b)d1(?:_|-|\\b|\\.)\", n):\n",
        "        return \"1d\"\n",
        "        \n",
        "    # Fallback to simple containment but ordered by length\n",
        "    mapping = {\n",
        "        \"15m\": \"15m\", \"m15\": \"15m\",\n",
        "        \"30m\": \"30m\", \"m30\": \"30m\",\n",
        "        \"4h\": \"4h\", \"h4\": \"4h\",\n",
        "        \"1h\": \"1h\", \"h1\": \"1h\",\n",
        "        \"5m\": \"5m\", \"m5\": \"5m\",\n",
        "        \"1m\": \"1m\", \"m1\": \"1m\"\n",
        "    }\n",
        "    \n",
        "    # Sort keys by length descending to avoid partial matches (e.g. m1 matching m15)\n",
        "    sorted_keys = sorted(mapping.keys(), key=len, reverse=True)\n",
        "    \n",
        "    for k in sorted_keys:\n",
        "        if k in n:\n",
        "            return mapping[k]\n",
        "            \n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def _pick_files(data_dir: str, pair_hint: str):\n",
        "    # Also look for parent dir if data_dir is relative\n",
        "    if not os.path.exists(data_dir):\n",
        "        # try try stepping up\n",
        "        if os.path.exists(os.path.join(\"..\", data_dir)):\n",
        "             data_dir = os.path.join(\"..\", data_dir)\n",
        "             \n",
        "    files = glob.glob(os.path.join(data_dir, \"*.csv\")) + glob.glob(os.path.join(data_dir, \"*.parquet\"))\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No data files found in {data_dir}. Put your CSVs there.\")\n",
        "        \n",
        "    # keep only those containing pair hint if possible\n",
        "    hinted = [f for f in files if pair_hint.lower() in os.path.basename(f).lower()]\n",
        "    if hinted:\n",
        "        files = hinted\n",
        "        \n",
        "    by_tf = {}\n",
        "    for f in files:\n",
        "        tf = _guess_tf_from_name(os.path.basename(f))\n",
        "        if tf:\n",
        "            # If duplicates, prefer the one with clearer naming or just overwrite\n",
        "            if tf not in by_tf:\n",
        "                by_tf[tf] = f\n",
        "            else:\n",
        "                # heuristic: prefer shorter filename (often cleaner)\n",
        "                if len(os.path.basename(f)) < len(os.path.basename(by_tf[tf])):\n",
        "                    by_tf[tf] = f\n",
        "    return by_tf\n",
        "\n",
        "def _standardize_ohlc(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Accept common column variants\n",
        "    cols = {c.lower():c for c in df.columns}\n",
        "    def pick(*names):\n",
        "        for n in names:\n",
        "            if n in cols: return cols[n]\n",
        "        return None\n",
        "\n",
        "    tcol = pick(\"time\",\"timestamp\",\"date\",\"datetime\")\n",
        "    if tcol is None:\n",
        "        # Fallback: assume first column if it looks like time? \n",
        "        # Or if columns are ['<DATE>', '<TIME>', ...]\n",
        "        # Let's try to be smart.\n",
        "        if \"<DATE>\" in df.columns: # MT4 export\n",
        "             df[\"timestamp\"] = df[\"<DATE>\"] + \" \" + df[\"<TIME>\"]\n",
        "             tcol = \"timestamp\"\n",
        "        elif len(df.columns) >= 5: # Blind guess: col 0\n",
        "             tcol = df.columns[0]\n",
        "        else:\n",
        "            raise ValueError(f\"Couldn't find a timestamp column. Have: {list(df.columns)[:20]}\")\n",
        "            \n",
        "    df = df.rename(columns={tcol:\"time\"})\n",
        "    df[\"time\"] = pd.to_datetime(df[\"time\"], utc=True, errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"time\"]).sort_values(\"time\").drop_duplicates(\"time\")\n",
        "    \n",
        "    # OHLC\n",
        "    for need, opts in [(\"open\",(\"open\",\"o\",\"<open>\")),(\"high\",(\"high\",\"h\",\"<high>\")),(\"low\",(\"low\",\"l\",\"<low>\")),(\"close\",(\"close\",\"c\",\"<close>\"))]:\n",
        "        c = pick(*opts)\n",
        "        if c is None:\n",
        "             # Blind guess if 5 cols?\n",
        "             if len(df.columns) >= 5 and need in [\"open\",\"high\",\"low\",\"close\"]:\n",
        "                 # map indices 1,2,3,4 to OHLC\n",
        "                 idx_map = {\"open\":1, \"high\":2, \"low\":3, \"close\":4}\n",
        "                 if idx_map[need] < len(df.columns):\n",
        "                     c = df.columns[idx_map[need]]\n",
        "        \n",
        "        if c is None:\n",
        "            raise ValueError(f\"Missing {need} column. Have: {list(df.columns)[:20]}\")\n",
        "        df = df.rename(columns={c:need})\n",
        "        \n",
        "    # optional volume\n",
        "    v = pick(\"volume\",\"vol\",\"tick_volume\",\"<vol>\",\"<tickvol>\")\n",
        "    if v:\n",
        "        df = df.rename(columns={v:\"volume\"})\n",
        "    else:\n",
        "        df[\"volume\"] = np.nan\n",
        "        \n",
        "    df = df.set_index(\"time\")\n",
        "    df = df[[\"open\",\"high\",\"low\",\"close\",\"volume\"]].astype(float)\n",
        "    return df\n",
        "\n",
        "def load_tf(data_dir: str, pair_hint: str):\n",
        "    by_tf = _pick_files(data_dir, pair_hint)\n",
        "    out = {}\n",
        "    for tf in [\"1m\",\"5m\",\"15m\",\"30m\",\"1h\",\"4h\"]:\n",
        "        if tf in by_tf:\n",
        "            f = by_tf[tf]\n",
        "            try:\n",
        "                if f.endswith(\".parquet\"):\n",
        "                    df = pd.read_parquet(f)\n",
        "                else:\n",
        "                    df = pd.read_csv(f)\n",
        "                out[tf] = _standardize_ohlc(df)\n",
        "                print(f\"Loaded {tf}: {os.path.basename(f)}  rows={len(out[tf])}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load {f}: {e}\")\n",
        "                \n",
        "    return out\n",
        "\n",
        "train_tf = load_tf(cfg.train_dir, cfg.pair_hint)\n",
        "test_tf  = load_tf(cfg.test_dir, cfg.pair_hint)\n",
        "\n",
        "assert \"15m\" in train_tf and \"15m\" in test_tf, \"Need at least 15m data files.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ===== 2) Resampling fallbacks (so macro features never go NaN silently) =====\n",
        "def resample_ohlc(df: pd.DataFrame, rule: str) -> pd.DataFrame:\n",
        "    o = df[\"open\"].resample(rule).first()\n",
        "    h = df[\"high\"].resample(rule).max()\n",
        "    l = df[\"low\"].resample(rule).min()\n",
        "    c = df[\"close\"].resample(rule).last()\n",
        "    v = df[\"volume\"].resample(rule).sum() if \"volume\" in df else None\n",
        "    out = pd.DataFrame({\"open\":o,\"high\":h,\"low\":l,\"close\":c})\n",
        "    out[\"volume\"] = v if v is not None else np.nan\n",
        "    out = out.dropna(subset=[\"open\",\"high\",\"low\",\"close\"])\n",
        "    return out\n",
        "\n",
        "def ensure_tf(bundle: dict, tf: str, base_tf: str=\"15m\"):\n",
        "    if tf in bundle:\n",
        "        return bundle[tf]\n",
        "    base = bundle[base_tf]\n",
        "    rule = {\"1h\":\"1H\",\"4h\":\"4H\",\"30m\":\"30T\",\"5m\":\"5T\",\"1m\":\"1T\"}[tf]\n",
        "    # We only safely resample to *higher* tf from 15m here\n",
        "    if tf in (\"1h\",\"4h\",\"30m\"):\n",
        "        df = resample_ohlc(base, rule)\n",
        "        bundle[tf] = df\n",
        "        print(f\"[fallback] Built {tf} by resampling {base_tf} -> {rule}. rows={len(df)}\")\n",
        "        return df\n",
        "    return None\n",
        "\n",
        "_ = ensure_tf(train_tf,\"1h\")\n",
        "_ = ensure_tf(train_tf,\"4h\")\n",
        "_ = ensure_tf(test_tf,\"1h\")\n",
        "_ = ensure_tf(test_tf,\"4h\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ===== 3) Indicators (EMA, ATR, ADX, Bollinger width) =====\n",
        "def ema(s: pd.Series, span: int) -> pd.Series:\n",
        "    return s.ewm(span=span, adjust=False).mean()\n",
        "\n",
        "def atr(df: pd.DataFrame, n: int=14) -> pd.Series:\n",
        "    h,l,c = df[\"high\"], df[\"low\"], df[\"close\"]\n",
        "    prev_c = c.shift(1)\n",
        "    tr = pd.concat([(h-l).abs(), (h-prev_c).abs(), (l-prev_c).abs()], axis=1).max(axis=1)\n",
        "    return tr.rolling(n).mean()\n",
        "\n",
        "def adx(df: pd.DataFrame, n: int=14) -> pd.Series:\n",
        "    h,l,c = df[\"high\"], df[\"low\"], df[\"close\"]\n",
        "    up = h.diff()\n",
        "    dn = -l.diff()\n",
        "    plus_dm = np.where((up > dn) & (up > 0), up, 0.0)\n",
        "    minus_dm = np.where((dn > up) & (dn > 0), dn, 0.0)\n",
        "    tr = pd.concat([(h-l).abs(), (h-c.shift(1)).abs(), (l-c.shift(1)).abs()], axis=1).max(axis=1)\n",
        "    atr_n = tr.rolling(n).sum()\n",
        "    plus_di = 100 * pd.Series(plus_dm, index=df.index).rolling(n).sum() / atr_n\n",
        "    minus_di= 100 * pd.Series(minus_dm, index=df.index).rolling(n).sum() / atr_n\n",
        "    dx = (100 * (plus_di - minus_di).abs() / (plus_di + minus_di)).replace([np.inf,-np.inf], np.nan)\n",
        "    return dx.rolling(n).mean()\n",
        "\n",
        "def bb_width(df: pd.DataFrame, n: int=20, k: float=2.0) -> pd.Series:\n",
        "    m = df[\"close\"].rolling(n).mean()\n",
        "    s = df[\"close\"].rolling(n).std()\n",
        "    upper = m + k*s\n",
        "    lower = m - k*s\n",
        "    return (upper-lower) / m\n",
        "\n",
        "def add_tf_features(tf_df: pd.DataFrame, prefix: str) -> pd.DataFrame:\n",
        "    out = pd.DataFrame(index=tf_df.index)\n",
        "    out[f\"{prefix}ema50\"] = ema(tf_df[\"close\"], 50)\n",
        "    out[f\"{prefix}ema200\"] = ema(tf_df[\"close\"], 200)\n",
        "    out[f\"{prefix}ema200_slope\"] = out[f\"{prefix}ema200\"].diff(5)\n",
        "    out[f\"{prefix}atr14\"] = atr(tf_df, 14)\n",
        "    out[f\"{prefix}adx14\"] = adx(tf_df, 14)\n",
        "    out[f\"{prefix}bb_width\"] = bb_width(tf_df, 20, 2.0)\n",
        "    # distance to ema200 in ATR units\n",
        "    out[f\"{prefix}dist_ema200_atr\"] = (tf_df[\"close\"] - out[f\"{prefix}ema200\"]) / (out[f\"{prefix}atr14\"] + 1e-12)\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Master built. Columns: 36\n",
            "Train date span: 2015-01-01 22:00:00+00:00 -> 2023-12-29 21:45:00+00:00\n",
            "Test  date span: 2024-01-01 22:00:00+00:00 -> 2025-12-30 23:45:00+00:00\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ===== 4) Build 15m master dataset with multi-TF context =====\n",
        "def attach_higher_tf(master_15m: pd.DataFrame, higher_df: pd.DataFrame, higher_feat: pd.DataFrame, tag: str) -> pd.DataFrame:\n",
        "    # forward-fill last completed higher candle features onto 15m index (no lookahead)\n",
        "    aligned = higher_feat.reindex(master_15m.index, method=\"ffill\")\n",
        "    aligned.columns = [f\"{tag}_{c}\" for c in aligned.columns]\n",
        "    return master_15m.join(aligned, how=\"left\")\n",
        "\n",
        "def build_master(bundle: dict) -> pd.DataFrame:\n",
        "    m15 = bundle[\"15m\"].copy()\n",
        "    # base features on 15m\n",
        "    f15 = add_tf_features(m15, prefix=\"15m_\")\n",
        "    df = m15.join(f15)\n",
        "\n",
        "    # attach 1h, 4h\n",
        "    h1 = ensure_tf(bundle,\"1h\"); f1 = add_tf_features(h1, prefix=\"1h_\")\n",
        "    h4 = ensure_tf(bundle,\"4h\"); f4 = add_tf_features(h4, prefix=\"4h_\")\n",
        "    df = attach_higher_tf(df, h1, f1, \"1h\")\n",
        "    df = attach_higher_tf(df, h4, f4, \"4h\")\n",
        "\n",
        "    # micro features from 1m if available\n",
        "    if \"1m\" in bundle:\n",
        "        m1 = bundle[\"1m\"]\n",
        "        # aggregate last 15m window ending at time t (decision time). Use shift(1) to avoid peeking into current forming minute.\n",
        "        m1r = m1.copy()\n",
        "        m1r[\"ret1\"] = m1r[\"close\"].pct_change()\n",
        "        agg = pd.DataFrame(index=df.index)\n",
        "        # resample to 15m bins aligned to master index\n",
        "        r = m1r[\"ret1\"].shift(1).resample(\"15T\")\n",
        "        agg[\"m1_ret_std\"] = r.std()\n",
        "        agg[\"m1_ret_mean\"]= r.mean()\n",
        "        # candle anatomy\n",
        "        m1r[\"range\"] = (m1r[\"high\"]-m1r[\"low\"])\n",
        "        m1r[\"lower_wick\"] = (np.minimum(m1r[\"open\"], m1r[\"close\"]) - m1r[\"low\"]).clip(lower=0)\n",
        "        rr = m1r[\"range\"].shift(1).resample(\"15T\")\n",
        "        lw = m1r[\"lower_wick\"].shift(1).resample(\"15T\")\n",
        "        agg[\"m1_range_mean\"] = rr.mean()\n",
        "        agg[\"m1_lowerwick_ratio\"] = (lw.mean() / (rr.mean()+1e-12))\n",
        "        agg = agg.reindex(df.index)\n",
        "        df = df.join(agg)\n",
        "    else:\n",
        "        df[\"m1_ret_std\"] = np.nan\n",
        "        df[\"m1_ret_mean\"] = np.nan\n",
        "        df[\"m1_range_mean\"] = np.nan\n",
        "        df[\"m1_lowerwick_ratio\"] = np.nan\n",
        "\n",
        "    # simple 15m returns\n",
        "    df[\"r1\"]  = df[\"close\"].pct_change(1)\n",
        "    df[\"r3\"]  = df[\"close\"].pct_change(3)\n",
        "    df[\"r6\"]  = df[\"close\"].pct_change(6)\n",
        "    df[\"r12\"] = df[\"close\"].pct_change(12)\n",
        "    # normalize some distances by 15m ATR\n",
        "    df[\"dist_ema50_atr\"]  = (df[\"close\"] - df[\"15m_ema50\"]) / (df[\"15m_atr14\"] + 1e-12)\n",
        "    df[\"dist_ema200_atr\"] = (df[\"close\"] - df[\"15m_ema200\"]) / (df[\"15m_atr14\"] + 1e-12)\n",
        "    # clean\n",
        "    return df\n",
        "\n",
        "train_df = build_master(train_tf)\n",
        "test_df  = build_master(test_tf)\n",
        "\n",
        "print(\"Master built. Columns:\", len(train_df.columns))\n",
        "print(\"Train date span:\", train_df.index.min(), \"->\", train_df.index.max())\n",
        "print(\"Test  date span:\", test_df.index.min(), \"->\", test_df.index.max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "macro_ok (train): {1: 224382}\n",
            "macro_ok (test) : {1: 49807}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ===== 5) Macro gate + diagnostics (NO silent 0-trade) =====\n",
        "def macro_gate(df: pd.DataFrame) -> pd.Series:\n",
        "    # Fill NaNs conservatively: if macro features missing, DO NOT block everything.\n",
        "    d = df.copy()\n",
        "    for c in [\"4h_dist_ema200_atr\",\"4h_ema200_slope\",\"4h_adx14\",\"4h_bb_width\",\n",
        "              \"1h_dist_ema200_atr\",\"1h_adx14\"]:\n",
        "        if c not in d:\n",
        "            d[c] = np.nan\n",
        "    # permissive fill: missing -> pass (so we can at least backtest)\n",
        "    # then you can tighten once you confirm features exist\n",
        "    dist4 = d[\"4h_dist_ema200_atr\"].fillna(0.1)\n",
        "    slope4= d[\"4h_ema200_slope\"].fillna(0.0)\n",
        "    adx4  = d[\"4h_adx14\"].fillna(30)\n",
        "    bbw4  = d[\"4h_bb_width\"].fillna(0.01)\n",
        "    dist1 = d[\"1h_dist_ema200_atr\"].fillna(0.0)\n",
        "    adx1  = d[\"1h_adx14\"].fillna(25)\n",
        "\n",
        "    ok = (\n",
        "        (dist4 > 0.0) &\n",
        "        (slope4 >= 0.0) &\n",
        "        (adx4 > 16) &\n",
        "        (dist1 > -0.35) &\n",
        "        (adx1 > 14) &\n",
        "        (bbw4 > 0.0005)\n",
        "    )\n",
        "    return ok.astype(int)\n",
        "\n",
        "train_df[\"macro_ok\"] = macro_gate(train_df)\n",
        "test_df[\"macro_ok\"]  = macro_gate(test_df)\n",
        "\n",
        "print(\"macro_ok (train):\", train_df[\"macro_ok\"].value_counts().to_dict())\n",
        "print(\"macro_ok (test) :\", test_df[\"macro_ok\"].value_counts().to_dict())\n",
        "\n",
        "# Hard guard: if macro_ok is all zeros, automatically relax and warn.\n",
        "if train_df[\"macro_ok\"].sum() < 100:\n",
        "    print(\"\\n[WARN] macro_ok too strict or macro features missing. Auto-relaxing to keep pipeline alive.\")\n",
        "    train_df[\"macro_ok\"] = 1\n",
        "    test_df[\"macro_ok\"]  = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label balance (train macro_ok==1): {0: 0.6714888003494042, 1: 0.3285111996505959}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ===== 6) Labels (15m triple-barrier) =====\n",
        "def triple_barrier_label(df: pd.DataFrame, tp_atr: float, sl_atr: float, max_hold: int) -> pd.Series:\n",
        "    # decision at t close; entry at t+1 open\n",
        "    tp = df[\"open\"].shift(-1) + tp_atr * df[\"15m_atr14\"]\n",
        "    sl = df[\"open\"].shift(-1) - sl_atr * df[\"15m_atr14\"]\n",
        "\n",
        "    y = np.zeros(len(df), dtype=int)\n",
        "    for i in range(len(df) - max_hold - 2):\n",
        "        entry_i = i + 1\n",
        "        if not np.isfinite(tp.iloc[i]) or not np.isfinite(sl.iloc[i]):\n",
        "            continue\n",
        "        hi = df[\"high\"].iloc[entry_i: entry_i + max_hold]\n",
        "        lo = df[\"low\"].iloc[entry_i: entry_i + max_hold]\n",
        "        hit_tp = np.where(hi.values >= tp.iloc[i])[0]\n",
        "        hit_sl = np.where(lo.values <= sl.iloc[i])[0]\n",
        "        if len(hit_tp)==0 and len(hit_sl)==0:\n",
        "            y[i]=0\n",
        "            continue\n",
        "        # worst-case ambiguity on same bar: assume SL first (conservative PF)\n",
        "        tp_first = hit_tp[0] if len(hit_tp)>0 else 10**9\n",
        "        sl_first = hit_sl[0] if len(hit_sl)>0 else 10**9\n",
        "        y[i] = 1 if tp_first < sl_first else 0\n",
        "    return pd.Series(y, index=df.index)\n",
        "\n",
        "train_df[\"y\"] = triple_barrier_label(train_df, cfg.tp_atr, cfg.sl_atr, cfg.max_hold_bars)\n",
        "test_df[\"y\"]  = triple_barrier_label(test_df, cfg.tp_atr, cfg.sl_atr, cfg.max_hold_bars)\n",
        "\n",
        "print(\"Label balance (train macro_ok==1):\", train_df.loc[train_df[\"macro_ok\"]==1,\"y\"].value_counts(normalize=True).to_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Edge AUC raw: 0.580923115569892 cal: 0.5821072031011185\n",
            "Entry AUC: 0.5151619389316484\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ===== 7) Train models (RF macro optional, XGB edge, LR entry) + Calibrate =====\n",
        "def split_core_val(df: pd.DataFrame):\n",
        "    core = df.loc[:cfg.core_end].copy()\n",
        "    val  = df.loc[cfg.core_end:cfg.val_end].copy()\n",
        "    # avoid overlap at boundary\n",
        "    val = val.iloc[1:].copy()\n",
        "    return core, val\n",
        "\n",
        "FEATURES_EDGE = [\n",
        "    \"r1\",\"r3\",\"r6\",\"r12\",\n",
        "    \"15m_adx14\",\"15m_bb_width\",\"15m_dist_ema200_atr\",\"dist_ema50_atr\",\n",
        "    \"1h_1h_dist_ema200_atr\",\"1h_1h_adx14\",\"4h_4h_dist_ema200_atr\",\"4h_4h_adx14\",\"4h_4h_bb_width\",\n",
        "]\n",
        "FEATURES_ENTRY = [\n",
        "    \"m1_ret_std\",\"m1_ret_mean\",\"m1_range_mean\",\"m1_lowerwick_ratio\",\n",
        "    \"15m_bb_width\",\"15m_adx14\"\n",
        "]\n",
        "\n",
        "def safe_matrix(df: pd.DataFrame, cols):\n",
        "    X = df[cols].copy()\n",
        "    # fill NaNs robustly\n",
        "    X = X.replace([np.inf,-np.inf], np.nan)\n",
        "    X = X.fillna(method=\"ffill\").fillna(method=\"bfill\").fillna(0.0)\n",
        "    return X\n",
        "\n",
        "core, val = split_core_val(train_df)\n",
        "core = core[core[\"macro_ok\"]==1].copy()\n",
        "val  = val[val[\"macro_ok\"]==1].copy()\n",
        "\n",
        "# if too few samples, relax (avoid \"no result\")\n",
        "if len(core) < 2000:\n",
        "    print(\"[WARN] Too few macro_ok samples in core. Forcing macro_ok=1 for training.\")\n",
        "    core, val = split_core_val(train_df)\n",
        "\n",
        "Xc = safe_matrix(core, FEATURES_EDGE)\n",
        "yc = core[\"y\"].astype(int)\n",
        "Xv = safe_matrix(val, FEATURES_EDGE)\n",
        "yv = val[\"y\"].astype(int)\n",
        "\n",
        "# Edge model\n",
        "if HAS_XGB:\n",
        "    edge = XGBClassifier(\n",
        "        n_estimators=600,\n",
        "        max_depth=4,\n",
        "        learning_rate=0.04,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_lambda=1.0,\n",
        "        min_child_weight=5,\n",
        "        objective=\"binary:logistic\",\n",
        "        eval_metric=\"logloss\",\n",
        "        tree_method=\"hist\",\n",
        "        random_state=42,\n",
        "    )\n",
        "else:\n",
        "    edge = RandomForestClassifier(\n",
        "        n_estimators=500,\n",
        "        max_depth=10,\n",
        "        min_samples_leaf=50,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "edge.fit(Xc, yc)\n",
        "p_val_raw = edge.predict_proba(Xv)[:,1]\n",
        "\n",
        "# calibration helps threshold stability\n",
        "cal = CalibratedClassifierCV(edge, method=\"sigmoid\", cv=3)\n",
        "cal.fit(Xc, yc)\n",
        "p_val = cal.predict_proba(Xv)[:,1]\n",
        "print(\"Edge AUC raw:\", roc_auc_score(yv, p_val_raw), \"cal:\", roc_auc_score(yv, p_val))\n",
        "\n",
        "# Entry model (use label = y as proxy; you can replace with adverse-move label later)\n",
        "Xe_c = safe_matrix(core, FEATURES_ENTRY)\n",
        "Xe_v = safe_matrix(val, FEATURES_ENTRY)\n",
        "entry = LogisticRegression(max_iter=2000, C=0.5)\n",
        "entry.fit(Xe_c, yc)\n",
        "p_entry_val = entry.predict_proba(Xe_v)[:,1]\n",
        "print(\"Entry AUC:\", roc_auc_score(yv, p_entry_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] No parameter set met constraints. We'll use defaults (may trade less).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ===== 8) Backtest engine (truthful) + PF-first threshold tuning =====\n",
        "def pip_to_price(pips: float) -> float:\n",
        "    return pips * 0.0001  # EURUSD-style. If you trade JPY pairs, adjust.\n",
        "\n",
        "def compute_lot_size(equity: float, entry: float, sl: float, risk: float) -> float:\n",
        "    # 1 lot = 100,000 base; pip value approx $10 per pip for EURUSD per lot\n",
        "    # risk_amount = equity*risk ; stop_pips = |entry-sl|/0.0001 ; lot = risk_amount / (stop_pips*10)\n",
        "    risk_amount = equity * risk\n",
        "    stop_pips = abs(entry - sl) / 0.0001\n",
        "    if stop_pips <= 0:\n",
        "        return 0.0\n",
        "    lots = risk_amount / (stop_pips * 10.0)\n",
        "    return max(0.0, lots)\n",
        "\n",
        "def run_backtest(df: pd.DataFrame, p_edge: np.ndarray, p_entry: np.ndarray,\n",
        "                 EDGE_TH: float, ENTRY_TH: float, tp_atr: float, sl_atr: float,\n",
        "                 max_hold: int, equity0: float=10_000.0):\n",
        "    spread = pip_to_price(cfg.spread_pips)\n",
        "    slip   = pip_to_price(cfg.slippage_pips)\n",
        "\n",
        "    eq = equity0\n",
        "    equity_curve = []\n",
        "    trades = []\n",
        "    in_pos = False\n",
        "    exit_i = -1\n",
        "\n",
        "    for i in range(len(df)-max_hold-2):\n",
        "        equity_curve.append((df.index[i], eq))\n",
        "        if in_pos and i < exit_i:\n",
        "            continue\n",
        "        in_pos = False\n",
        "\n",
        "        if df.iloc[i][\"macro_ok\"] != 1:\n",
        "            continue\n",
        "        if not (p_edge[i] > EDGE_TH and p_entry[i] > ENTRY_TH):\n",
        "            continue\n",
        "\n",
        "        # decision at t close, entry at t+1 open with costs\n",
        "        entry_i = i + 1\n",
        "        entry_mid = df[\"open\"].iloc[entry_i]\n",
        "        entry = entry_mid + spread/2 + slip\n",
        "\n",
        "        atrv = df[\"15m_atr14\"].iloc[i]\n",
        "        if not np.isfinite(atrv) or atrv <= 0:\n",
        "            continue\n",
        "        tp = entry_mid + tp_atr * atrv\n",
        "        sl = entry_mid - sl_atr * atrv\n",
        "\n",
        "        lots = compute_lot_size(eq, entry, sl, cfg.risk_per_trade)\n",
        "        if lots <= 0:\n",
        "            continue\n",
        "\n",
        "        # walk forward up to max_hold bars\n",
        "        hi = df[\"high\"].iloc[entry_i: entry_i+max_hold]\n",
        "        lo = df[\"low\"].iloc[entry_i: entry_i+max_hold]\n",
        "        # conservative: if both hit same bar -> SL first\n",
        "        hit_tp = np.where(hi.values >= tp)[0]\n",
        "        hit_sl = np.where(lo.values <= sl)[0]\n",
        "        if len(hit_tp)==0 and len(hit_sl)==0:\n",
        "            # time exit at last bar close\n",
        "            ex_i = entry_i + max_hold - 1\n",
        "            exit_mid = df[\"close\"].iloc[ex_i]\n",
        "            exit_px = exit_mid - spread/2 - slip\n",
        "            pnl_pips = (exit_px - entry) / 0.0001\n",
        "            reason = \"time\"\n",
        "        else:\n",
        "            tp_first = hit_tp[0] if len(hit_tp)>0 else 10**9\n",
        "            sl_first = hit_sl[0] if len(hit_sl)>0 else 10**9\n",
        "            if tp_first < sl_first:\n",
        "                ex_i = entry_i + tp_first\n",
        "                exit_mid = tp\n",
        "                exit_px = exit_mid - spread/2 - slip\n",
        "                pnl_pips = (exit_px - entry) / 0.0001\n",
        "                reason=\"tp\"\n",
        "            else:\n",
        "                ex_i = entry_i + sl_first\n",
        "                exit_mid = sl\n",
        "                exit_px = exit_mid - spread/2 - slip\n",
        "                pnl_pips = (exit_px - entry) / 0.0001\n",
        "                reason=\"sl\"\n",
        "\n",
        "        pnl_usd = pnl_pips * 10.0 * lots\n",
        "        pnl_usd -= cfg.commission_per_lot_usd * lots\n",
        "        eq += pnl_usd\n",
        "\n",
        "        trades.append({\n",
        "            \"entry_time\": df.index[entry_i],\n",
        "            \"exit_time\": df.index[ex_i],\n",
        "            \"entry\": float(entry),\n",
        "            \"exit\": float(exit_px),\n",
        "            \"lots\": float(lots),\n",
        "            \"pnl_usd\": float(pnl_usd),\n",
        "            \"reason\": reason\n",
        "        })\n",
        "        in_pos = True\n",
        "        exit_i = ex_i\n",
        "\n",
        "    equity_curve.append((df.index[min(len(df)-1, len(df)-1)], eq))\n",
        "    eq_df = pd.DataFrame(equity_curve, columns=[\"time\",\"equity\"]).set_index(\"time\")\n",
        "    tr = pd.DataFrame(trades)\n",
        "    return tr, eq_df\n",
        "\n",
        "def profit_factor(trades: pd.DataFrame) -> float:\n",
        "    if trades is None or len(trades)==0:\n",
        "        return 0.0\n",
        "    g = trades.loc[trades[\"pnl_usd\"]>0, \"pnl_usd\"].sum()\n",
        "    l = -trades.loc[trades[\"pnl_usd\"]<0, \"pnl_usd\"].sum()\n",
        "    return float(g/l) if l>0 else float(\"inf\")\n",
        "\n",
        "def max_drawdown(eq: pd.Series) -> float:\n",
        "    peak = eq.cummax()\n",
        "    dd = (eq - peak) / peak\n",
        "    return float(dd.min())\n",
        "\n",
        "# Build validation predictions aligned to val index\n",
        "p_edge_val = cal.predict_proba(safe_matrix(val, FEATURES_EDGE))[:,1]\n",
        "p_ent_val  = entry.predict_proba(safe_matrix(val, FEATURES_ENTRY))[:,1]\n",
        "\n",
        "# PF-first tuning: search thresholds + TP/SL around defaults\n",
        "grid_edge = np.linspace(0.55, 0.75, 9)\n",
        "grid_ent  = np.linspace(0.50, 0.70, 9)\n",
        "grid_tp   = [1.8, 2.0, 2.2, 2.4]\n",
        "grid_sl   = [1.2, 1.4, 1.5, 1.6]\n",
        "\n",
        "best = None\n",
        "for et in grid_edge:\n",
        "    for it in grid_ent:\n",
        "        for tp in grid_tp:\n",
        "            for sl in grid_sl:\n",
        "                tr, eq = run_backtest(val, p_edge_val, p_ent_val, et, it, tp, sl, cfg.max_hold_bars)\n",
        "                if len(tr) < 80:  # avoid ultra few trades\n",
        "                    continue\n",
        "                pf = profit_factor(tr)\n",
        "                dd = max_drawdown(eq[\"equity\"])\n",
        "                if dd < -0.20:  # dd constraint\n",
        "                    continue\n",
        "                score = pf\n",
        "                cand = (score, pf, dd, len(tr), et, it, tp, sl)\n",
        "                if best is None or cand[0] > best[0]:\n",
        "                    best = cand\n",
        "\n",
        "if best is None:\n",
        "    print(\"[WARN] No parameter set met constraints. We'll use defaults (may trade less).\")\n",
        "    EDGE_TH, ENTRY_TH, TP_ATR, SL_ATR = cfg.edge_th, cfg.entry_th, cfg.tp_atr, cfg.sl_atr\n",
        "else:\n",
        "    _, pf, dd, ntr, EDGE_TH, ENTRY_TH, TP_ATR, SL_ATR = best\n",
        "    print(\"BEST on VAL:\", {\"PF\":pf, \"MaxDD\":dd, \"Trades\":ntr, \"EDGE_TH\":EDGE_TH, \"ENTRY_TH\":ENTRY_TH, \"TP_ATR\":TP_ATR, \"SL_ATR\":SL_ATR})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST RESULTS: {'ProfitFactor': 0.0, 'Trades': 0, 'MaxDD': 0.0, 'FinalEquity': 10000.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(Empty DataFrame\n",
              " Columns: []\n",
              " Index: [],\n",
              " Empty DataFrame\n",
              " Columns: []\n",
              " Index: [])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# ===== 9) Final train on full train (macro_ok==1), then TEST backtest =====\n",
        "train_ok = train_df[train_df[\"macro_ok\"]==1].copy()\n",
        "if len(train_ok) < 2000:\n",
        "    train_ok = train_df.copy()\n",
        "\n",
        "X_train = safe_matrix(train_ok, FEATURES_EDGE)\n",
        "y_train = train_ok[\"y\"].astype(int)\n",
        "\n",
        "# re-fit base edge then calibrate\n",
        "if HAS_XGB:\n",
        "    edge2 = XGBClassifier(\n",
        "        n_estimators=900,\n",
        "        max_depth=4,\n",
        "        learning_rate=0.03,\n",
        "        subsample=0.85,\n",
        "        colsample_bytree=0.85,\n",
        "        reg_lambda=1.0,\n",
        "        min_child_weight=5,\n",
        "        objective=\"binary:logistic\",\n",
        "        eval_metric=\"logloss\",\n",
        "        tree_method=\"hist\",\n",
        "        random_state=42,\n",
        "    )\n",
        "else:\n",
        "    edge2 = RandomForestClassifier(n_estimators=800, max_depth=10, min_samples_leaf=30, random_state=42, n_jobs=-1)\n",
        "\n",
        "edge2.fit(X_train, y_train)\n",
        "cal2 = CalibratedClassifierCV(edge2, method=\"sigmoid\", cv=3)\n",
        "cal2.fit(X_train, y_train)\n",
        "\n",
        "entry2 = LogisticRegression(max_iter=2000, C=0.5)\n",
        "entry2.fit(safe_matrix(train_ok, FEATURES_ENTRY), y_train)\n",
        "\n",
        "p_edge_test = cal2.predict_proba(safe_matrix(test_df, FEATURES_EDGE))[:,1]\n",
        "p_ent_test  = entry2.predict_proba(safe_matrix(test_df, FEATURES_ENTRY))[:,1]\n",
        "\n",
        "tr_test, eq_test = run_backtest(test_df, p_edge_test, p_ent_test, EDGE_TH, ENTRY_TH, TP_ATR, SL_ATR, cfg.max_hold_bars)\n",
        "\n",
        "pf_test = profit_factor(tr_test)\n",
        "dd_test = max_drawdown(eq_test[\"equity\"])\n",
        "print(\"TEST RESULTS:\", {\"ProfitFactor\": pf_test, \"Trades\": int(len(tr_test)), \"MaxDD\": dd_test, \"FinalEquity\": float(eq_test['equity'].iloc[-1])})\n",
        "\n",
        "tr_test.head(), tr_test.tail()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Candidates breakdown:\n",
            "  macro_ok==1: 49807\n",
            "  p_edge > TH: 0\n",
            "  p_entry> TH: 0\n",
            "  all conditions: 0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ===== 10) Quick sanity checks (why 0 trades?) =====\n",
        "def explain_zero_trades(df: pd.DataFrame, p_edge: np.ndarray, p_ent: np.ndarray, EDGE_TH: float, ENTRY_TH: float):\n",
        "    c_macro = int((df[\"macro_ok\"]==1).sum())\n",
        "    c_edge  = int((p_edge > EDGE_TH).sum())\n",
        "    c_ent   = int((p_ent > ENTRY_TH).sum())\n",
        "    c_both  = int(((df[\"macro_ok\"]==1) & (p_edge>EDGE_TH) & (p_ent>ENTRY_TH)).sum())\n",
        "    print(\"Candidates breakdown:\")\n",
        "    print(\"  macro_ok==1:\", c_macro)\n",
        "    print(\"  p_edge > TH:\", c_edge)\n",
        "    print(\"  p_entry> TH:\", c_ent)\n",
        "    print(\"  all conditions:\", c_both)\n",
        "\n",
        "explain_zero_trades(test_df, p_edge_test, p_ent_test, EDGE_TH, ENTRY_TH)\n",
        "\n",
        "# If you still see 0, temporarily drop thresholds to verify engine works:\n",
        "# tr_dbg, _ = run_backtest(test_df, p_edge_test, p_ent_test, 0.50, 0.50, TP_ATR, SL_ATR, cfg.max_hold_bars)\n",
        "# print(\"DEBUG trades:\", len(tr_dbg), \"PF:\", profit_factor(tr_dbg))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
