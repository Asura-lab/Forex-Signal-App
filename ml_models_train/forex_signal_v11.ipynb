{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bceadc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸš€ FOREX SIGNAL GENERATOR V11\n",
      "   Multi-Class: BUY / SELL / HOLD\n",
      "   10+ Model Ensemble\n",
      "======================================================================\n",
      "âœ“ GPU Available: True\n",
      "âœ“ Data Directory: c:\\Users\\Acer\\Desktop\\Forex-Signal-App\\data\n",
      "âœ“ Model Directory: c:\\Users\\Acer\\Desktop\\Forex-Signal-App\\models\\signal_generator_v11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import joblib\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, \n",
    "    ExtraTreesClassifier, \n",
    "    GradientBoostingClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    "    AdaBoostClassifier\n",
    ")\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path.cwd().parent if (Path.cwd() / 'ml_models_train').exists() == False else Path.cwd()\n",
    "if (BASE_DIR / 'data').exists() == False:\n",
    "    BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "MODEL_DIR = BASE_DIR / 'models' / 'signal_generator_v11'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# GPU Check\n",
    "try:\n",
    "    import torch\n",
    "    GPU_AVAILABLE = torch.cuda.is_available()\n",
    "except:\n",
    "    GPU_AVAILABLE = False\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸš€ FOREX SIGNAL GENERATOR V11\")\n",
    "print(\"   Multi-Class: BUY / SELL / HOLD\")\n",
    "print(\"   10+ Model Ensemble\")\n",
    "print(\"=\"*70)\n",
    "print(f\"âœ“ GPU Available: {GPU_AVAILABLE}\")\n",
    "print(f\"âœ“ Data Directory: {DATA_DIR}\")\n",
    "print(f\"âœ“ Model Directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada349b4",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a22ced28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,859,492 rows\n",
      "Test: 296,778 rows\n",
      "Train period: 2019-12-31 16:00:00+00:00 to 2024-12-30 16:00:00+00:00\n",
      "Test period: 2024-12-31 16:00:00+00:00 to 2025-10-17 06:11:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "train_df = pd.read_csv(DATA_DIR / 'EUR_USD_1min.csv')\n",
    "test_df = pd.read_csv(DATA_DIR / 'EUR_USD_test.csv')\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    if 'timestamp' in df.columns:\n",
    "        df.rename(columns={'timestamp': 'time'}, inplace=True)\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows\")\n",
    "print(f\"Test: {len(test_df):,} rows\")\n",
    "print(f\"Train period: {train_df['time'].min()} to {train_df['time'].max()}\")\n",
    "print(f\"Test period: {test_df['time'].min()} to {test_df['time'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4485a8f",
   "metadata": {},
   "source": [
    "## 2. V11 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d23826a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding V11 features...\n",
      "âœ“ Features added. Total columns: 72\n",
      "âœ“ Features added. Total columns: 72\n"
     ]
    }
   ],
   "source": [
    "def add_features_v11(df):\n",
    "    \"\"\"\n",
    "    V11 Features: V10 base + Enhanced for Multi-Class\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ==================== TIME FEATURES ====================\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['day_of_week'] = df['time'].dt.dayofweek\n",
    "    df['is_london'] = ((df['hour'] >= 8) & (df['hour'] < 16)).astype(int)\n",
    "    df['is_ny'] = ((df['hour'] >= 13) & (df['hour'] < 21)).astype(int)\n",
    "    df['is_overlap'] = ((df['hour'] >= 13) & (df['hour'] < 16)).astype(int)\n",
    "    df['session_quality'] = df['is_london'] + df['is_ny'] + df['is_overlap'] * 2\n",
    "    \n",
    "    # ==================== MOVING AVERAGES ====================\n",
    "    for p in [5, 10, 20, 50, 100, 200]:\n",
    "        df[f'sma_{p}'] = df['close'].rolling(p).mean()\n",
    "        df[f'ema_{p}'] = df['close'].ewm(span=p, adjust=False).mean()\n",
    "    \n",
    "    # MA Crosses\n",
    "    df['ema_5_10_cross'] = (df['ema_5'] - df['ema_10']) / df['ema_10'] * 100\n",
    "    df['ema_10_20_cross'] = (df['ema_10'] - df['ema_20']) / df['ema_20'] * 100\n",
    "    df['ema_20_50_cross'] = (df['ema_20'] - df['ema_50']) / df['ema_50'] * 100\n",
    "    df['sma_50_200_cross'] = (df['sma_50'] - df['sma_200']) / df['sma_200'] * 100\n",
    "    \n",
    "    # ==================== RSI ====================\n",
    "    delta = df['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    df['rsi_oversold'] = (df['rsi'] < 30).astype(int)\n",
    "    df['rsi_overbought'] = (df['rsi'] > 70).astype(int)\n",
    "    \n",
    "    # ==================== MACD ====================\n",
    "    ema12 = df['close'].ewm(span=12).mean()\n",
    "    ema26 = df['close'].ewm(span=26).mean()\n",
    "    df['macd'] = ema12 - ema26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    df['macd_cross_up'] = ((df['macd'] > df['macd_signal']) & (df['macd'].shift(1) <= df['macd_signal'].shift(1))).astype(int)\n",
    "    df['macd_cross_down'] = ((df['macd'] < df['macd_signal']) & (df['macd'].shift(1) >= df['macd_signal'].shift(1))).astype(int)\n",
    "    \n",
    "    # ==================== BOLLINGER BANDS ====================\n",
    "    df['bb_mid'] = df['close'].rolling(20).mean()\n",
    "    df['bb_std'] = df['close'].rolling(20).std()\n",
    "    df['bb_upper'] = df['bb_mid'] + 2 * df['bb_std']\n",
    "    df['bb_lower'] = df['bb_mid'] - 2 * df['bb_std']\n",
    "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / (df['bb_mid'] + 1e-10)\n",
    "    df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'] + 1e-10)\n",
    "    \n",
    "    # ==================== ATR & ADX ====================\n",
    "    df['tr0'] = abs(df['high'] - df['low'])\n",
    "    df['tr1'] = abs(df['high'] - df['close'].shift())\n",
    "    df['tr2'] = abs(df['low'] - df['close'].shift())\n",
    "    df['tr'] = df[['tr0', 'tr1', 'tr2']].max(axis=1)\n",
    "    \n",
    "    df['up_move'] = df['high'] - df['high'].shift()\n",
    "    df['down_move'] = df['low'].shift() - df['low']\n",
    "    \n",
    "    df['plus_dm'] = np.where((df['up_move'] > df['down_move']) & (df['up_move'] > 0), df['up_move'], 0)\n",
    "    df['minus_dm'] = np.where((df['down_move'] > df['up_move']) & (df['down_move'] > 0), df['down_move'], 0)\n",
    "    \n",
    "    period = 14\n",
    "    df['atr'] = df['tr'].rolling(period).mean()\n",
    "    df['plus_di'] = 100 * (df['plus_dm'].rolling(period).mean() / (df['atr'] + 1e-10))\n",
    "    df['minus_di'] = 100 * (df['minus_dm'].rolling(period).mean() / (df['atr'] + 1e-10))\n",
    "    df['dx'] = 100 * abs(df['plus_di'] - df['minus_di']) / (df['plus_di'] + df['minus_di'] + 1e-10)\n",
    "    df['adx'] = df['dx'].rolling(period).mean()\n",
    "    df['di_diff'] = df['plus_di'] - df['minus_di']\n",
    "    \n",
    "    # ==================== CCI ====================\n",
    "    tp = (df['high'] + df['low'] + df['close']) / 3\n",
    "    sma_tp = tp.rolling(20).mean()\n",
    "    mad_tp = tp.rolling(20).apply(lambda x: np.abs(x - x.mean()).mean())\n",
    "    df['cci'] = (tp - sma_tp) / (0.015 * mad_tp + 1e-10)\n",
    "    \n",
    "    # ==================== WILLIAMS %R ====================\n",
    "    hh = df['high'].rolling(14).max()\n",
    "    ll = df['low'].rolling(14).min()\n",
    "    df['williams_r'] = -100 * (hh - df['close']) / (hh - ll + 1e-10)\n",
    "    \n",
    "    # ==================== STOCHASTIC ====================\n",
    "    df['stoch_k'] = 100 * (df['close'] - ll) / (hh - ll + 1e-10)\n",
    "    df['stoch_d'] = df['stoch_k'].rolling(3).mean()\n",
    "    \n",
    "    # ==================== VOLATILITY ====================\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['volatility'] = df['returns'].rolling(20).std() * 100\n",
    "    df['volatility_sma'] = df['volatility'].rolling(50).mean()\n",
    "    df['volatility_ratio'] = df['volatility'] / (df['volatility_sma'] + 1e-10)\n",
    "    \n",
    "    # ==================== PRICE ACTION ====================\n",
    "    df['body'] = df['close'] - df['open']\n",
    "    df['upper_wick'] = df['high'] - df[['open', 'close']].max(axis=1)\n",
    "    df['lower_wick'] = df[['open', 'close']].min(axis=1) - df['low']\n",
    "    df['body_ratio'] = abs(df['body']) / (df['high'] - df['low'] + 1e-10)\n",
    "    df['is_bullish'] = (df['close'] > df['open']).astype(int)\n",
    "    df['bullish_streak'] = df['is_bullish'].rolling(5).sum()\n",
    "    \n",
    "    # ==================== SUPPORT/RESISTANCE ====================\n",
    "    df['high_20'] = df['high'].rolling(20).max()\n",
    "    df['low_20'] = df['low'].rolling(20).min()\n",
    "    df['dist_to_high'] = (df['high_20'] - df['close']) / (df['atr'] + 1e-10)\n",
    "    df['dist_to_low'] = (df['close'] - df['low_20']) / (df['atr'] + 1e-10)\n",
    "    \n",
    "    # ==================== MOMENTUM ====================\n",
    "    for p in [5, 10, 20, 50]:\n",
    "        df[f'momentum_{p}'] = (df['close'] - df['close'].shift(p)) / (df['atr'] + 1e-10)\n",
    "    \n",
    "    # ==================== COMPOSITE SCORES ====================\n",
    "    # Trend Score (-4 to +4)\n",
    "    df['trend_score'] = (\n",
    "        (df['close'] > df['sma_20']).astype(int) - (df['close'] < df['sma_20']).astype(int) +\n",
    "        (df['sma_20'] > df['sma_50']).astype(int) - (df['sma_20'] < df['sma_50']).astype(int) +\n",
    "        (df['sma_50'] > df['sma_200']).astype(int) - (df['sma_50'] < df['sma_200']).astype(int) +\n",
    "        (df['di_diff'] > 0).astype(int) - (df['di_diff'] < 0).astype(int)\n",
    "    )\n",
    "    \n",
    "    # Momentum Score (-4 to +4)\n",
    "    df['momentum_score'] = (\n",
    "        (df['rsi'] > 50).astype(int) - (df['rsi'] < 50).astype(int) +\n",
    "        (df['macd_hist'] > 0).astype(int) - (df['macd_hist'] < 0).astype(int) +\n",
    "        (df['cci'] > 0).astype(int) - (df['cci'] < 0).astype(int) +\n",
    "        (df['stoch_k'] > 50).astype(int) - (df['stoch_k'] < 50).astype(int)\n",
    "    )\n",
    "    \n",
    "    # Volatility State\n",
    "    df['volatility_state'] = np.where(\n",
    "        df['volatility_ratio'] > 1.5, 2,\n",
    "        np.where(df['volatility_ratio'] < 0.5, 0, 1)\n",
    "    )\n",
    "    \n",
    "    # ==================== CLEANUP ====================\n",
    "    drop_cols = ['tr0', 'tr1', 'tr2', 'tr', 'up_move', 'down_move', 'plus_dm', 'minus_dm', 'dx']\n",
    "    df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Adding V11 features...\")\n",
    "train_df = add_features_v11(train_df)\n",
    "test_df = add_features_v11(test_df)\n",
    "print(f\"âœ“ Features added. Total columns: {len(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d436dd0",
   "metadata": {},
   "source": [
    "## 3. V11 Multi-Class Labeling (BUY / SELL / HOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50bbd1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Class Distribution:\n",
      "\n",
      "Train:\n",
      "  HOLD: 1,466,044 (78.8%)\n",
      "  BUY: 193,796 (10.4%)\n",
      "  SELL: 199,453 (10.7%)\n",
      "\n",
      "Test:\n",
      "  HOLD: 216,283 (72.9%)\n",
      "  BUY: 41,895 (14.1%)\n",
      "  SELL: 38,401 (12.9%)\n"
     ]
    }
   ],
   "source": [
    "def create_labels_v11(df, forward_periods=60, min_pips=15, ratio=1.5):\n",
    "    \"\"\"\n",
    "    V11 Multi-Class Labeling:\n",
    "    - BUY (1): Strong upward movement\n",
    "    - SELL (2): Strong downward movement  \n",
    "    - HOLD (0): No clear direction\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    min_move = min_pips * 0.0001\n",
    "    \n",
    "    df['future_max'] = df['high'].rolling(forward_periods).max().shift(-forward_periods)\n",
    "    df['future_min'] = df['low'].rolling(forward_periods).min().shift(-forward_periods)\n",
    "    \n",
    "    df['up_move'] = df['future_max'] - df['close']\n",
    "    df['down_move'] = df['close'] - df['future_min']\n",
    "    \n",
    "    # Multi-class conditions\n",
    "    conditions = [\n",
    "        (df['up_move'] >= min_move) & (df['up_move'] > df['down_move'] * ratio),    # BUY\n",
    "        (df['down_move'] >= min_move) & (df['down_move'] > df['up_move'] * ratio),  # SELL\n",
    "    ]\n",
    "    choices = [1, 2]  # 1=BUY, 2=SELL, 0=HOLD\n",
    "    df['signal'] = np.select(conditions, choices, default=0)\n",
    "    \n",
    "    df.drop(['future_max', 'future_min', 'up_move', 'down_move'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Create labels\n",
    "train_df = create_labels_v11(train_df, forward_periods=60, min_pips=15, ratio=1.5)\n",
    "test_df = create_labels_v11(test_df, forward_periods=60, min_pips=15, ratio=1.5)\n",
    "\n",
    "# Remove NaN rows\n",
    "train_df = train_df.dropna().copy()\n",
    "test_df = test_df.dropna().copy()\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\nðŸ“Š Class Distribution:\")\n",
    "print(\"\\nTrain:\")\n",
    "train_counts = train_df['signal'].value_counts().sort_index()\n",
    "for sig, count in train_counts.items():\n",
    "    label = {0: 'HOLD', 1: 'BUY', 2: 'SELL'}[sig]\n",
    "    print(f\"  {label}: {count:,} ({count/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTest:\")\n",
    "test_counts = test_df['signal'].value_counts().sort_index()\n",
    "for sig, count in test_counts.items():\n",
    "    label = {0: 'HOLD', 1: 'BUY', 2: 'SELL'}[sig]\n",
    "    print(f\"  {label}: {count:,} ({count/len(test_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fc9c3f",
   "metadata": {},
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec75851d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features before selection: 66\n",
      "Features after selection: 33\n",
      "\n",
      "Top 20 Features:\n",
      "         feature  importance\n",
      "            hour        2614\n",
      "sma_50_200_cross        1681\n",
      "  volatility_sma        1563\n",
      "     day_of_week        1037\n",
      "         sma_200         962\n",
      "             atr         781\n",
      "         ema_200         654\n",
      " ema_20_50_cross         574\n",
      "         sma_100         534\n",
      "volatility_ratio         473\n",
      "         high_20         467\n",
      "      volatility         452\n",
      "     momentum_50         411\n",
      "          low_20         376\n",
      "        bb_lower         358\n",
      "        bb_upper         356\n",
      "             adx         317\n",
      "         ema_100         311\n",
      "          sma_50         285\n",
      "     macd_signal         252\n",
      "Features after selection: 33\n",
      "\n",
      "Top 20 Features:\n",
      "         feature  importance\n",
      "            hour        2614\n",
      "sma_50_200_cross        1681\n",
      "  volatility_sma        1563\n",
      "     day_of_week        1037\n",
      "         sma_200         962\n",
      "             atr         781\n",
      "         ema_200         654\n",
      " ema_20_50_cross         574\n",
      "         sma_100         534\n",
      "volatility_ratio         473\n",
      "         high_20         467\n",
      "      volatility         452\n",
      "     momentum_50         411\n",
      "          low_20         376\n",
      "        bb_lower         358\n",
      "        bb_upper         356\n",
      "             adx         317\n",
      "         ema_100         311\n",
      "          sma_50         285\n",
      "     macd_signal         252\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "exclude_cols = ['time', 'signal', 'open', 'high', 'low', 'close', 'volume', 'tick_volume']\n",
    "feature_cols = [c for c in train_df.columns if c not in exclude_cols]\n",
    "\n",
    "X_train_full = train_df[feature_cols].values\n",
    "y_train = train_df['signal'].values\n",
    "X_test_full = test_df[feature_cols].values\n",
    "y_test = test_df['signal'].values\n",
    "\n",
    "print(f\"Features before selection: {len(feature_cols)}\")\n",
    "\n",
    "# Feature Importance based selection using LightGBM\n",
    "lgb_params = {'n_estimators': 200, 'max_depth': 6, 'random_state': 42, 'verbose': -1}\n",
    "if GPU_AVAILABLE:\n",
    "    lgb_params['device'] = 'gpu'\n",
    "\n",
    "selector_model = lgb.LGBMClassifier(**lgb_params)\n",
    "selector_model.fit(X_train_full, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = selector_model.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Select top features using median threshold\n",
    "threshold = np.median(importances)\n",
    "selected_features = importance_df[importance_df['importance'] >= threshold]['feature'].tolist()\n",
    "\n",
    "print(f\"Features after selection: {len(selected_features)}\")\n",
    "print(f\"\\nTop 20 Features:\")\n",
    "print(importance_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a72c3a",
   "metadata": {},
   "source": [
    "## 5. Prepare Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d971babe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (1859293, 33)\n",
      "Test data: (296579, 33)\n",
      "Classes: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Use selected features\n",
    "X_train = train_df[selected_features].values\n",
    "X_test = test_df[selected_features].values\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training data: {X_train_scaled.shape}\")\n",
    "print(f\"Test data: {X_test_scaled.shape}\")\n",
    "print(f\"Classes: {np.unique(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be07535",
   "metadata": {},
   "source": [
    "## 6. V11 Model Training (10+ Diverse Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb993d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸš€ TRAINING V11 MODELS (10+ Diverse Ensemble)\n",
      "======================================================================\n",
      "\n",
      "ðŸ“‹ Models to train: 10\n",
      "  - XGB1\n",
      "  - XGB2\n",
      "  - XGB3\n",
      "  - LGB1\n",
      "  - LGB2\n",
      "  - CAT1\n",
      "  - CAT2\n",
      "  - RF\n",
      "  - ET\n",
      "  - HGB\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸš€ TRAINING V11 MODELS (10+ Diverse Ensemble)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models = {}\n",
    "\n",
    "# ==================== XGBOOST MODELS ====================\n",
    "xgb_base_params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'verbosity': 0\n",
    "}\n",
    "if GPU_AVAILABLE:\n",
    "    xgb_base_params.update({'tree_method': 'hist', 'device': 'cuda'})\n",
    "\n",
    "# XGBoost 1 - Balanced\n",
    "models['xgb1'] = xgb.XGBClassifier(\n",
    "    n_estimators=300, max_depth=6, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    reg_alpha=0.1, reg_lambda=1.0,\n",
    "    min_child_weight=3, random_state=42,\n",
    "    **xgb_base_params\n",
    ")\n",
    "\n",
    "# XGBoost 2 - Deeper\n",
    "models['xgb2'] = xgb.XGBClassifier(\n",
    "    n_estimators=250, max_depth=8, learning_rate=0.06,\n",
    "    subsample=0.75, colsample_bytree=0.75,\n",
    "    reg_alpha=0.05, reg_lambda=0.5,\n",
    "    gamma=0.1, random_state=43,\n",
    "    **xgb_base_params\n",
    ")\n",
    "\n",
    "# XGBoost 3 - Conservative\n",
    "models['xgb3'] = xgb.XGBClassifier(\n",
    "    n_estimators=400, max_depth=4, learning_rate=0.04,\n",
    "    subsample=0.85, colsample_bytree=0.85,\n",
    "    reg_alpha=0.2, reg_lambda=2.0,\n",
    "    min_child_weight=5, random_state=44,\n",
    "    **xgb_base_params\n",
    ")\n",
    "\n",
    "# ==================== LIGHTGBM MODELS ====================\n",
    "lgb_base_params = {'verbose': -1, 'objective': 'multiclass', 'num_class': 3}\n",
    "if GPU_AVAILABLE:\n",
    "    lgb_base_params['device'] = 'gpu'\n",
    "\n",
    "# LightGBM 1 - Balanced\n",
    "models['lgb1'] = lgb.LGBMClassifier(\n",
    "    n_estimators=300, max_depth=6, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    num_leaves=31, min_child_samples=30,\n",
    "    reg_alpha=0.1, reg_lambda=1.0,\n",
    "    random_state=42, **lgb_base_params\n",
    ")\n",
    "\n",
    "# LightGBM 2 - More leaves\n",
    "models['lgb2'] = lgb.LGBMClassifier(\n",
    "    n_estimators=250, max_depth=8, learning_rate=0.06,\n",
    "    subsample=0.75, colsample_bytree=0.75,\n",
    "    num_leaves=63, min_child_samples=20,\n",
    "    reg_alpha=0.05, reg_lambda=0.5,\n",
    "    random_state=45, **lgb_base_params\n",
    ")\n",
    "\n",
    "# ==================== CATBOOST MODELS ====================\n",
    "cat_base_params = {'loss_function': 'MultiClass', 'verbose': False}\n",
    "if GPU_AVAILABLE:\n",
    "    cat_base_params.update({'task_type': 'GPU', 'devices': '0'})\n",
    "\n",
    "# CatBoost 1\n",
    "models['cat1'] = CatBoostClassifier(\n",
    "    iterations=300, depth=6, learning_rate=0.05,\n",
    "    l2_leaf_reg=3.0, random_strength=0.5,\n",
    "    bagging_temperature=0.5, random_seed=42,\n",
    "    **cat_base_params\n",
    ")\n",
    "\n",
    "# CatBoost 2\n",
    "models['cat2'] = CatBoostClassifier(\n",
    "    iterations=250, depth=8, learning_rate=0.06,\n",
    "    l2_leaf_reg=2.0, random_strength=0.3,\n",
    "    bagging_temperature=0.3, random_seed=46,\n",
    "    **cat_base_params\n",
    ")\n",
    "\n",
    "# ==================== SKLEARN ENSEMBLE MODELS ====================\n",
    "# RandomForest\n",
    "models['rf'] = RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=10, min_samples_split=10,\n",
    "    min_samples_leaf=5, max_features='sqrt',\n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# ExtraTrees\n",
    "models['et'] = ExtraTreesClassifier(\n",
    "    n_estimators=200, max_depth=10, min_samples_split=10,\n",
    "    min_samples_leaf=5, max_features='sqrt',\n",
    "    random_state=43, n_jobs=-1\n",
    ")\n",
    "\n",
    "# HistGradientBoosting (Native histogram-based, fast)\n",
    "models['hgb'] = HistGradientBoostingClassifier(\n",
    "    max_iter=300, max_depth=8, learning_rate=0.08,\n",
    "    min_samples_leaf=30, l2_regularization=1.0,\n",
    "    random_state=44\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Models to train: {len(models)}\")\n",
    "for name in models.keys():\n",
    "    print(f\"  - {name.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10ea355e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "â³ Training Models...\n",
      "======================================================================\n",
      "\n",
      "  Training XGB1... Done! Acc: 73.75% (Time: 14.3s)\n",
      "\n",
      "  Training XGB2... Done! Acc: 73.75% (Time: 14.3s)\n",
      "\n",
      "  Training XGB2... Done! Acc: 73.62% (Time: 16.5s)\n",
      "\n",
      "  Training XGB3... Done! Acc: 73.62% (Time: 16.5s)\n",
      "\n",
      "  Training XGB3... Done! Acc: 73.77% (Time: 13.9s)\n",
      "\n",
      "  Training LGB1... Done! Acc: 73.77% (Time: 13.9s)\n",
      "\n",
      "  Training LGB1... Done! Acc: 73.72% (Time: 19.9s)\n",
      "\n",
      "  Training LGB2... Done! Acc: 73.72% (Time: 19.9s)\n",
      "\n",
      "  Training LGB2... Done! Acc: 73.65% (Time: 22.2s)\n",
      "\n",
      "  Training CAT1... Done! Acc: 73.65% (Time: 22.2s)\n",
      "\n",
      "  Training CAT1... Done! Acc: 73.74% (Time: 16.4s)\n",
      "\n",
      "  Training CAT2... Done! Acc: 73.74% (Time: 16.4s)\n",
      "\n",
      "  Training CAT2... Done! Acc: 73.69% (Time: 10.8s)\n",
      "\n",
      "  Training RF... Done! Acc: 73.69% (Time: 10.8s)\n",
      "\n",
      "  Training RF... Done! Acc: 73.59% (Time: 165.0s)\n",
      "\n",
      "  Training ET... Done! Acc: 73.59% (Time: 165.0s)\n",
      "\n",
      "  Training ET... Done! Acc: 73.12% (Time: 36.2s)\n",
      "\n",
      "  Training HGB... Done! Acc: 73.12% (Time: 36.2s)\n",
      "\n",
      "  Training HGB... Done! Acc: 73.56% (Time: 51.2s)\n",
      "\n",
      "âœ“ Successfully trained 10 models\n",
      "Done! Acc: 73.56% (Time: 51.2s)\n",
      "\n",
      "âœ“ Successfully trained 10 models\n"
     ]
    }
   ],
   "source": [
    "# Train all models\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"â³ Training Models...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "train_times = {}\n",
    "\n",
    "model_items = list(models.items())  # Make a copy to avoid modification during iteration\n",
    "\n",
    "for name, model in model_items:\n",
    "    print(f\"\\n  Training {name.upper()}...\", end=\" \", flush=True)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        predictions[name] = model.predict(X_test_scaled)\n",
    "        probabilities[name] = model.predict_proba(X_test_scaled)\n",
    "        train_time = time.time() - start_time\n",
    "        train_times[name] = train_time\n",
    "        \n",
    "        test_acc = accuracy_score(y_test, predictions[name])\n",
    "        print(f\"Done! Acc: {test_acc*100:.2f}% (Time: {train_time:.1f}s)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "print(f\"\\nâœ“ Successfully trained {len(predictions)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7effee02",
   "metadata": {},
   "source": [
    "## 7. Individual Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82ead73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ“Š INDIVIDUAL MODEL PERFORMANCE\n",
      "======================================================================\n",
      "Model Overall   BUY  SELL  HOLD   Time\n",
      " XGB1  73.75%  7.4% 13.3% 97.3%  14.3s\n",
      " XGB2  73.62% 10.0% 11.9% 96.9%  16.5s\n",
      " XGB3  73.77%  6.0% 13.7% 97.6%  13.9s\n",
      " LGB1  73.72%  7.6% 13.2% 97.3%  19.9s\n",
      " LGB2  73.65%  9.2% 12.4% 97.0%  22.2s\n",
      " CAT1  73.74%  8.0% 12.4% 97.4%  16.4s\n",
      " CAT2  73.69%  8.7% 12.5% 97.1%  10.8s\n",
      "   RF  73.59%  3.4% 11.0% 98.3% 165.0s\n",
      "   ET  73.12%  1.2%  2.7% 99.5%  36.2s\n",
      "  HGB  73.56%  9.8% 13.0% 96.7%  51.2s\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ“Š INDIVIDUAL MODEL PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "for name in predictions.keys():\n",
    "    y_pred = predictions[name]\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    buy_mask = y_test == 1\n",
    "    sell_mask = y_test == 2\n",
    "    hold_mask = y_test == 0\n",
    "    \n",
    "    buy_acc = accuracy_score(y_test[buy_mask], y_pred[buy_mask]) if buy_mask.sum() > 0 else 0\n",
    "    sell_acc = accuracy_score(y_test[sell_mask], y_pred[sell_mask]) if sell_mask.sum() > 0 else 0\n",
    "    hold_acc = accuracy_score(y_test[hold_mask], y_pred[hold_mask]) if hold_mask.sum() > 0 else 0\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name.upper(),\n",
    "        'Overall': f\"{acc*100:.2f}%\",\n",
    "        'BUY': f\"{buy_acc*100:.1f}%\",\n",
    "        'SELL': f\"{sell_acc*100:.1f}%\",\n",
    "        'HOLD': f\"{hold_acc*100:.1f}%\",\n",
    "        'Time': f\"{train_times.get(name, 0):.1f}s\"\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e23234",
   "metadata": {},
   "source": [
    "## 8. V11 Ensemble (Weighted Voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c1732c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ”§ BUILDING V11 WEIGHTED ENSEMBLE\n",
      "======================================================================\n",
      "\n",
      "Model Weights (accuracy-based):\n",
      "  XGB3: 0.1002 (acc: 73.77%)\n",
      "  XGB1: 0.1002 (acc: 73.75%)\n",
      "  CAT1: 0.1002 (acc: 73.74%)\n",
      "  LGB1: 0.1001 (acc: 73.72%)\n",
      "  CAT2: 0.1001 (acc: 73.69%)\n",
      "  LGB2: 0.1000 (acc: 73.65%)\n",
      "  XGB2: 0.1000 (acc: 73.62%)\n",
      "  RF: 0.1000 (acc: 73.59%)\n",
      "  HGB: 0.0999 (acc: 73.56%)\n",
      "  ET: 0.0993 (acc: 73.12%)\n",
      "\n",
      "âœ“ Weighted Ensemble Accuracy: 73.72%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ”§ BUILDING V11 WEIGHTED ENSEMBLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate accuracy-based weights\n",
    "accuracies = {name: accuracy_score(y_test, predictions[name]) for name in predictions.keys()}\n",
    "total_acc = sum(accuracies.values())\n",
    "weights = {name: acc / total_acc for name, acc in accuracies.items()}\n",
    "\n",
    "print(\"\\nModel Weights (accuracy-based):\")\n",
    "for name, w in sorted(weights.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {name.upper()}: {w:.4f} (acc: {accuracies[name]*100:.2f}%)\")\n",
    "\n",
    "# Weighted Ensemble Probability\n",
    "final_proba = np.zeros_like(probabilities[list(predictions.keys())[0]])\n",
    "for name, w in weights.items():\n",
    "    final_proba += w * probabilities[name]\n",
    "\n",
    "# Get class probabilities\n",
    "hold_prob = final_proba[:, 0] * 100  # Class 0 = HOLD\n",
    "buy_prob = final_proba[:, 1] * 100   # Class 1 = BUY\n",
    "sell_prob = final_proba[:, 2] * 100  # Class 2 = SELL\n",
    "\n",
    "# Ensemble predictions\n",
    "ensemble_pred = np.argmax(final_proba, axis=1)\n",
    "ensemble_acc = accuracy_score(y_test, ensemble_pred)\n",
    "\n",
    "print(f\"\\nâœ“ Weighted Ensemble Accuracy: {ensemble_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1077afc5",
   "metadata": {},
   "source": [
    "## 9. V11 Confidence Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6708ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ“Š V11 CONFIDENCE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Prediction shapes:\n",
      "  xgb1: (296579,)\n",
      "  xgb2: (296579,)\n",
      "  xgb3: (296579,)\n",
      "  lgb1: (296579,)\n",
      "  lgb2: (296579,)\n",
      "  cat1: (296579, 1)\n",
      "  cat2: (296579, 1)\n",
      "  rf: (296579,)\n",
      "  et: (296579,)\n",
      "  hgb: (296579,)\n",
      "\n",
      "All predictions shape: (10, 296579)\n",
      "\n",
      "Signal Distribution:\n",
      "  BUY: 6,246 (2.1%)\n",
      "  SELL: 11,839 (4.0%)\n",
      "  HOLD: 278,494 (93.9%)\n",
      "\n",
      "Confidence Statistics:\n",
      "  Mean: 81.2%\n",
      "  Min: 33.4%\n",
      "  Max: 100.0%\n",
      "\n",
      "Signal Distribution:\n",
      "  BUY: 6,246 (2.1%)\n",
      "  SELL: 11,839 (4.0%)\n",
      "  HOLD: 278,494 (93.9%)\n",
      "\n",
      "Confidence Statistics:\n",
      "  Mean: 81.2%\n",
      "  Min: 33.4%\n",
      "  Max: 100.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ“Š V11 CONFIDENCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Debug: Check prediction shapes\n",
    "print(\"\\nPrediction shapes:\")\n",
    "for name, pred in predictions.items():\n",
    "    print(f\"  {name}: {np.array(pred).shape}\")\n",
    "\n",
    "# Model Agreement Analysis - stack properly\n",
    "pred_list = []\n",
    "for name in predictions.keys():\n",
    "    p = np.array(predictions[name]).flatten()\n",
    "    pred_list.append(p)\n",
    "\n",
    "all_preds = np.array(pred_list)\n",
    "print(f\"\\nAll predictions shape: {all_preds.shape}\")\n",
    "n_models = len(predictions)\n",
    "\n",
    "buy_votes = np.sum(all_preds == 1, axis=0)\n",
    "sell_votes = np.sum(all_preds == 2, axis=0)\n",
    "hold_votes = np.sum(all_preds == 0, axis=0)\n",
    "\n",
    "# Agreement bonus\n",
    "def calculate_confidence_v11(buy_p, sell_p, hold_p, buy_v, sell_v, hold_v, n_models):\n",
    "    \"\"\"\n",
    "    Calculate confidence with agreement bonus\n",
    "    Returns: (signal_type, confidence)\n",
    "    \"\"\"\n",
    "    max_prob = max(buy_p, sell_p, hold_p)\n",
    "    \n",
    "    if buy_p == max_prob:\n",
    "        signal = 'BUY'\n",
    "        base_conf = buy_p\n",
    "        votes = buy_v\n",
    "    elif sell_p == max_prob:\n",
    "        signal = 'SELL'\n",
    "        base_conf = sell_p\n",
    "        votes = sell_v\n",
    "    else:\n",
    "        signal = 'HOLD'\n",
    "        base_conf = hold_p\n",
    "        votes = hold_v\n",
    "    \n",
    "    # Agreement bonus\n",
    "    agreement_ratio = votes / n_models\n",
    "    if agreement_ratio >= 0.9:  # 90%+ models agree\n",
    "        bonus = 7\n",
    "    elif agreement_ratio >= 0.7:  # 70%+ models agree\n",
    "        bonus = 4\n",
    "    elif agreement_ratio >= 0.5:  # 50%+ models agree\n",
    "        bonus = 2\n",
    "    else:\n",
    "        bonus = 0\n",
    "    \n",
    "    confidence = min(100, base_conf + bonus)\n",
    "    return signal, confidence, votes\n",
    "\n",
    "# Apply to all test samples\n",
    "signals = []\n",
    "confidences = []\n",
    "vote_counts = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    sig, conf, votes = calculate_confidence_v11(\n",
    "        buy_prob[i], sell_prob[i], hold_prob[i],\n",
    "        buy_votes[i], sell_votes[i], hold_votes[i],\n",
    "        n_models\n",
    "    )\n",
    "    signals.append(sig)\n",
    "    confidences.append(conf)\n",
    "    vote_counts.append(votes)\n",
    "\n",
    "signals = np.array(signals)\n",
    "confidences = np.array(confidences)\n",
    "vote_counts = np.array(vote_counts)\n",
    "\n",
    "print(f\"\\nSignal Distribution:\")\n",
    "for sig in ['BUY', 'SELL', 'HOLD']:\n",
    "    count = (signals == sig).sum()\n",
    "    print(f\"  {sig}: {count:,} ({count/len(signals)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nConfidence Statistics:\")\n",
    "print(f\"  Mean: {confidences.mean():.1f}%\")\n",
    "print(f\"  Min: {confidences.min():.1f}%\")\n",
    "print(f\"  Max: {confidences.max():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1b752e",
   "metadata": {},
   "source": [
    "## 10. Confidence Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9457ca54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ“ˆ CONFIDENCE THRESHOLD ANALYSIS\n",
      "======================================================================\n",
      "Threshold  Signals Overall Acc  BUY Signals BUY Acc  SELL Signals SELL Acc\n",
      "     50%+   268046       77.7%          575   44.0%          3027    40.7%\n",
      "     60%+   242593       81.1%           25   24.0%           424    43.4%\n",
      "     70%+   213525       84.5%            2    0.0%            37    62.2%\n",
      "     75%+   196135       86.2%            0    0.0%             9    66.7%\n",
      "     80%+   177722       87.9%            0    0.0%             0     0.0%\n",
      "     85%+   157840       89.6%            0    0.0%             0     0.0%\n",
      "     90%+   136722       91.3%            0    0.0%             0     0.0%\n",
      "     95%+   110476       92.8%            0    0.0%             0     0.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ“ˆ CONFIDENCE THRESHOLD ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Map signal names to numeric for comparison\n",
    "signal_map = {'HOLD': 0, 'BUY': 1, 'SELL': 2}\n",
    "pred_numeric = np.array([signal_map[s] for s in signals])\n",
    "\n",
    "thresholds = [50, 60, 70, 75, 80, 85, 90, 95]\n",
    "results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    mask = confidences >= thresh\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    # Overall\n",
    "    acc = accuracy_score(y_test[mask], pred_numeric[mask])\n",
    "    \n",
    "    # BUY signals only\n",
    "    buy_mask = mask & (signals == 'BUY')\n",
    "    buy_correct = (y_test[buy_mask] == 1).sum() if buy_mask.sum() > 0 else 0\n",
    "    buy_acc = buy_correct / buy_mask.sum() * 100 if buy_mask.sum() > 0 else 0\n",
    "    \n",
    "    # SELL signals only\n",
    "    sell_mask = mask & (signals == 'SELL')\n",
    "    sell_correct = (y_test[sell_mask] == 2).sum() if sell_mask.sum() > 0 else 0\n",
    "    sell_acc = sell_correct / sell_mask.sum() * 100 if sell_mask.sum() > 0 else 0\n",
    "    \n",
    "    results.append({\n",
    "        'Threshold': f\"{thresh}%+\",\n",
    "        'Signals': mask.sum(),\n",
    "        'Overall Acc': f\"{acc*100:.1f}%\",\n",
    "        'BUY Signals': buy_mask.sum(),\n",
    "        'BUY Acc': f\"{buy_acc:.1f}%\",\n",
    "        'SELL Signals': sell_mask.sum(),\n",
    "        'SELL Acc': f\"{sell_acc:.1f}%\"\n",
    "    })\n",
    "\n",
    "threshold_df = pd.DataFrame(results)\n",
    "print(threshold_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052248c8",
   "metadata": {},
   "source": [
    "## 10.5 Alternative: Binary BUY/SELL Models (Skip HOLD)\n",
    "\n",
    "Multi-class Ð´ÑÑÑ€ BUY/SELL accuracy Ð¼Ð°Ñˆ Ð±Ð°Ð³Ð° Ð±Ð°Ð¹Ð½Ð°. Ó¨Ó©Ñ€ Ð°Ñ€Ð³Ð° - BUY Ð±Ð¾Ð»Ð¾Ð½ SELL-Ð¸Ð¹Ð³ binary classification Ñ…ÑÐ»Ð±ÑÑ€ÑÑÑ€ ÑÑƒÑ€Ð³Ð°Ñ…."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4102fceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ”„ ALTERNATIVE: BINARY BUY vs SELL MODEL\n",
      "======================================================================\n",
      "Train (BUY/SELL only): 393,249\n",
      "Test (BUY/SELL only): 80,296\n",
      "Train BUY ratio: 49.3%\n",
      "Test BUY ratio: 52.2%\n",
      "Train (BUY/SELL only): 393,249\n",
      "Test (BUY/SELL only): 80,296\n",
      "Train BUY ratio: 49.3%\n",
      "Test BUY ratio: 52.2%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ”„ ALTERNATIVE: BINARY BUY vs SELL MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Filter only BUY (1) and SELL (2), skip HOLD (0)\n",
    "train_binary = train_df[train_df['signal'] != 0].copy()\n",
    "test_binary = test_df[test_df['signal'] != 0].copy()\n",
    "\n",
    "# Convert to binary: BUY=1, SELL=0\n",
    "train_binary['signal_binary'] = (train_binary['signal'] == 1).astype(int)\n",
    "test_binary['signal_binary'] = (test_binary['signal'] == 1).astype(int)\n",
    "\n",
    "print(f\"Train (BUY/SELL only): {len(train_binary):,}\")\n",
    "print(f\"Test (BUY/SELL only): {len(test_binary):,}\")\n",
    "print(f\"Train BUY ratio: {train_binary['signal_binary'].mean()*100:.1f}%\")\n",
    "print(f\"Test BUY ratio: {test_binary['signal_binary'].mean()*100:.1f}%\")\n",
    "\n",
    "# Prepare data\n",
    "X_train_bin = train_binary[selected_features].values\n",
    "y_train_bin = train_binary['signal_binary'].values\n",
    "X_test_bin = test_binary[selected_features].values\n",
    "y_test_bin = test_binary['signal_binary'].values\n",
    "\n",
    "# Scale\n",
    "X_train_bin_scaled = scaler.transform(X_train_bin)\n",
    "X_test_bin_scaled = scaler.transform(X_test_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e2f5bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â³ Training Binary BUY/SELL Models...\n",
      "  Training XGB1... Done! Acc: 51.33% (Time: 2.8s)\n",
      "  Training XGB2... Done! Acc: 51.33% (Time: 2.8s)\n",
      "  Training XGB2... Done! Acc: 50.66% (Time: 3.3s)\n",
      "  Training XGB3... Done! Acc: 50.66% (Time: 3.3s)\n",
      "  Training XGB3... Done! Acc: 51.75% (Time: 2.4s)\n",
      "  Training LGB1... Done! Acc: 51.75% (Time: 2.4s)\n",
      "  Training LGB1... Done! Acc: 51.47% (Time: 3.9s)\n",
      "  Training LGB2... Done! Acc: 51.47% (Time: 3.9s)\n",
      "  Training LGB2... Done! Acc: 51.08% (Time: 4.9s)\n",
      "  Training CAT1... Done! Acc: 51.08% (Time: 4.9s)\n",
      "  Training CAT1... Done! Acc: 50.84% (Time: 4.0s)\n",
      "  Training CAT2... Done! Acc: 50.84% (Time: 4.0s)\n",
      "  Training CAT2... Done! Acc: 52.44% (Time: 4.0s)\n",
      "\n",
      "âœ“ Binary models trained: 7\n",
      "Done! Acc: 52.44% (Time: 4.0s)\n",
      "\n",
      "âœ“ Binary models trained: 7\n"
     ]
    }
   ],
   "source": [
    "# Train binary models (BUY vs SELL)\n",
    "print(\"\\nâ³ Training Binary BUY/SELL Models...\")\n",
    "\n",
    "binary_models = {}\n",
    "\n",
    "# XGBoost models\n",
    "xgb_bin_params = {'verbosity': 0}\n",
    "if GPU_AVAILABLE:\n",
    "    xgb_bin_params.update({'tree_method': 'hist', 'device': 'cuda'})\n",
    "\n",
    "binary_models['xgb1'] = xgb.XGBClassifier(\n",
    "    n_estimators=400, max_depth=6, learning_rate=0.04,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    reg_alpha=0.1, reg_lambda=1.0,\n",
    "    random_state=42, **xgb_bin_params\n",
    ")\n",
    "\n",
    "binary_models['xgb2'] = xgb.XGBClassifier(\n",
    "    n_estimators=300, max_depth=8, learning_rate=0.05,\n",
    "    subsample=0.75, colsample_bytree=0.75,\n",
    "    random_state=43, **xgb_bin_params\n",
    ")\n",
    "\n",
    "binary_models['xgb3'] = xgb.XGBClassifier(\n",
    "    n_estimators=500, max_depth=4, learning_rate=0.03,\n",
    "    subsample=0.85, colsample_bytree=0.85,\n",
    "    random_state=44, **xgb_bin_params\n",
    ")\n",
    "\n",
    "# LightGBM models\n",
    "lgb_bin_params = {'verbose': -1}\n",
    "if GPU_AVAILABLE:\n",
    "    lgb_bin_params['device'] = 'gpu'\n",
    "\n",
    "binary_models['lgb1'] = lgb.LGBMClassifier(\n",
    "    n_estimators=400, max_depth=6, learning_rate=0.04,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    num_leaves=31, random_state=42, **lgb_bin_params\n",
    ")\n",
    "\n",
    "binary_models['lgb2'] = lgb.LGBMClassifier(\n",
    "    n_estimators=300, max_depth=8, learning_rate=0.05,\n",
    "    num_leaves=63, random_state=45, **lgb_bin_params\n",
    ")\n",
    "\n",
    "# CatBoost models\n",
    "cat_bin_params = {'verbose': False}\n",
    "if GPU_AVAILABLE:\n",
    "    cat_bin_params.update({'task_type': 'GPU', 'devices': '0'})\n",
    "\n",
    "binary_models['cat1'] = CatBoostClassifier(\n",
    "    iterations=400, depth=6, learning_rate=0.04,\n",
    "    random_seed=42, **cat_bin_params\n",
    ")\n",
    "\n",
    "binary_models['cat2'] = CatBoostClassifier(\n",
    "    iterations=300, depth=8, learning_rate=0.05,\n",
    "    random_seed=46, **cat_bin_params\n",
    ")\n",
    "\n",
    "# Train all binary models\n",
    "bin_predictions = {}\n",
    "bin_probabilities = {}\n",
    "\n",
    "for name, model in binary_models.items():\n",
    "    print(f\"  Training {name.upper()}...\", end=\" \", flush=True)\n",
    "    start = time.time()\n",
    "    model.fit(X_train_bin_scaled, y_train_bin)\n",
    "    bin_predictions[name] = model.predict(X_test_bin_scaled)\n",
    "    bin_probabilities[name] = model.predict_proba(X_test_bin_scaled)\n",
    "    acc = accuracy_score(y_test_bin, bin_predictions[name])\n",
    "    print(f\"Done! Acc: {acc*100:.2f}% (Time: {time.time()-start:.1f}s)\")\n",
    "\n",
    "print(f\"\\nâœ“ Binary models trained: {len(binary_models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7b9eb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ“Š BINARY ENSEMBLE RESULTS (BUY vs SELL)\n",
      "======================================================================\n",
      "\n",
      "Model Weights:\n",
      "  CAT2: 0.1458 (acc: 52.44%)\n",
      "  XGB3: 0.1439 (acc: 51.75%)\n",
      "  LGB1: 0.1431 (acc: 51.47%)\n",
      "  XGB1: 0.1427 (acc: 51.33%)\n",
      "  LGB2: 0.1421 (acc: 51.08%)\n",
      "  CAT1: 0.1414 (acc: 50.84%)\n",
      "  XGB2: 0.1409 (acc: 50.66%)\n",
      "\n",
      "Binary Signal Distribution:\n",
      "  BUY: 39,016 (48.6%)\n",
      "  SELL: 41,280 (51.4%)\n",
      "\n",
      "Confidence: Mean=60.8%, Min=50.0%, Max=99.0%\n"
     ]
    }
   ],
   "source": [
    "# Binary Ensemble\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ“Š BINARY ENSEMBLE RESULTS (BUY vs SELL)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate weights\n",
    "bin_accuracies = {name: accuracy_score(y_test_bin, bin_predictions[name]) for name in binary_models.keys()}\n",
    "total_acc = sum(bin_accuracies.values())\n",
    "bin_weights = {name: acc / total_acc for name, acc in bin_accuracies.items()}\n",
    "\n",
    "print(\"\\nModel Weights:\")\n",
    "for name, w in sorted(bin_weights.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {name.upper()}: {w:.4f} (acc: {bin_accuracies[name]*100:.2f}%)\")\n",
    "\n",
    "# Weighted probability\n",
    "bin_final_proba = np.zeros_like(bin_probabilities['xgb1'])\n",
    "for name, w in bin_weights.items():\n",
    "    bin_final_proba += w * bin_probabilities[name]\n",
    "\n",
    "bin_buy_prob = bin_final_proba[:, 1] * 100  # BUY probability\n",
    "bin_sell_prob = bin_final_proba[:, 0] * 100  # SELL probability\n",
    "\n",
    "# Model agreement\n",
    "bin_all_preds = np.array([bin_predictions[name].flatten() for name in binary_models.keys()])\n",
    "bin_buy_votes = np.sum(bin_all_preds == 1, axis=0)\n",
    "n_bin_models = len(binary_models)\n",
    "\n",
    "# Calculate confidence with agreement bonus\n",
    "def calc_binary_confidence(buy_p, buy_v, n_models):\n",
    "    \"\"\"BUY probability with agreement bonus\"\"\"\n",
    "    if buy_p >= 50:\n",
    "        signal = 'BUY'\n",
    "        base_conf = buy_p\n",
    "        votes = buy_v\n",
    "    else:\n",
    "        signal = 'SELL'\n",
    "        base_conf = 100 - buy_p\n",
    "        votes = n_models - buy_v\n",
    "    \n",
    "    agreement = votes / n_models\n",
    "    if agreement >= 0.85:\n",
    "        bonus = 7\n",
    "    elif agreement >= 0.7:\n",
    "        bonus = 4\n",
    "    elif agreement >= 0.5:\n",
    "        bonus = 2\n",
    "    else:\n",
    "        bonus = 0\n",
    "    \n",
    "    return signal, min(100, base_conf + bonus), votes\n",
    "\n",
    "# Apply\n",
    "bin_signals = []\n",
    "bin_confidences = []\n",
    "bin_votes = []\n",
    "\n",
    "for i in range(len(y_test_bin)):\n",
    "    sig, conf, votes = calc_binary_confidence(bin_buy_prob[i], bin_buy_votes[i], n_bin_models)\n",
    "    bin_signals.append(sig)\n",
    "    bin_confidences.append(conf)\n",
    "    bin_votes.append(votes)\n",
    "\n",
    "bin_signals = np.array(bin_signals)\n",
    "bin_confidences = np.array(bin_confidences)\n",
    "\n",
    "print(f\"\\nBinary Signal Distribution:\")\n",
    "print(f\"  BUY: {(bin_signals == 'BUY').sum():,} ({(bin_signals == 'BUY').mean()*100:.1f}%)\")\n",
    "print(f\"  SELL: {(bin_signals == 'SELL').sum():,} ({(bin_signals == 'SELL').mean()*100:.1f}%)\")\n",
    "print(f\"\\nConfidence: Mean={bin_confidences.mean():.1f}%, Min={bin_confidences.min():.1f}%, Max={bin_confidences.max():.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ec96acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ“ˆ BINARY CONFIDENCE THRESHOLD ANALYSIS\n",
      "======================================================================\n",
      "Threshold  Signals Overall   BUY BUY Acc  SELL SELL Acc\n",
      "     50%+    80296   51.5% 39016   53.8% 41280    49.4%\n",
      "     60%+    45254   52.0% 22346   54.4% 22908    49.7%\n",
      "     70%+     5046   53.3%  2386   57.3%  2660    49.7%\n",
      "     75%+     1612   54.0%   692   57.7%   920    51.2%\n",
      "     80%+      602   57.1%   263   63.9%   339    51.9%\n",
      "     85%+      204   59.8%    65   84.6%   139    48.2%\n",
      "     90%+       71   85.9%    20  100.0%    51    80.4%\n",
      "     95%+       11  100.0%     0    0.0%    11   100.0%\n"
     ]
    }
   ],
   "source": [
    "# Binary Confidence Threshold Analysis\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ“ˆ BINARY CONFIDENCE THRESHOLD ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# y_test_bin: 1=BUY, 0=SELL\n",
    "# bin_signals: 'BUY' or 'SELL'\n",
    "bin_pred_numeric = (bin_signals == 'BUY').astype(int)\n",
    "\n",
    "thresholds = [50, 60, 70, 75, 80, 85, 90, 95]\n",
    "bin_results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    mask = bin_confidences >= thresh\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    # Overall accuracy\n",
    "    acc = accuracy_score(y_test_bin[mask], bin_pred_numeric[mask])\n",
    "    \n",
    "    # BUY accuracy\n",
    "    buy_mask = mask & (bin_signals == 'BUY')\n",
    "    if buy_mask.sum() > 0:\n",
    "        buy_correct = (y_test_bin[buy_mask] == 1).sum()\n",
    "        buy_acc = buy_correct / buy_mask.sum() * 100\n",
    "    else:\n",
    "        buy_acc = 0\n",
    "    \n",
    "    # SELL accuracy\n",
    "    sell_mask = mask & (bin_signals == 'SELL')\n",
    "    if sell_mask.sum() > 0:\n",
    "        sell_correct = (y_test_bin[sell_mask] == 0).sum()\n",
    "        sell_acc = sell_correct / sell_mask.sum() * 100\n",
    "    else:\n",
    "        sell_acc = 0\n",
    "    \n",
    "    bin_results.append({\n",
    "        'Threshold': f\"{thresh}%+\",\n",
    "        'Signals': mask.sum(),\n",
    "        'Overall': f\"{acc*100:.1f}%\",\n",
    "        'BUY': buy_mask.sum(),\n",
    "        'BUY Acc': f\"{buy_acc:.1f}%\",\n",
    "        'SELL': sell_mask.sum(),\n",
    "        'SELL Acc': f\"{sell_acc:.1f}%\"\n",
    "    })\n",
    "\n",
    "bin_thresh_df = pd.DataFrame(bin_results)\n",
    "print(bin_thresh_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1c84070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ“ˆ BINARY BACKTEST RESULTS\n",
      "======================================================================\n",
      "Thresh  Signals   BUY  SELL Win Rate BUY WR SELL WR    Pips   PF\n",
      "  60%+    45254 22346 22908    52.0%  54.4%   49.7% +73,438  1.8\n",
      "  70%+     5046  2386  2660    53.3%  57.3%   49.7% +12,527  1.9\n",
      "  75%+     1612   692   920    54.0%  57.7%   51.2%  +5,802  2.0\n",
      "  80%+      602   263   339    57.1%  63.9%   51.9%  +3,178  2.2\n",
      "  85%+      204    65   139    59.8%  84.6%   48.2%  +1,614  2.5\n",
      "  90%+       71    20    51    85.9% 100.0%   80.4%  +1,369 10.1\n",
      "  95%+       11     0    11   100.0%   0.0%  100.0%    +445    âˆž\n"
     ]
    }
   ],
   "source": [
    "# Binary Backtest\n",
    "def backtest_binary_v11(test_df_bin, signals, confidences, y_test, threshold=75, sl_atr_mult=1.5, tp_atr_mult=2.5):\n",
    "    \"\"\"\n",
    "    Backtest for binary BUY/SELL predictions\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'total': 0, 'wins': 0, 'losses': 0,\n",
    "        'buy_signals': 0, 'buy_wins': 0,\n",
    "        'sell_signals': 0, 'sell_wins': 0,\n",
    "        'total_pips': 0, 'buy_pips': 0, 'sell_pips': 0\n",
    "    }\n",
    "    \n",
    "    test_clean = test_df_bin.reset_index(drop=True)\n",
    "    atr_values = test_clean['atr'].values if 'atr' in test_clean.columns else np.full(len(test_clean), 0.0008)\n",
    "    \n",
    "    for i in range(len(signals)):\n",
    "        if confidences[i] < threshold:\n",
    "            continue\n",
    "        \n",
    "        results['total'] += 1\n",
    "        actual_buy = (y_test[i] == 1)\n",
    "        predicted_buy = (signals[i] == 'BUY')\n",
    "        \n",
    "        atr = atr_values[i] if i < len(atr_values) else 0.0008\n",
    "        tp_pips = atr * tp_atr_mult * 10000\n",
    "        sl_pips = atr * sl_atr_mult * 10000\n",
    "        \n",
    "        if predicted_buy:\n",
    "            results['buy_signals'] += 1\n",
    "            if actual_buy:\n",
    "                results['wins'] += 1\n",
    "                results['buy_wins'] += 1\n",
    "                results['total_pips'] += tp_pips\n",
    "                results['buy_pips'] += tp_pips\n",
    "            else:\n",
    "                results['losses'] += 1\n",
    "                results['total_pips'] -= sl_pips\n",
    "                results['buy_pips'] -= sl_pips\n",
    "        else:\n",
    "            results['sell_signals'] += 1\n",
    "            if not actual_buy:  # actual is SELL\n",
    "                results['wins'] += 1\n",
    "                results['sell_wins'] += 1\n",
    "                results['total_pips'] += tp_pips\n",
    "                results['sell_pips'] += tp_pips\n",
    "            else:\n",
    "                results['losses'] += 1\n",
    "                results['total_pips'] -= sl_pips\n",
    "                results['sell_pips'] -= sl_pips\n",
    "    \n",
    "    results['win_rate'] = results['wins'] / results['total'] * 100 if results['total'] > 0 else 0\n",
    "    results['buy_wr'] = results['buy_wins'] / results['buy_signals'] * 100 if results['buy_signals'] > 0 else 0\n",
    "    results['sell_wr'] = results['sell_wins'] / results['sell_signals'] * 100 if results['sell_signals'] > 0 else 0\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run backtest\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ“ˆ BINARY BACKTEST RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "bt_results = []\n",
    "for thresh in [60, 70, 75, 80, 85, 90, 95]:\n",
    "    res = backtest_binary_v11(test_binary, bin_signals, bin_confidences, y_test_bin, threshold=thresh)\n",
    "    if res['total'] > 0:\n",
    "        pf = (res['wins'] * 20) / (res['losses'] * 12 + 1) if res['losses'] > 0 else float('inf')\n",
    "        bt_results.append({\n",
    "            'Thresh': f\"{thresh}%+\",\n",
    "            'Signals': res['total'],\n",
    "            'BUY': res['buy_signals'],\n",
    "            'SELL': res['sell_signals'],\n",
    "            'Win Rate': f\"{res['win_rate']:.1f}%\",\n",
    "            'BUY WR': f\"{res['buy_wr']:.1f}%\",\n",
    "            'SELL WR': f\"{res['sell_wr']:.1f}%\",\n",
    "            'Pips': f\"{res['total_pips']:+,.0f}\",\n",
    "            'PF': f\"{pf:.1f}\" if pf != float('inf') else \"âˆž\"\n",
    "        })\n",
    "\n",
    "bt_df = pd.DataFrame(bt_results)\n",
    "print(bt_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99ce80ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ’¾ SAVING V11 BINARY MODELS\n",
      "======================================================================\n",
      "  âœ“ Saved xgb1_v11_binary.joblib\n",
      "  âœ“ Saved xgb2_v11_binary.joblib\n",
      "  âœ“ Saved xgb3_v11_binary.joblib\n",
      "  âœ“ Saved lgb1_v11_binary.joblib\n",
      "  âœ“ Saved lgb2_v11_binary.joblib\n",
      "  âœ“ Saved cat1_v11_binary.joblib\n",
      "  âœ“ Saved cat2_v11_binary.joblib\n",
      "  âœ“ Saved scaler_v11.joblib\n",
      "  âœ“ Saved feature_cols_v11.joblib\n",
      "  âœ“ Saved weights_v11.joblib\n",
      "  âœ“ Saved config_v11.joblib\n",
      "\n",
      "âœ… All V11 models saved to c:\\Users\\Acer\\Desktop\\Forex-Signal-App\\models\\signal_generator_v11\n"
     ]
    }
   ],
   "source": [
    "# Save V11 Binary Models\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ’¾ SAVING V11 BINARY MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save binary models\n",
    "for name, model in binary_models.items():\n",
    "    model_path = MODEL_DIR / f\"{name}_v11_binary.joblib\"\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"  âœ“ Saved {name}_v11_binary.joblib\")\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, MODEL_DIR / 'scaler_v11.joblib')\n",
    "print(f\"  âœ“ Saved scaler_v11.joblib\")\n",
    "\n",
    "# Save feature columns\n",
    "joblib.dump(selected_features, MODEL_DIR / 'feature_cols_v11.joblib')\n",
    "print(f\"  âœ“ Saved feature_cols_v11.joblib\")\n",
    "\n",
    "# Save weights\n",
    "joblib.dump(bin_weights, MODEL_DIR / 'weights_v11.joblib')\n",
    "print(f\"  âœ“ Saved weights_v11.joblib\")\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    'version': 'v11',\n",
    "    'model_type': 'binary',  # BUY vs SELL\n",
    "    'n_models': len(binary_models),\n",
    "    'model_names': list(binary_models.keys()),\n",
    "    'n_classes': 2,\n",
    "    'class_mapping': {0: 'SELL', 1: 'BUY'},\n",
    "    'n_features': len(selected_features),\n",
    "    'weights': bin_weights,\n",
    "    'gpu_available': GPU_AVAILABLE,\n",
    "    'labeling_params': {\n",
    "        'forward_periods': 60,\n",
    "        'min_pips': 15,\n",
    "        'ratio': 1.5\n",
    "    },\n",
    "    'performance': {\n",
    "        '90%+': {'signals': 71, 'accuracy': 85.9, 'pips': 1369, 'pf': 10.1},\n",
    "        '85%+': {'signals': 204, 'accuracy': 59.8, 'pips': 1614, 'pf': 2.5},\n",
    "        '80%+': {'signals': 602, 'accuracy': 57.1, 'pips': 3178, 'pf': 2.2}\n",
    "    }\n",
    "}\n",
    "joblib.dump(config, MODEL_DIR / 'config_v11.joblib')\n",
    "print(f\"  âœ“ Saved config_v11.joblib\")\n",
    "\n",
    "print(f\"\\nâœ… All V11 models saved to {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ef868",
   "metadata": {},
   "source": [
    "## 11. Backtest Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d850c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ“ˆ BACKTEST RESULTS\n",
      "======================================================================\n",
      "Threshold  Signals  BUY  SELL Win Rate BUY WR SELL WR   Pips\n",
      "     70%+       39    2    37    59.0%   0.0%   62.2% +662.2\n",
      "     75%+        9    0     9    66.7%   0.0%   66.7% +120.5\n",
      "Threshold  Signals  BUY  SELL Win Rate BUY WR SELL WR   Pips\n",
      "     70%+       39    2    37    59.0%   0.0%   62.2% +662.2\n",
      "     75%+        9    0     9    66.7%   0.0%   66.7% +120.5\n"
     ]
    }
   ],
   "source": [
    "def backtest_v11(test_df, signals, confidences, y_test, threshold=75, sl_atr_mult=1.5, tp_atr_mult=2.5):\n",
    "    \"\"\"\n",
    "    V11 Backtest with BUY and SELL signals\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'total_signals': 0,\n",
    "        'buy_signals': 0,\n",
    "        'sell_signals': 0,\n",
    "        'wins': 0,\n",
    "        'losses': 0,\n",
    "        'buy_wins': 0,\n",
    "        'buy_losses': 0,\n",
    "        'sell_wins': 0,\n",
    "        'sell_losses': 0,\n",
    "        'total_pips': 0,\n",
    "        'buy_pips': 0,\n",
    "        'sell_pips': 0\n",
    "    }\n",
    "    \n",
    "    test_clean = test_df.reset_index(drop=True)\n",
    "    atr_values = test_clean['atr'].values if 'atr' in test_clean.columns else np.full(len(test_clean), 0.0008)\n",
    "    \n",
    "    for i in range(len(signals)):\n",
    "        if confidences[i] < threshold:\n",
    "            continue\n",
    "        if signals[i] == 'HOLD':\n",
    "            continue\n",
    "        \n",
    "        results['total_signals'] += 1\n",
    "        \n",
    "        # Determine if prediction was correct\n",
    "        actual = y_test[i]\n",
    "        \n",
    "        if signals[i] == 'BUY':\n",
    "            results['buy_signals'] += 1\n",
    "            is_correct = (actual == 1)  # BUY was correct\n",
    "            \n",
    "            if is_correct:\n",
    "                results['wins'] += 1\n",
    "                results['buy_wins'] += 1\n",
    "                pips = atr_values[i] * tp_atr_mult * 10000\n",
    "                results['total_pips'] += pips\n",
    "                results['buy_pips'] += pips\n",
    "            else:\n",
    "                results['losses'] += 1\n",
    "                results['buy_losses'] += 1\n",
    "                pips = atr_values[i] * sl_atr_mult * 10000\n",
    "                results['total_pips'] -= pips\n",
    "                results['buy_pips'] -= pips\n",
    "        \n",
    "        elif signals[i] == 'SELL':\n",
    "            results['sell_signals'] += 1\n",
    "            is_correct = (actual == 2)  # SELL was correct\n",
    "            \n",
    "            if is_correct:\n",
    "                results['wins'] += 1\n",
    "                results['sell_wins'] += 1\n",
    "                pips = atr_values[i] * tp_atr_mult * 10000\n",
    "                results['total_pips'] += pips\n",
    "                results['sell_pips'] += pips\n",
    "            else:\n",
    "                results['losses'] += 1\n",
    "                results['sell_losses'] += 1\n",
    "                pips = atr_values[i] * sl_atr_mult * 10000\n",
    "                results['total_pips'] -= pips\n",
    "                results['sell_pips'] -= pips\n",
    "    \n",
    "    # Calculate win rates\n",
    "    results['win_rate'] = results['wins'] / results['total_signals'] * 100 if results['total_signals'] > 0 else 0\n",
    "    results['buy_win_rate'] = results['buy_wins'] / results['buy_signals'] * 100 if results['buy_signals'] > 0 else 0\n",
    "    results['sell_win_rate'] = results['sell_wins'] / results['sell_signals'] * 100 if results['sell_signals'] > 0 else 0\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run backtest for different thresholds\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ“ˆ BACKTEST RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "backtest_results = []\n",
    "for thresh in [70, 75, 80, 85, 90]:\n",
    "    res = backtest_v11(test_df, signals, confidences, y_test, threshold=thresh)\n",
    "    if res['total_signals'] > 0:\n",
    "        backtest_results.append({\n",
    "            'Threshold': f\"{thresh}%+\",\n",
    "            'Signals': res['total_signals'],\n",
    "            'BUY': res['buy_signals'],\n",
    "            'SELL': res['sell_signals'],\n",
    "            'Win Rate': f\"{res['win_rate']:.1f}%\",\n",
    "            'BUY WR': f\"{res['buy_win_rate']:.1f}%\",\n",
    "            'SELL WR': f\"{res['sell_win_rate']:.1f}%\",\n",
    "            'Pips': f\"{res['total_pips']:+.1f}\"\n",
    "        })\n",
    "\n",
    "bt_df = pd.DataFrame(backtest_results)\n",
    "print(bt_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e7a422",
   "metadata": {},
   "source": [
    "## 12. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c38682",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ’¾ SAVING V11 MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save all trained models\n",
    "for name, model in models.items():\n",
    "    model_path = MODEL_DIR / f\"{name}_v11.joblib\"\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"  âœ“ Saved {name}_v11.joblib\")\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, MODEL_DIR / 'scaler_v11.joblib')\n",
    "print(f\"  âœ“ Saved scaler_v11.joblib\")\n",
    "\n",
    "# Save feature columns\n",
    "joblib.dump(selected_features, MODEL_DIR / 'feature_cols_v11.joblib')\n",
    "print(f\"  âœ“ Saved feature_cols_v11.joblib\")\n",
    "\n",
    "# Save weights\n",
    "joblib.dump(weights, MODEL_DIR / 'weights_v11.joblib')\n",
    "print(f\"  âœ“ Saved weights_v11.joblib\")\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    'version': 'v11',\n",
    "    'n_models': len(models),\n",
    "    'model_names': list(models.keys()),\n",
    "    'n_classes': 3,\n",
    "    'class_mapping': {0: 'HOLD', 1: 'BUY', 2: 'SELL'},\n",
    "    'n_features': len(selected_features),\n",
    "    'weights': weights,\n",
    "    'gpu_available': GPU_AVAILABLE,\n",
    "    'labeling_params': {\n",
    "        'forward_periods': 60,\n",
    "        'min_pips': 15,\n",
    "        'ratio': 1.5\n",
    "    }\n",
    "}\n",
    "joblib.dump(config, MODEL_DIR / 'config_v11.joblib')\n",
    "print(f\"  âœ“ Saved config_v11.joblib\")\n",
    "\n",
    "print(f\"\\nâœ“ All models saved to {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0401a",
   "metadata": {},
   "source": [
    "## 13. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ed391",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š V11 TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "ðŸŽ¯ V11 Multi-Class Signal Generator\n",
    "\n",
    "Models Trained: {len(models)}\n",
    "  - XGBoost: 3 variants\n",
    "  - LightGBM: 2 variants\n",
    "  - CatBoost: 2 variants\n",
    "  - RandomForest: 1\n",
    "  - ExtraTrees: 1\n",
    "  - HistGradientBoosting: 1\n",
    "\n",
    "Features: {len(selected_features)}\n",
    "Classes: 3 (HOLD, BUY, SELL)\n",
    "\n",
    "Ensemble Accuracy: {ensemble_acc*100:.2f}%\n",
    "\n",
    "Best Individual Model:\n",
    "  {max(accuracies, key=accuracies.get).upper()}: {max(accuracies.values())*100:.2f}%\n",
    "\n",
    "GPU Used: {GPU_AVAILABLE}\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nâœ… V11 Training Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
