{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a818f128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä FOREX SIGNAL MODELS COMPARISON\n",
      "   V2, V3, V4, V5, V6, V7, V8, V9, V10\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä FOREX SIGNAL MODELS COMPARISON\")\n",
    "print(\"   V2, V3, V4, V5, V6, V7, V8, V9, V10\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f56de",
   "metadata": {},
   "source": [
    "## 1. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34df5c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: 296,778 rows\n",
      "Period: 2024-12-31 16:00:00+00:00 to 2025-10-17 06:11:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_df_raw = pd.read_csv(DATA_DIR / 'EUR_USD_test.csv')\n",
    "if 'timestamp' in test_df_raw.columns:\n",
    "    test_df_raw.rename(columns={'timestamp': 'time'}, inplace=True)\n",
    "test_df_raw['time'] = pd.to_datetime(test_df_raw['time'])\n",
    "\n",
    "print(f\"Test data: {len(test_df_raw):,} rows\")\n",
    "print(f\"Period: {test_df_raw['time'].min()} to {test_df_raw['time'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4aec9",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b516724b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Feature functions defined\n"
     ]
    }
   ],
   "source": [
    "def add_features_v2_v3(df):\n",
    "    \"\"\"V2, V3 Features\"\"\"\n",
    "    df = df.copy()\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['day_of_week'] = df['time'].dt.dayofweek\n",
    "    \n",
    "    for p in [5, 10, 20, 50]:\n",
    "        df[f'sma_{p}'] = df['close'].rolling(p).mean()\n",
    "        df[f'ema_{p}'] = df['close'].ewm(span=p).mean()\n",
    "    \n",
    "    delta = df['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    df['rsi'] = 100 - (100 / (1 + gain / (loss + 1e-10)))\n",
    "    \n",
    "    ema12 = df['close'].ewm(span=12).mean()\n",
    "    ema26 = df['close'].ewm(span=26).mean()\n",
    "    df['macd'] = ema12 - ema26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    df['bb_mid'] = df['close'].rolling(20).mean()\n",
    "    df['bb_std'] = df['close'].rolling(20).std()\n",
    "    df['bb_upper'] = df['bb_mid'] + 2 * df['bb_std']\n",
    "    df['bb_lower'] = df['bb_mid'] - 2 * df['bb_std']\n",
    "    \n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['volatility'] = df['returns'].rolling(20).std()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_features_v6_v7_v8_v9_v10(df):\n",
    "    \"\"\"V6, V7, V8, V9, V10 Features (extended)\"\"\"\n",
    "    df = df.copy()\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['day_of_week'] = df['time'].dt.dayofweek\n",
    "    df['is_london'] = ((df['hour'] >= 8) & (df['hour'] < 16)).astype(int)\n",
    "    df['is_ny'] = ((df['hour'] >= 13) & (df['hour'] < 21)).astype(int)\n",
    "    df['is_overlap'] = ((df['hour'] >= 13) & (df['hour'] < 16)).astype(int)\n",
    "    \n",
    "    for p in [5, 10, 20, 50, 200]:\n",
    "        df[f'sma_{p}'] = df['close'].rolling(p).mean()\n",
    "        df[f'ema_{p}'] = df['close'].ewm(span=p, adjust=False).mean()\n",
    "    \n",
    "    delta = df['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    df['rsi'] = 100 - (100 / (1 + gain / (loss + 1e-10)))\n",
    "    \n",
    "    ema12 = df['close'].ewm(span=12).mean()\n",
    "    ema26 = df['close'].ewm(span=26).mean()\n",
    "    df['macd'] = ema12 - ema26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    df['macd_momentum'] = df['macd_hist'] - df['macd_hist'].shift(3)\n",
    "    \n",
    "    df['bb_mid'] = df['close'].rolling(20).mean()\n",
    "    df['bb_std'] = df['close'].rolling(20).std()\n",
    "    df['bb_upper'] = df['bb_mid'] + 2 * df['bb_std']\n",
    "    df['bb_lower'] = df['bb_mid'] - 2 * df['bb_std']\n",
    "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / (df['bb_mid'] + 1e-10)\n",
    "    \n",
    "    # ADX\n",
    "    df['tr0'] = abs(df['high'] - df['low'])\n",
    "    df['tr1'] = abs(df['high'] - df['close'].shift())\n",
    "    df['tr2'] = abs(df['low'] - df['close'].shift())\n",
    "    df['tr'] = df[['tr0', 'tr1', 'tr2']].max(axis=1)\n",
    "    df['up_move'] = df['high'] - df['high'].shift()\n",
    "    df['down_move'] = df['low'].shift() - df['low']\n",
    "    df['plus_dm'] = np.where((df['up_move'] > df['down_move']) & (df['up_move'] > 0), df['up_move'], 0)\n",
    "    df['minus_dm'] = np.where((df['down_move'] > df['up_move']) & (df['down_move'] > 0), df['down_move'], 0)\n",
    "    period = 14\n",
    "    df['atr'] = df['tr'].rolling(period).mean()\n",
    "    df['plus_di'] = 100 * (df['plus_dm'].rolling(period).mean() / (df['atr'] + 1e-10))\n",
    "    df['minus_di'] = 100 * (df['minus_dm'].rolling(period).mean() / (df['atr'] + 1e-10))\n",
    "    df['dx'] = 100 * abs(df['plus_di'] - df['minus_di']) / (df['plus_di'] + df['minus_di'] + 1e-10)\n",
    "    df['adx'] = df['dx'].rolling(period).mean()\n",
    "    \n",
    "    # CCI - Optimized (avoid slow apply)\n",
    "    tp = (df['high'] + df['low'] + df['close']) / 3\n",
    "    sma_tp = tp.rolling(20).mean()\n",
    "    # Use std as approximation for MAD (faster)\n",
    "    std_tp = tp.rolling(20).std()\n",
    "    df['cci'] = (tp - sma_tp) / (0.015 * std_tp + 1e-10)\n",
    "    \n",
    "    # Williams %R\n",
    "    hh = df['high'].rolling(14).max()\n",
    "    ll = df['low'].rolling(14).min()\n",
    "    df['williams_r'] = -100 * (hh - df['close']) / (hh - ll + 1e-10)\n",
    "    \n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['volatility'] = df['returns'].rolling(20).std() * 100\n",
    "    \n",
    "    # Composite features\n",
    "    df['rsi_x_adx'] = df['rsi'] * df['adx'] / 100\n",
    "    df['momentum_score'] = (\n",
    "        (df['rsi'] > 50).astype(int) + \n",
    "        (df['macd'] > df['macd_signal']).astype(int) + \n",
    "        (df['plus_di'] > df['minus_di']).astype(int)\n",
    "    )\n",
    "    df['price_position'] = (df['close'] - df['sma_50']) / (df['atr'] + 1e-10)\n",
    "    df['trend_score'] = (\n",
    "        (df['close'] > df['sma_20']).astype(int) +\n",
    "        (df['sma_20'] > df['sma_50']).astype(int) +\n",
    "        (df['sma_50'] > df['sma_200']).astype(int) +\n",
    "        (df['adx'] > 25).astype(int)\n",
    "    )\n",
    "    df['rsi_zone'] = pd.cut(df['rsi'], bins=[0, 30, 45, 55, 70, 100], labels=[0, 1, 2, 3, 4]).astype(float)\n",
    "    df['close_vs_high'] = (df['high'].rolling(20).max() - df['close']) / (df['atr'] + 1e-10)\n",
    "    df['close_vs_low'] = (df['close'] - df['low'].rolling(20).min()) / (df['atr'] + 1e-10)\n",
    "    df['volume_ratio'] = 1.0\n",
    "    \n",
    "    # Cleanup\n",
    "    drop_cols = ['tr0', 'tr1', 'tr2', 'tr', 'up_move', 'down_move', 'plus_dm', 'minus_dm']\n",
    "    df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_features_v10_extended(df):\n",
    "    \"\"\"V10 Extended Features\"\"\"\n",
    "    df = add_features_v6_v7_v8_v9_v10(df)\n",
    "    \n",
    "    # V10 specific features\n",
    "    df['trend_strength'] = (\n",
    "        (df['close'] > df['ema_5']).astype(int) +\n",
    "        (df['ema_5'] > df['ema_10']).astype(int) +\n",
    "        (df['ema_10'] > df['ema_20']).astype(int) +\n",
    "        (df['ema_20'] > df['ema_50']).astype(int) +\n",
    "        (df['adx'] > 20).astype(int)\n",
    "    )\n",
    "    \n",
    "    df['momentum_alignment'] = (\n",
    "        (df['rsi'] > 55).astype(int) +\n",
    "        (df['macd_hist'] > 0).astype(int) +\n",
    "        (df['cci'] > 50).astype(int) +\n",
    "        (df['williams_r'] > -30).astype(int)\n",
    "    )\n",
    "    \n",
    "    df['volatility_sma'] = df['volatility'].rolling(50).mean()\n",
    "    df['volatility_state'] = np.where(\n",
    "        df['volatility'] > df['volatility_sma'] * 1.5, 2,\n",
    "        np.where(df['volatility'] < df['volatility_sma'] * 0.5, 0, 1)\n",
    "    )\n",
    "    \n",
    "    df['body'] = df['close'] - df['open']\n",
    "    df['upper_wick'] = df['high'] - df[['open', 'close']].max(axis=1)\n",
    "    df['lower_wick'] = df[['open', 'close']].min(axis=1) - df['low']\n",
    "    df['body_ratio'] = abs(df['body']) / (df['high'] - df['low'] + 1e-10)\n",
    "    df['is_bullish'] = (df['close'] > df['open']).astype(int)\n",
    "    df['bullish_streak'] = df['is_bullish'].rolling(5).sum()\n",
    "    \n",
    "    df['dist_to_high20'] = (df['high'].rolling(20).max() - df['close']) / (df['atr'] + 1e-10)\n",
    "    df['dist_to_low20'] = (df['close'] - df['low'].rolling(20).min()) / (df['atr'] + 1e-10)\n",
    "    \n",
    "    df['rsi_5'] = df['rsi'].rolling(5).mean()\n",
    "    df['rsi_20'] = df['rsi'].rolling(20).mean()\n",
    "    df['rsi_trend'] = df['rsi_5'] - df['rsi_20']\n",
    "    \n",
    "    df['above_bb_upper'] = (df['close'] > df['bb_upper']).astype(int)\n",
    "    df['below_bb_lower'] = (df['close'] < df['bb_lower']).astype(int)\n",
    "    df['bb_breakout'] = df['above_bb_upper'] - df['below_bb_lower']\n",
    "    \n",
    "    df['price_change_5'] = (df['close'] - df['close'].shift(5)) / (df['atr'] + 1e-10)\n",
    "    df['price_change_10'] = (df['close'] - df['close'].shift(10)) / (df['atr'] + 1e-10)\n",
    "    df['price_change_20'] = (df['close'] - df['close'].shift(20)) / (df['atr'] + 1e-10)\n",
    "    \n",
    "    df['session_quality'] = df['is_london'].astype(int) + df['is_ny'].astype(int) + df['is_overlap'].astype(int) * 2\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_labels(df, forward_periods=60, min_pips=15, ratio=1.5):\n",
    "    \"\"\"Create BUY/SELL labels\"\"\"\n",
    "    df = df.copy()\n",
    "    min_move = min_pips * 0.0001\n",
    "    \n",
    "    df['future_max'] = df['high'].rolling(forward_periods).max().shift(-forward_periods)\n",
    "    df['future_min'] = df['low'].rolling(forward_periods).min().shift(-forward_periods)\n",
    "    \n",
    "    df['up_move'] = df['future_max'] - df['close']\n",
    "    df['down_move'] = df['close'] - df['future_min']\n",
    "    \n",
    "    conditions = [\n",
    "        (df['up_move'] >= min_move) & (df['up_move'] > df['down_move'] * ratio),\n",
    "        (df['down_move'] >= min_move) & (df['down_move'] > df['up_move'] * ratio)\n",
    "    ]\n",
    "    choices = [1, 0]\n",
    "    df['signal'] = np.select(conditions, choices, default=-1)\n",
    "    \n",
    "    df.drop(['future_max', 'future_min', 'up_move', 'down_move'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "print(\"‚úì Feature functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c57cce2",
   "metadata": {},
   "source": [
    "## 3. Evaluate Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7db92637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "# Store results\n",
    "all_results = {}\n",
    "\n",
    "def evaluate_model(version, test_df, feature_func, model_names, ensemble_type='weighted'):\n",
    "    \"\"\"\n",
    "    Evaluate a model version\n",
    "    \"\"\"\n",
    "    model_dir = MODELS_DIR / f'signal_generator_{version}'\n",
    "    \n",
    "    if not model_dir.exists():\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load models and preprocessing\n",
    "        scaler = joblib.load(model_dir / f'scaler_{version}.joblib')\n",
    "        feature_cols = joblib.load(model_dir / f'feature_cols_{version}.joblib')\n",
    "        \n",
    "        # Prepare test data\n",
    "        test_prepared = feature_func(test_df.copy())\n",
    "        test_prepared = create_labels(test_prepared)\n",
    "        test_binary = test_prepared[test_prepared['signal'] != -1].copy()\n",
    "        \n",
    "        # Add missing columns\n",
    "        for c in feature_cols:\n",
    "            if c not in test_binary.columns:\n",
    "                test_binary[c] = 0\n",
    "        \n",
    "        test_clean = test_binary.dropna(subset=feature_cols).copy()\n",
    "        X_test = test_clean[feature_cols].values\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        y_test = test_clean['signal'].values\n",
    "        \n",
    "        # Load and run models\n",
    "        models = {}\n",
    "        predictions = {}\n",
    "        probabilities = {}\n",
    "        \n",
    "        for name in model_names:\n",
    "            model_path = model_dir / f'{name}_{version}.joblib'\n",
    "            if model_path.exists():\n",
    "                models[name] = joblib.load(model_path)\n",
    "                predictions[name] = models[name].predict(X_test_scaled)\n",
    "                probabilities[name] = models[name].predict_proba(X_test_scaled)\n",
    "        \n",
    "        if not models:\n",
    "            return None\n",
    "        \n",
    "        # Load weights if available\n",
    "        weights_path = model_dir / f'weights_{version}.joblib'\n",
    "        if weights_path.exists():\n",
    "            weights = joblib.load(weights_path)\n",
    "        else:\n",
    "            # Equal weights\n",
    "            weights = {name: 1/len(models) for name in models.keys()}\n",
    "        \n",
    "        # Ensemble predictions\n",
    "        first_model = list(models.keys())[0]\n",
    "        final_proba = np.zeros_like(probabilities[first_model])\n",
    "        for name in models.keys():\n",
    "            w = weights.get(name, 1/len(models))\n",
    "            final_proba += w * probabilities[name]\n",
    "        \n",
    "        buy_prob = final_proba[:, 1] * 100\n",
    "        \n",
    "        # Agreement bonus\n",
    "        all_preds = np.array([predictions[name] for name in models.keys()])\n",
    "        buy_votes = np.sum(all_preds == 1, axis=0)\n",
    "        n_models = len(models)\n",
    "        \n",
    "        all_agree = buy_votes == n_models\n",
    "        most_agree = buy_votes >= (n_models - 1)\n",
    "        \n",
    "        confidence = buy_prob.copy()\n",
    "        confidence[all_agree] = np.minimum(confidence[all_agree] + 5, 100)\n",
    "        confidence[most_agree & ~all_agree] = np.minimum(confidence[most_agree & ~all_agree] + 2, 100)\n",
    "        \n",
    "        # Calculate results at different thresholds\n",
    "        results = {\n",
    "            'version': version,\n",
    "            'n_models': len(models),\n",
    "            'total_signals': len(y_test),\n",
    "            'thresholds': {}\n",
    "        }\n",
    "        \n",
    "        for thresh in [50, 60, 70, 75, 80, 85, 90]:\n",
    "            mask = confidence >= thresh\n",
    "            if mask.sum() > 0:\n",
    "                signals = mask.sum()\n",
    "                correct = y_test[mask].sum()\n",
    "                acc = correct / signals * 100\n",
    "                results['thresholds'][thresh] = {\n",
    "                    'signals': int(signals),\n",
    "                    'correct': int(correct),\n",
    "                    'accuracy': acc\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error evaluating {version}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úì Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599fb5f8",
   "metadata": {},
   "source": [
    "## 4. Evaluate V2, V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "434d160f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Evaluating V2, V3...\n",
      "======================================================================\n",
      "‚úì V2: 3 models\n",
      "‚úì V2: 3 models\n",
      "‚úì V3: 3 models\n",
      "‚úì V3: 3 models\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Evaluating V2, V3...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# V2\n",
    "result = evaluate_model('v2', test_df_raw, add_features_v2_v3, ['xgboost', 'lightgbm', 'rf'])\n",
    "if result:\n",
    "    all_results['V2'] = result\n",
    "    print(f\"‚úì V2: {result['n_models']} models\")\n",
    "\n",
    "# V3\n",
    "result = evaluate_model('v3', test_df_raw, add_features_v2_v3, ['xgboost', 'lightgbm', 'rf'])\n",
    "if result:\n",
    "    all_results['V3'] = result\n",
    "    print(f\"‚úì V3: {result['n_models']} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a02e911",
   "metadata": {},
   "source": [
    "## 5. Evaluate V4, V5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc7b2db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Evaluating V4, V5...\n",
      "======================================================================\n",
      "‚úì V4: Voting ensemble (ATR-based labels)\n",
      "‚úì V5: 3 models\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Evaluating V4, V5...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# V4 - Stacking/Voting structure with ATR-based labels\n",
    "def add_features_v4(df):\n",
    "    \"\"\"V4 Features with ATR\"\"\"\n",
    "    df = df.copy()\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['day_of_week'] = df['time'].dt.dayofweek\n",
    "    df['is_london'] = ((df['hour'] >= 7) & (df['hour'] < 16)).astype(int)\n",
    "    df['is_ny'] = ((df['hour'] >= 12) & (df['hour'] < 21)).astype(int)\n",
    "    df['is_overlap'] = ((df['hour'] >= 12) & (df['hour'] < 16)).astype(int)\n",
    "    \n",
    "    for period in [10, 20, 50, 200]:\n",
    "        df[f'sma_{period}'] = df['close'].rolling(period).mean()\n",
    "        df[f'ema_{period}'] = df['close'].ewm(span=period, adjust=False).mean()\n",
    "    \n",
    "    delta = df['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    df['rsi_14'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    ema12 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    ema26 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['macd'] = ema12 - ema26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = abs(df['high'] - df['close'].shift())\n",
    "    low_close = abs(df['low'] - df['close'].shift())\n",
    "    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['atr_14'] = tr.rolling(14).mean()\n",
    "    \n",
    "    df['bb_middle'] = df['close'].rolling(20).mean()\n",
    "    bb_std = df['close'].rolling(20).std()\n",
    "    df['bb_upper'] = df['bb_middle'] + 2 * bb_std\n",
    "    df['bb_lower'] = df['bb_middle'] - 2 * bb_std\n",
    "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']\n",
    "    \n",
    "    df['dist_sma50'] = (df['close'] - df['sma_50']) / df['sma_50'] * 100\n",
    "    df['dist_sma200'] = (df['close'] - df['sma_200']) / df['sma_200'] * 100\n",
    "    \n",
    "    df['atr_50'] = tr.rolling(50).mean()\n",
    "    df['volatility_ratio'] = df['atr_14'] / (df['atr_50'] + 1e-10)\n",
    "    \n",
    "    df['mom_10'] = df['close'].pct_change(10)\n",
    "    df['mom_30'] = df['close'].pct_change(30)\n",
    "    \n",
    "    for col in ['rsi_14', 'macd', 'mom_10', 'volatility_ratio']:\n",
    "        df[f'{col}_lag1'] = df[col].shift(1)\n",
    "        df[f'{col}_lag2'] = df[col].shift(2)\n",
    "        df[f'{col}_lag3'] = df[col].shift(3)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_labels_v4(df, forward_periods=60):\n",
    "    \"\"\"V4 ATR-based labels\"\"\"\n",
    "    df = df.copy()\n",
    "    closes = df['close'].values\n",
    "    highs = df['high'].values\n",
    "    lows = df['low'].values\n",
    "    atrs = df['atr_14'].values\n",
    "    n = len(df)\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        if i + forward_periods >= n or np.isnan(atrs[i]):\n",
    "            labels.append(0)\n",
    "            continue\n",
    "        entry = closes[i]\n",
    "        atr = atrs[i]\n",
    "        tp_dist = atr * 1.5\n",
    "        sl_dist = atr * 1.0\n",
    "        tp_price = entry + tp_dist\n",
    "        sl_price = entry - sl_dist\n",
    "        future_highs = highs[i+1 : i+forward_periods+1]\n",
    "        future_lows = lows[i+1 : i+forward_periods+1]\n",
    "        sl_hit = np.where(future_lows <= sl_price)[0]\n",
    "        first_sl = sl_hit[0] if len(sl_hit) > 0 else forward_periods + 1\n",
    "        tp_hit = np.where(future_highs >= tp_price)[0]\n",
    "        first_tp = tp_hit[0] if len(tp_hit) > 0 else forward_periods + 1\n",
    "        labels.append(1 if first_tp < first_sl else 0)\n",
    "    \n",
    "    df['target'] = labels\n",
    "    return df\n",
    "\n",
    "v4_dir = MODELS_DIR / 'signal_generator_v4'\n",
    "if v4_dir.exists():\n",
    "    try:\n",
    "        # Load V4 models\n",
    "        v4_scaler = joblib.load(v4_dir / 'scaler_v4.joblib')\n",
    "        v4_features = joblib.load(v4_dir / 'features_v4.joblib')  # Selected 20 features\n",
    "        v4_selector = joblib.load(v4_dir / 'selector_v4.joblib')\n",
    "        v4_stacking = joblib.load(v4_dir / 'stacking_v4.joblib')\n",
    "        v4_voting = joblib.load(v4_dir / 'voting_v4.joblib')\n",
    "        \n",
    "        # Prepare test data with V4 features\n",
    "        test_v4 = add_features_v4(test_df_raw.copy())\n",
    "        test_v4 = create_labels_v4(test_v4)\n",
    "        \n",
    "        # Get ALL feature columns (not just selected) - same as training\n",
    "        exclude_cols = ['time', 'target', 'open', 'high', 'low', 'close', 'tick_volume', 'volume']\n",
    "        all_feature_cols = [c for c in test_v4.columns if c not in exclude_cols]\n",
    "        \n",
    "        test_v4_clean = test_v4.dropna().copy()\n",
    "        \n",
    "        # Scale ALL features first (scaler expects 40 features)\n",
    "        X_test_v4_all = test_v4_clean[all_feature_cols].values\n",
    "        X_test_v4_scaled = v4_scaler.transform(X_test_v4_all)\n",
    "        \n",
    "        # Then select features (selector reduces from 40 to 20)\n",
    "        X_test_v4_selected = v4_selector.transform(X_test_v4_scaled)\n",
    "        y_test_v4 = test_v4_clean['target'].values\n",
    "        \n",
    "        # Predict with voting model (more reliable than stacking)\n",
    "        v4_proba = v4_voting.predict_proba(X_test_v4_selected)\n",
    "        v4_confidence = v4_proba[:, 1] * 100\n",
    "        \n",
    "        # Calculate results\n",
    "        v4_results = {'version': 'v4', 'n_models': 3, 'total_signals': len(y_test_v4), 'thresholds': {}}\n",
    "        for thresh in [50, 60, 70, 75, 80, 85, 90]:\n",
    "            mask = v4_confidence >= thresh\n",
    "            if mask.sum() > 0:\n",
    "                signals = mask.sum()\n",
    "                correct = y_test_v4[mask].sum()\n",
    "                v4_results['thresholds'][thresh] = {\n",
    "                    'signals': int(signals),\n",
    "                    'correct': int(correct),\n",
    "                    'accuracy': correct / signals * 100\n",
    "                }\n",
    "        \n",
    "        all_results['V4'] = v4_results\n",
    "        print(f\"‚úì V4: Voting ensemble (ATR-based labels)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  V4 error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# V5\n",
    "result = evaluate_model('v5', test_df_raw, add_features_v2_v3, ['xgboost', 'lightgbm', 'rf'])\n",
    "if result:\n",
    "    all_results['V5'] = result\n",
    "    print(f\"‚úì V5: {result['n_models']} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb836e0",
   "metadata": {},
   "source": [
    "## 6. Evaluate V6, V7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79f71da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Evaluating V6, V7...\n",
      "======================================================================\n",
      "‚úì V6: 5 models\n",
      "‚úì V6: 5 models\n",
      "‚úì V7: 5 models\n",
      "‚úì V7: 5 models\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Evaluating V6, V7...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# V6 - Binary models\n",
    "v6_dir = MODELS_DIR / 'signal_generator_v6'\n",
    "if v6_dir.exists():\n",
    "    try:\n",
    "        scaler = joblib.load(v6_dir / 'scaler_v6_bin.joblib')\n",
    "        feature_cols = joblib.load(v6_dir / 'feature_cols_v6.joblib')\n",
    "        weights = joblib.load(v6_dir / 'weights_v6.joblib')\n",
    "        \n",
    "        test_prepared = add_features_v6_v7_v8_v9_v10(test_df_raw.copy())\n",
    "        test_prepared = create_labels(test_prepared)\n",
    "        test_binary = test_prepared[test_prepared['signal'] != -1].copy()\n",
    "        \n",
    "        for c in feature_cols:\n",
    "            if c not in test_binary.columns:\n",
    "                test_binary[c] = 0\n",
    "        \n",
    "        test_clean = test_binary.dropna(subset=feature_cols).copy()\n",
    "        X_test = test_clean[feature_cols].values\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        y_test = test_clean['signal'].values\n",
    "        \n",
    "        v6_models = {}\n",
    "        v6_preds = {}\n",
    "        v6_proba = {}\n",
    "        for name in ['xgb1', 'xgb2', 'lgb1', 'lgb2', 'cat']:\n",
    "            path = v6_dir / f'{name}_v6_bin.joblib'\n",
    "            if path.exists():\n",
    "                v6_models[name] = joblib.load(path)\n",
    "                v6_preds[name] = v6_models[name].predict(X_test_scaled)\n",
    "                v6_proba[name] = v6_models[name].predict_proba(X_test_scaled)\n",
    "        \n",
    "        if v6_models:\n",
    "            first = list(v6_models.keys())[0]\n",
    "            final_proba = np.zeros_like(v6_proba[first])\n",
    "            for name in v6_models.keys():\n",
    "                w = weights.get(name, 1/len(v6_models))\n",
    "                final_proba += w * v6_proba[name]\n",
    "            \n",
    "            buy_prob = final_proba[:, 1] * 100\n",
    "            all_preds = np.array([v6_preds[name] for name in v6_models.keys()])\n",
    "            buy_votes = np.sum(all_preds == 1, axis=0)\n",
    "            all_agree = buy_votes == len(v6_models)\n",
    "            most_agree = buy_votes >= (len(v6_models) - 1)\n",
    "            \n",
    "            confidence = buy_prob.copy()\n",
    "            confidence[all_agree] = np.minimum(confidence[all_agree] + 5, 100)\n",
    "            confidence[most_agree & ~all_agree] = np.minimum(confidence[most_agree & ~all_agree] + 2, 100)\n",
    "            \n",
    "            results = {'version': 'v6', 'n_models': len(v6_models), 'total_signals': len(y_test), 'thresholds': {}}\n",
    "            for thresh in [50, 60, 70, 75, 80, 85, 90]:\n",
    "                mask = confidence >= thresh\n",
    "                if mask.sum() > 0:\n",
    "                    signals = mask.sum()\n",
    "                    correct = y_test[mask].sum()\n",
    "                    results['thresholds'][thresh] = {'signals': int(signals), 'correct': int(correct), 'accuracy': correct/signals*100}\n",
    "            \n",
    "            all_results['V6'] = results\n",
    "            print(f\"‚úì V6: {len(v6_models)} models\")\n",
    "    except Exception as e:\n",
    "        print(f\"  V6 error: {e}\")\n",
    "\n",
    "# V7\n",
    "result = evaluate_model('v7', test_df_raw, add_features_v6_v7_v8_v9_v10, ['xgb1', 'xgb2', 'lgb1', 'lgb2', 'cat'])\n",
    "if result:\n",
    "    all_results['V7'] = result\n",
    "    print(f\"‚úì V7: {result['n_models']} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb26569",
   "metadata": {},
   "source": [
    "## 7. Evaluate V8, V9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8c50d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Evaluating V8, V9...\n",
      "======================================================================\n",
      "‚úì V8: 5 models\n",
      "‚úì V8: 5 models\n",
      "‚úì V9: 5 models\n",
      "‚úì V9: 5 models\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Evaluating V8, V9...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# V8\n",
    "result = evaluate_model('v8', test_df_raw, add_features_v6_v7_v8_v9_v10, ['xgb1', 'xgb2', 'lgb1', 'lgb2', 'cat'])\n",
    "if result:\n",
    "    all_results['V8'] = result\n",
    "    print(f\"‚úì V8: {result['n_models']} models\")\n",
    "\n",
    "# V9\n",
    "result = evaluate_model('v9', test_df_raw, add_features_v6_v7_v8_v9_v10, ['xgb1', 'xgb2', 'lgb1', 'lgb2', 'cat'])\n",
    "if result:\n",
    "    all_results['V9'] = result\n",
    "    print(f\"‚úì V9: {result['n_models']} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85b45ae",
   "metadata": {},
   "source": [
    "## 8. Evaluate V10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d54de3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Evaluating V10...\n",
      "======================================================================\n",
      "‚úì V10: 7 models\n",
      "‚úì V10: 7 models\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Evaluating V10...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "v10_dir = MODELS_DIR / 'signal_generator_v10'\n",
    "if v10_dir.exists():\n",
    "    try:\n",
    "        scaler = joblib.load(v10_dir / 'scaler_v10.joblib')\n",
    "        feature_cols = joblib.load(v10_dir / 'feature_cols_v10.joblib')\n",
    "        weights = joblib.load(v10_dir / 'weights_v10.joblib')\n",
    "        \n",
    "        test_prepared = add_features_v10_extended(test_df_raw.copy())\n",
    "        test_prepared = create_labels(test_prepared)\n",
    "        test_binary = test_prepared[test_prepared['signal'] != -1].copy()\n",
    "        \n",
    "        for c in feature_cols:\n",
    "            if c not in test_binary.columns:\n",
    "                test_binary[c] = 0\n",
    "        \n",
    "        test_clean = test_binary.dropna(subset=feature_cols).copy()\n",
    "        X_test = test_clean[feature_cols].values\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        y_test = test_clean['signal'].values\n",
    "        \n",
    "        v10_models = {}\n",
    "        v10_preds = {}\n",
    "        v10_proba = {}\n",
    "        for name in ['xgb1', 'xgb2', 'xgb3', 'lgb1', 'lgb2', 'cat1', 'cat2']:\n",
    "            path = v10_dir / f'{name}_v10.joblib'\n",
    "            if path.exists():\n",
    "                v10_models[name] = joblib.load(path)\n",
    "                v10_preds[name] = v10_models[name].predict(X_test_scaled)\n",
    "                v10_proba[name] = v10_models[name].predict_proba(X_test_scaled)\n",
    "        \n",
    "        if v10_models:\n",
    "            first = list(v10_models.keys())[0]\n",
    "            final_proba = np.zeros_like(v10_proba[first])\n",
    "            for name in v10_models.keys():\n",
    "                w = weights.get(name, 1/len(v10_models))\n",
    "                final_proba += w * v10_proba[name]\n",
    "            \n",
    "            buy_prob = final_proba[:, 1] * 100\n",
    "            all_preds = np.array([v10_preds[name] for name in v10_models.keys()])\n",
    "            buy_votes = np.sum(all_preds == 1, axis=0)\n",
    "            \n",
    "            # V10 uses 7 models\n",
    "            all_agree = buy_votes == 7\n",
    "            strong_agree = buy_votes >= 6\n",
    "            most_agree = buy_votes >= 5\n",
    "            \n",
    "            confidence = buy_prob.copy()\n",
    "            confidence[all_agree] = np.minimum(confidence[all_agree] + 7, 100)\n",
    "            confidence[strong_agree & ~all_agree] = np.minimum(confidence[strong_agree & ~all_agree] + 4, 100)\n",
    "            confidence[most_agree & ~strong_agree] = np.minimum(confidence[most_agree & ~strong_agree] + 2, 100)\n",
    "            \n",
    "            results = {'version': 'v10', 'n_models': len(v10_models), 'total_signals': len(y_test), 'thresholds': {}}\n",
    "            for thresh in [50, 60, 70, 75, 80, 85, 90]:\n",
    "                mask = confidence >= thresh\n",
    "                if mask.sum() > 0:\n",
    "                    signals = mask.sum()\n",
    "                    correct = y_test[mask].sum()\n",
    "                    results['thresholds'][thresh] = {'signals': int(signals), 'correct': int(correct), 'accuracy': correct/signals*100}\n",
    "            \n",
    "            all_results['V10'] = results\n",
    "            print(f\"‚úì V10: {len(v10_models)} models\")\n",
    "    except Exception as e:\n",
    "        print(f\"  V10 error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d8bcbf",
   "metadata": {},
   "source": [
    "## 9. üìä COMPARISON TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "901f8f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üìä ALL MODELS COMPARISON - 75% THRESHOLD\n",
      "====================================================================================================\n",
      "\n",
      "   Model | Models |   75%+ Sig |   75%+ Acc |   80%+ Sig |   80%+ Acc |   85%+ Sig |   85%+ Acc\n",
      "----------------------------------------------------------------------------------------------------\n",
      "     V10 |      7 |        826 |      60.7% |        255 |      71.8% |         64 |      96.9%\n",
      "      V2 |      3 |         62 |      71.0% |          0 |       0.0% |          0 |       0.0%\n",
      "      V3 |      3 |          0 |       0.0% |          0 |       0.0% |          0 |       0.0%\n",
      "      V4 |      3 |          0 |       0.0% |          0 |       0.0% |          0 |       0.0%\n",
      "      V5 |      3 |          0 |       0.0% |          0 |       0.0% |          0 |       0.0%\n",
      "      V6 |      5 |       1391 |      56.9% |        409 |      57.9% |        104 |      63.5%\n",
      "      V7 |      5 |       4243 |      50.3% |       1341 |      49.1% |        365 |      61.1%\n",
      "      V8 |      5 |       4649 |      56.2% |       1299 |      54.7% |        298 |      68.8%\n",
      "      V9 |      5 |        489 |      60.9% |        122 |      46.7% |         21 |      38.1%\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"üìä ALL MODELS COMPARISON - 75% THRESHOLD\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\n{'Model':>8} | {'Models':>6} | {'75%+ Sig':>10} | {'75%+ Acc':>10} | {'80%+ Sig':>10} | {'80%+ Acc':>10} | {'85%+ Sig':>10} | {'85%+ Acc':>10}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "comparison_data = []\n",
    "for version, data in sorted(all_results.items()):\n",
    "    row = {'version': version, 'n_models': data['n_models']}\n",
    "    \n",
    "    for thresh in [75, 80, 85]:\n",
    "        if thresh in data['thresholds']:\n",
    "            row[f'{thresh}_sig'] = data['thresholds'][thresh]['signals']\n",
    "            row[f'{thresh}_acc'] = data['thresholds'][thresh]['accuracy']\n",
    "        else:\n",
    "            row[f'{thresh}_sig'] = 0\n",
    "            row[f'{thresh}_acc'] = 0\n",
    "    \n",
    "    comparison_data.append(row)\n",
    "    \n",
    "    print(f\"{version:>8} | {row['n_models']:>6} | {row['75_sig']:>10} | {row['75_acc']:>9.1f}% | {row['80_sig']:>10} | {row['80_acc']:>9.1f}% | {row['85_sig']:>10} | {row['85_acc']:>9.1f}%\")\n",
    "\n",
    "# Find best at each threshold\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1507e74",
   "metadata": {},
   "source": [
    "## 10. üèÜ BEST MODEL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d26bd3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üèÜ BEST MODEL ANALYSIS\n",
      "====================================================================================================\n",
      "\n",
      "ü•á Best at 75%+: V2\n",
      "   Accuracy: 71.0%\n",
      "   Signals: 62\n",
      "\n",
      "ü•á Best at 80%+: V10\n",
      "   Accuracy: 71.8%\n",
      "   Signals: 255\n",
      "\n",
      "ü•á Best at 85%+: V10\n",
      "   Accuracy: 96.9%\n",
      "   Signals: 64\n",
      "\n",
      "ü•á Best at 90%+: V10\n",
      "   Accuracy: 100.0%\n",
      "   Signals: 17\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"üèÜ BEST MODEL ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Find best at different thresholds\n",
    "for thresh in [75, 80, 85, 90]:\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    best_signals = 0\n",
    "    \n",
    "    for version, data in all_results.items():\n",
    "        if thresh in data['thresholds']:\n",
    "            acc = data['thresholds'][thresh]['accuracy']\n",
    "            sig = data['thresholds'][thresh]['signals']\n",
    "            if acc > best_acc and sig >= 10:  # Minimum 10 signals for reliability\n",
    "                best_acc = acc\n",
    "                best_model = version\n",
    "                best_signals = sig\n",
    "    \n",
    "    if best_model:\n",
    "        print(f\"\\nü•á Best at {thresh}%+: {best_model}\")\n",
    "        print(f\"   Accuracy: {best_acc:.1f}%\")\n",
    "        print(f\"   Signals: {best_signals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36c85a7",
   "metadata": {},
   "source": [
    "## 11. üìà DETAILED COMPARISON CHART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffe6e544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üìà DETAILED ACCURACY BY THRESHOLD\n",
      "====================================================================================================\n",
      "\n",
      " Threshold |      V10 |       V2 |       V3 |       V4 |       V5 |       V6 |       V7 |       V8 |       V9\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "      50%+ |    52.5% |    52.0% |      N/A |    48.0% |      N/A |    51.7% |    54.1% |    51.8% |    50.5%\n",
      "      60%+ |    53.6% |    54.4% |      N/A |    56.7% |      N/A |    52.4% |    54.0% |    51.9% |    51.3%\n",
      "      70%+ |    57.0% |    55.3% |      N/A |   100.0% |      N/A |    55.3% |    50.9% |    53.3% |    59.1%\n",
      "      75%+ |    60.7% |    71.0% |      N/A |      N/A |      N/A |    56.9% |    50.3% |    56.2% |    60.9%\n",
      "      80%+ |    71.8% |      N/A |      N/A |      N/A |      N/A |    57.9% |    49.1% |    54.7% |    46.7%\n",
      "      85%+ |    96.9% |      N/A |      N/A |      N/A |      N/A |    63.5% |    61.1% |    68.8% |    38.1%\n",
      "      90%+ |   100.0% |      N/A |      N/A |      N/A |      N/A |    76.2% |    54.9% |    87.2% |      N/A\n",
      "\n",
      "====================================================================================================\n",
      "SIGNAL COUNTS BY THRESHOLD\n",
      "====================================================================================================\n",
      "\n",
      " Threshold |      V10 |       V2 |       V3 |       V4 |       V5 |       V6 |       V7 |       V8 |       V9\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "      50%+ |    40116 |    40817 |      N/A |      977 |      N/A |    39978 |    27461 |    56457 |    38572\n",
      "      60%+ |    19870 |     9247 |      N/A |       67 |      N/A |    20445 |    14612 |    35904 |    15666\n",
      "      70%+ |     2606 |     1289 |      N/A |        2 |      N/A |     4438 |     8207 |    13454 |     2115\n",
      "      75%+ |      826 |       62 |      N/A |      N/A |      N/A |     1391 |     4243 |     4649 |      489\n",
      "      80%+ |      255 |      N/A |      N/A |      N/A |      N/A |      409 |     1341 |     1299 |      122\n",
      "      85%+ |       64 |      N/A |      N/A |      N/A |      N/A |      104 |      365 |      298 |       21\n",
      "      90%+ |       17 |      N/A |      N/A |      N/A |      N/A |       21 |       51 |       39 |      N/A\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"üìà DETAILED ACCURACY BY THRESHOLD\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Create comparison dataframe\n",
    "thresholds = [50, 60, 70, 75, 80, 85, 90]\n",
    "\n",
    "print(f\"\\n{'Threshold':>10}\", end=\"\")\n",
    "for version in sorted(all_results.keys()):\n",
    "    print(f\" | {version:>8}\", end=\"\")\n",
    "print(\"\")\n",
    "print(\"-\"*(12 + 11*len(all_results)))\n",
    "\n",
    "for thresh in thresholds:\n",
    "    print(f\"{thresh:>8}%+\", end=\"\")\n",
    "    for version in sorted(all_results.keys()):\n",
    "        if thresh in all_results[version]['thresholds']:\n",
    "            acc = all_results[version]['thresholds'][thresh]['accuracy']\n",
    "            print(f\" | {acc:>7.1f}%\", end=\"\")\n",
    "        else:\n",
    "            print(f\" |      N/A\", end=\"\")\n",
    "    print(\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SIGNAL COUNTS BY THRESHOLD\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\n{'Threshold':>10}\", end=\"\")\n",
    "for version in sorted(all_results.keys()):\n",
    "    print(f\" | {version:>8}\", end=\"\")\n",
    "print(\"\")\n",
    "print(\"-\"*(12 + 11*len(all_results)))\n",
    "\n",
    "for thresh in thresholds:\n",
    "    print(f\"{thresh:>8}%+\", end=\"\")\n",
    "    for version in sorted(all_results.keys()):\n",
    "        if thresh in all_results[version]['thresholds']:\n",
    "            sig = all_results[version]['thresholds'][thresh]['signals']\n",
    "            print(f\" | {sig:>8}\", end=\"\")\n",
    "        else:\n",
    "            print(f\" |      N/A\", end=\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a066b9",
   "metadata": {},
   "source": [
    "## 12. üéØ FINAL RECOMMENDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2820f98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üéØ FINAL RECOMMENDATION\n",
      "====================================================================================================\n",
      "\n",
      "üìä MODEL RANKINGS (weighted score):\n",
      "--------------------------------------------------\n",
      "ü•á 1. V10: 953.4 points\n",
      "ü•à 2. V8: 731.2 points\n",
      "ü•â 3. V6: 719.7 points\n",
      "   4. V7: 653.1 points\n",
      "   5. V9: 560.2 points\n",
      "\n",
      "====================================================================================================\n",
      "üèÜ RECOMMENDED MODEL: V10\n",
      "====================================================================================================\n",
      "\n",
      "üìã V10 DETAILS:\n",
      "   Models in ensemble: 7\n",
      "\n",
      "   Performance:\n",
      "   75%+: 60.7% accuracy, 826 signals\n",
      "   80%+: 71.8% accuracy, 255 signals\n",
      "   85%+: 96.9% accuracy, 64 signals\n",
      "   90%+: 100.0% accuracy, 17 signals\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"üéØ FINAL RECOMMENDATION\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Score each model based on accuracy at key thresholds\n",
    "scores = {}\n",
    "for version, data in all_results.items():\n",
    "    score = 0\n",
    "    \n",
    "    # 75% threshold - weight 3\n",
    "    if 75 in data['thresholds'] and data['thresholds'][75]['signals'] >= 100:\n",
    "        score += data['thresholds'][75]['accuracy'] * 3\n",
    "    \n",
    "    # 80% threshold - weight 4\n",
    "    if 80 in data['thresholds'] and data['thresholds'][80]['signals'] >= 50:\n",
    "        score += data['thresholds'][80]['accuracy'] * 4\n",
    "    \n",
    "    # 85% threshold - weight 5\n",
    "    if 85 in data['thresholds'] and data['thresholds'][85]['signals'] >= 20:\n",
    "        score += data['thresholds'][85]['accuracy'] * 5\n",
    "    \n",
    "    scores[version] = score\n",
    "\n",
    "# Sort by score\n",
    "sorted_scores = sorted(scores.items(), key=lambda x: -x[1])\n",
    "\n",
    "print(\"\\nüìä MODEL RANKINGS (weighted score):\")\n",
    "print(\"-\"*50)\n",
    "for i, (version, score) in enumerate(sorted_scores):\n",
    "    if score > 0:\n",
    "        medal = \"ü•á\" if i == 0 else \"ü•à\" if i == 1 else \"ü•â\" if i == 2 else \"  \"\n",
    "        print(f\"{medal} {i+1}. {version}: {score:.1f} points\")\n",
    "\n",
    "# Best model\n",
    "if sorted_scores:\n",
    "    best = sorted_scores[0][0]\n",
    "    print(f\"\\n\" + \"=\"*100)\n",
    "    print(f\"üèÜ RECOMMENDED MODEL: {best}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    if best in all_results:\n",
    "        data = all_results[best]\n",
    "        print(f\"\\nüìã {best} DETAILS:\")\n",
    "        print(f\"   Models in ensemble: {data['n_models']}\")\n",
    "        print(f\"\\n   Performance:\")\n",
    "        for thresh in [75, 80, 85, 90]:\n",
    "            if thresh in data['thresholds']:\n",
    "                t = data['thresholds'][thresh]\n",
    "                print(f\"   {thresh}%+: {t['accuracy']:.1f}% accuracy, {t['signals']} signals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20641c11",
   "metadata": {},
   "source": [
    "## 13. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd33b897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä SUMMARY TABLE:\n",
      "Model  Ensemble Size  75%+ Signals  75%+ Accuracy  80%+ Signals  80%+ Accuracy  85%+ Signals  85%+ Accuracy\n",
      "  V10              7           826      60.653753           255      71.764706            64      96.875000\n",
      "   V2              3            62      70.967742             0       0.000000             0       0.000000\n",
      "   V3              3             0       0.000000             0       0.000000             0       0.000000\n",
      "   V4              3             0       0.000000             0       0.000000             0       0.000000\n",
      "   V5              3             0       0.000000             0       0.000000             0       0.000000\n",
      "   V6              5          1391      56.865564           409      57.946210           104      63.461538\n",
      "   V7              5          4243      50.341739          1341      49.142431           365      61.095890\n",
      "   V8              5          4649      56.205636          1299      54.657429           298      68.791946\n",
      "   V9              5           489      60.940695           122      46.721311            21      38.095238\n"
     ]
    }
   ],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for version, data in sorted(all_results.items()):\n",
    "    row = {\n",
    "        'Model': version,\n",
    "        'Ensemble Size': data['n_models'],\n",
    "        '75%+ Signals': data['thresholds'].get(75, {}).get('signals', 0),\n",
    "        '75%+ Accuracy': data['thresholds'].get(75, {}).get('accuracy', 0),\n",
    "        '80%+ Signals': data['thresholds'].get(80, {}).get('signals', 0),\n",
    "        '80%+ Accuracy': data['thresholds'].get(80, {}).get('accuracy', 0),\n",
    "        '85%+ Signals': data['thresholds'].get(85, {}).get('signals', 0),\n",
    "        '85%+ Accuracy': data['thresholds'].get(85, {}).get('accuracy', 0),\n",
    "    }\n",
    "    summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nüìä SUMMARY TABLE:\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
