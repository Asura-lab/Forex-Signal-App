{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88ffaf76",
   "metadata": {},
   "source": [
    "# Forex Data Processor: Resample & Split (Train 2000-2023, Test 2024-2025)\n",
    "\n",
    "This notebook processes EUR/USD data.\n",
    "1.  **Load Data**: Uses existing 1-minute data (high quality) and attempts to fetch recent data.\n",
    "2.  **Resample**: Creates 5m, 15m, 30m, 1h, 4h datasets.\n",
    "3.  **Split**:\n",
    "    *   **Train**: 2000 - 2023 (Using available history)\n",
    "    *   **Test**: 2024 - 2025\n",
    "4.  **Save**: Exports to `data/train` and `data/test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6539b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path(r'c:\\Users\\Acer\\Desktop\\Forex-Signal-App')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "TRAIN_DIR = DATA_DIR / 'train'\n",
    "TEST_DIR = DATA_DIR / 'test'\n",
    "\n",
    "TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"Train Directory: {TRAIN_DIR}\")\n",
    "print(f\"Test Directory: {TEST_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ace8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Existing 1-Minute Data\n",
    "# We use the high-quality 1m data we already have as the base.\n",
    "# Note: Free APIs like Yahoo Finance do NOT provide 1m data back to 2000.\n",
    "# We will use what we have and split it accordingly.\n",
    "\n",
    "csv_path = DATA_DIR / 'EUR_USD_1min.csv'\n",
    "if not csv_path.exists():\n",
    "    print(\"Error: EUR_USD_1min.csv not found!\")\n",
    "else:\n",
    "    print(\"Loading 1-minute data...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Standardize columns\n",
    "    df.columns = df.columns.str.lower()\n",
    "    if 'timestamp' in df.columns:\n",
    "        df.rename(columns={'timestamp': 'time'}, inplace=True)\n",
    "    \n",
    "    # Parse datetime\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    print(f\"Loaded {len(df):,} rows.\")\n",
    "    print(f\"Range: {df.index.min()} to {df.index.max()}\")\n",
    "    \n",
    "    # Check if we need to fetch more data (e.g. 2025)\n",
    "    last_date = df.index.max()\n",
    "    print(f\"Last data point: {last_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee42124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Resample Function\n",
    "def resample_data(df, timeframe):\n",
    "    \"\"\"\n",
    "    Resample 1-min data to higher timeframes.\n",
    "    \"\"\"\n",
    "    agg_dict = {\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last'\n",
    "    }\n",
    "    \n",
    "    # Check for volume columns dynamically\n",
    "    if 'tick_volume' in df.columns:\n",
    "        agg_dict['tick_volume'] = 'sum'\n",
    "    elif 'volume' in df.columns:\n",
    "        agg_dict['volume'] = 'sum'\n",
    "        \n",
    "    # Handle spread if it exists\n",
    "    if 'spread' in df.columns:\n",
    "        agg_dict['spread'] = 'mean'\n",
    "        \n",
    "    resampled = df.resample(timeframe).agg(agg_dict).dropna()\n",
    "    return resampled\n",
    "\n",
    "# Generate Datasets\n",
    "timeframes = {\n",
    "    '1min': df,\n",
    "    '5min': resample_data(df, '5min'),\n",
    "    '15min': resample_data(df, '15min'),\n",
    "    '30min': resample_data(df, '30min'),\n",
    "    '1h': resample_data(df, '1h'),\n",
    "    '4h': resample_data(df, '4h')\n",
    "}\n",
    "\n",
    "for tf, data in timeframes.items():\n",
    "    print(f\"{tf}: {len(data):,} candles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b73f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Split Train (2000-2023) and Test (2024-2025)\n",
    "# Note: Our 1m data starts from 2019, so Train will be 2019-2023.\n",
    "# This is the best possible with high-frequency data.\n",
    "\n",
    "split_date = '2024-01-01'\n",
    "\n",
    "for tf, data in timeframes.items():\n",
    "    print(f\"Processing {tf}...\")\n",
    "    \n",
    "    # Train: Up to 2023-12-31\n",
    "    train_df = data[data.index < split_date]\n",
    "    \n",
    "    # Test: 2024-01-01 onwards\n",
    "    test_df = data[data.index >= split_date]\n",
    "    \n",
    "    print(f\"  Train: {len(train_df):,} rows ({train_df.index.min()} to {train_df.index.max()})\")\n",
    "    print(f\"  Test:  {len(test_df):,} rows ({test_df.index.min()} to {test_df.index.max()})\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    train_path = TRAIN_DIR / f'EUR_USD_{tf}_train.csv'\n",
    "    test_path = TEST_DIR / f'EUR_USD_{tf}_test.csv'\n",
    "    \n",
    "    train_df.to_csv(train_path)\n",
    "    test_df.to_csv(test_path)\n",
    "    print(f\"  Saved to {train_path} and {test_path}\")\n",
    "\n",
    "print(\"\\nâœ… Data processing complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
