# ğŸ—ï¸ Deep Learning Architectures

Ğ­Ğ½Ñ project Ğ½ÑŒ **3 Ó©Ó©Ñ€ deep learning Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€** Ğ°ÑˆĞ¸Ğ³Ğ»Ğ°Ğ½ Forex Ğ·Ğ°Ñ… Ğ·ÑÑĞ»Ğ¸Ğ¹Ğ½ prediction Ñ…Ğ¸Ğ¹Ğ½Ñ.

## ğŸ“Š ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹Ğ½ Ñ…ÑƒÑ€Ğ°Ğ°Ğ½Ğ³ÑƒĞ¹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PARALLEL PREDICTION MODELS:                    â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 15-min Model   â”‚ Transformer + LSTM    â”‚    â”‚
â”‚  â”‚ Expected: 88%  â”‚ Focus: Quick scalping â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 30-min Model   â”‚ Bi-LSTM + Attention   â”‚    â”‚
â”‚  â”‚ Expected: 85%  â”‚ Focus: Swing trades   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 60-min Model   â”‚ CNN-LSTM Hybrid       â”‚    â”‚
â”‚  â”‚ Expected: 82%  â”‚ Focus: Trend followingâ”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1ï¸âƒ£ Transformer + LSTM (15-Ğ¼Ğ¸Ğ½ÑƒÑ‚)

### ğŸ¯ Ğ—Ğ¾Ñ€Ğ¸Ğ»Ğ³Ğ¾

**Scalping strategy** - Ğ±Ğ¾Ğ³Ğ¸Ğ½Ğ¾ Ñ…ÑƒĞ³Ğ°Ñ†Ğ°Ğ°Ğ½Ñ‹ Ñ…ÑƒÑ€Ğ´Ğ°Ğ½ Ğ°Ñ€Ğ¸Ğ»Ğ¶Ğ°Ğ°

### ğŸ—ï¸ ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€

```
INPUT (60 steps, n_features)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Multi-Head Attention    â”‚
â”‚ â€¢ 8 attention heads     â”‚
â”‚ â€¢ Pattern recognition   â”‚
â”‚ â€¢ Parallel processing   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feed-Forward Network    â”‚
â”‚ â€¢ 256 units             â”‚
â”‚ â€¢ ReLU activation       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LSTM Layer 1            â”‚
â”‚ â€¢ 128 units             â”‚
â”‚ â€¢ Temporal dependency   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LSTM Layer 2            â”‚
â”‚ â€¢ 64 units              â”‚
â”‚ â€¢ Sequential patterns   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
OUTPUT: Direction (3 classes) + Confidence
```

### âœ¨ ĞĞ½Ñ†Ğ»Ğ¾Ğ³

- **Multi-head attention**: 8 Ó©Ó©Ñ€ attention head Ğ½ÑŒ Ó©Ó©Ñ€ Ó©Ó©Ñ€ pattern-Ò¯Ò¯Ğ´Ğ¸Ğ¹Ğ³ Ğ¾Ğ»Ğ½Ğ¾
- **Transformer block**: Parallel processing, long-range dependencies
- **LSTM layers**: Sequential temporal modeling
- **Batch normalization**: Stable training
- **Dropout**: Overfitting prevention (30%)

### ğŸ“ˆ Performance Target

- **Accuracy**: 88%+
- **Sequence length**: 60 steps (15 minutes)
- **Parameters**: ~500K
- **Best for**: 15-min to 30-min scalping

### ğŸ’¡ Ğ¯Ğ°Ğ³Ğ°Ğ°Ğ´ ÑĞ½Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹Ğ³ ÑĞ¾Ğ½Ğ³Ğ¾ÑĞ¾Ğ½?

15-Ğ¼Ğ¸Ğ½ÑƒÑ‚ timeframe Ğ½ÑŒ **Ñ…ÑƒÑ€Ğ´Ğ°Ğ½ Ó©Ó©Ñ€Ñ‡Ğ»Ó©Ğ»Ñ‚** Ğ±Ò¯Ñ…Ğ¸Ğ¹ Ğ·Ğ°Ñ… Ğ·ÑÑĞ». Transformer-Ğ¸Ğ¹Ğ½ attention mechanism Ğ½ÑŒ:

- Ğ‘Ğ¾Ğ³Ğ¸Ğ½Ğ¾ Ñ…ÑƒĞ³Ğ°Ñ†Ğ°Ğ°Ğ½Ñ‹ pattern-Ò¯Ò¯Ğ´Ğ¸Ğ¹Ğ³ Ğ¸Ğ»Ñ€Ò¯Ò¯Ğ»Ğ½Ñ
- Market regime shifts-Ğ¸Ğ¹Ğ³ Ñ…ÑƒÑ€Ğ´Ğ°Ğ½ Ğ¾Ğ¹Ğ»Ğ³Ğ¾Ğ½Ğ¾
- Multiple timeframe relationships-Ğ¸Ğ¹Ğ³ capture Ñ…Ğ¸Ğ¹Ğ½Ñ

---

## 2ï¸âƒ£ Bi-LSTM + Attention (30-Ğ¼Ğ¸Ğ½ÑƒÑ‚)

### ğŸ¯ Ğ—Ğ¾Ñ€Ğ¸Ğ»Ğ³Ğ¾

**Swing trading strategy** - Ğ´ÑƒĞ½Ğ´ Ñ…ÑƒĞ³Ğ°Ñ†Ğ°Ğ°Ğ½Ñ‹ trend Ğ´Ğ°Ğ³Ğ°ÑĞ°Ğ½ Ğ°Ñ€Ğ¸Ğ»Ğ¶Ğ°Ğ°

### ğŸ—ï¸ ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€

```
INPUT (48 steps, n_features)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Bidirectional LSTM 1    â”‚
â”‚ â€¢ Forward: 128 units    â”‚
â”‚ â€¢ Backward: 128 units   â”‚
â”‚ â€¢ Total: 256 context    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Batch Normalization     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Bidirectional LSTM 2    â”‚
â”‚ â€¢ Forward: 64 units     â”‚
â”‚ â€¢ Backward: 64 units    â”‚
â”‚ â€¢ Total: 128 context    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Custom Attention Layer  â”‚
â”‚ â€¢ 128 attention units   â”‚
â”‚ â€¢ Learnable weights     â”‚
â”‚ â€¢ Context vector        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dense Layers            â”‚
â”‚ â€¢ 128 â†’ 64 units        â”‚
â”‚ â€¢ ReLU activation       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
OUTPUT: Direction (3 classes) + Confidence
```

### âœ¨ ĞĞ½Ñ†Ğ»Ğ¾Ğ³

- **Bidirectional LSTM**: Ó¨Ğ¼Ğ½Ó©Ñ… Ğ±Ğ¾Ğ»Ğ¾Ğ½ Ğ´Ğ°Ñ€Ğ°Ğ°Ğ³Ğ¸Ğ¹Ğ½ context-Ğ¸Ğ¹Ğ³ Ñ…Ğ¾Ñ‘ÑƒĞ»Ğ°Ğ½Ğ³ Ğ½ÑŒ Ğ°ÑˆĞ¸Ğ³Ğ»Ğ°Ğ½Ğ°
- **Custom attention**: Ğ§ÑƒÑ…Ğ°Ğ» time steps-Ò¯Ò¯Ğ´ Ğ´ÑÑÑ€ Ğ°Ğ½Ñ…Ğ°Ğ°Ñ€Ğ°Ğ» Ñ‚Ó©Ğ²Ğ»Ó©Ñ€Ò¯Ò¯Ğ»Ğ½Ñ
- **Attention weights**: Learnable parameters for importance
- **Recurrent dropout**: Temporal regularization (15%)

### ğŸ“ˆ Performance Target

- **Accuracy**: 85%+
- **Sequence length**: 48 steps (30 minutes)
- **Parameters**: ~400K
- **Best for**: 30-min to 2-hour swing trades

### ğŸ’¡ Ğ¯Ğ°Ğ³Ğ°Ğ°Ğ´ ÑĞ½Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹Ğ³ ÑĞ¾Ğ½Ğ³Ğ¾ÑĞ¾Ğ½?

30-Ğ¼Ğ¸Ğ½ÑƒÑ‚ timeframe Ğ½ÑŒ **trend formation** Ò¯Ğ·Ò¯Ò¯Ğ»Ğ½Ñ. Bi-LSTM + Attention:

- Ó¨Ğ¼Ğ½Ó©Ñ… Ğ±Ğ¾Ğ»Ğ¾Ğ½ Ğ´Ğ°Ñ€Ğ°Ğ°Ğ³Ğ¸Ğ¹Ğ½ context-Ğ¸Ğ¹Ğ³ Ñ…Ğ¾ÑĞ»ÑƒÑƒĞ»Ğ½Ğ°
- Support/resistance levels-Ğ¸Ğ¹Ğ³ Ğ¸Ğ»Ñ€Ò¯Ò¯Ğ»Ğ½Ñ
- Ğ§ÑƒÑ…Ğ°Ğ» price action moments-Ğ´ focus Ñ…Ğ¸Ğ¹Ğ½Ñ

---

## 3ï¸âƒ£ CNN-LSTM Hybrid (60-Ğ¼Ğ¸Ğ½ÑƒÑ‚)

### ğŸ¯ Ğ—Ğ¾Ñ€Ğ¸Ğ»Ğ³Ğ¾

**Trend following strategy** - ÑƒÑ€Ñ‚ Ñ…ÑƒĞ³Ğ°Ñ†Ğ°Ğ°Ğ½Ñ‹ trend-Ğ¸Ğ¹Ğ½ Ğ´Ğ°Ğ³ÑƒÑƒ

### ğŸ—ï¸ ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€

```
INPUT (48 steps, n_features)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv1D Layer 1          â”‚
â”‚ â€¢ 64 filters            â”‚
â”‚ â€¢ Kernel size: 3        â”‚
â”‚ â€¢ Local pattern extract â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv1D Layer 2          â”‚
â”‚ â€¢ 128 filters           â”‚
â”‚ â€¢ Kernel size: 3        â”‚
â”‚ â€¢ Complex features      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv1D Layer 3          â”‚
â”‚ â€¢ 64 filters            â”‚
â”‚ â€¢ Kernel size: 3        â”‚
â”‚ â€¢ Feature refinement    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MaxPooling1D            â”‚
â”‚ â€¢ Pool size: 2          â”‚
â”‚ â€¢ Dimensionality reduce â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LSTM Layer 1            â”‚
â”‚ â€¢ 128 units             â”‚
â”‚ â€¢ Temporal modeling     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LSTM Layer 2            â”‚
â”‚ â€¢ 64 units              â”‚
â”‚ â€¢ Sequence refinement   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
OUTPUT: Direction (3 classes) + Confidence
```

### âœ¨ ĞĞ½Ñ†Ğ»Ğ¾Ğ³

- **1D Convolutions**: Price pattern-Ò¯Ò¯Ğ´Ğ¸Ğ¹Ğ³ automatic feature extraction
- **Multiple conv layers**: Hierarchical feature learning
- **Max pooling**: Dimensionality reduction, translation invariance
- **LSTM processing**: Temporal dependencies from CNN features
- **Hybrid approach**: Best of both worlds

### ğŸ“ˆ Performance Target

- **Accuracy**: 82%+
- **Sequence length**: 48 steps (60 minutes)
- **Parameters**: ~450K
- **Best for**: 1-hour to 4-hour trend following

### ğŸ’¡ Ğ¯Ğ°Ğ³Ğ°Ğ°Ğ´ ÑĞ½Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹Ğ³ ÑĞ¾Ğ½Ğ³Ğ¾ÑĞ¾Ğ½?

60-Ğ¼Ğ¸Ğ½ÑƒÑ‚ timeframe Ğ½ÑŒ **established trends** Ò¯Ğ·Ò¯Ò¯Ğ»Ğ½Ñ. CNN-LSTM:

- CNN Ğ½ÑŒ chart patterns-Ğ¸Ğ¹Ğ³ Ğ¾Ğ»Ğ½Ğ¾ (head & shoulders, triangles)
- LSTM Ğ½ÑŒ trend continuation/reversal-Ğ¸Ğ¹Ğ³ prediction Ñ…Ğ¸Ğ¹Ğ½Ñ
- Hierarchical features Ğ½ÑŒ multi-scale analysis Ó©Ğ³Ğ½Ó©

---

## ğŸ”„ ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹Ğ½ Ñ…Ğ°Ñ€ÑŒÑ†ÑƒÑƒĞ»Ğ°Ğ»Ñ‚

| ĞĞ½Ñ†Ğ»Ğ¾Ğ³              | Transformer+LSTM     | Bi-LSTM+Attention                | CNN-LSTM                      |
| ------------------- | -------------------- | -------------------------------- | ----------------------------- |
| **Timeframe**       | 15-min               | 30-min                           | 60-min                        |
| **Strategy**        | Scalping             | Swing Trading                    | Trend Following               |
| **Sequence Length** | 60 steps             | 48 steps                         | 48 steps                      |
| **Key Mechanism**   | Multi-head attention | Bidirectional + Custom attention | Feature extraction + temporal |
| **Parameters**      | ~500K                | ~400K                            | ~450K                         |
| **Training Speed**  | Medium               | Fast                             | Slow                          |
| **Inference Speed** | Fast                 | Very Fast                        | Medium                        |
| **Best For**        | Quick reversals      | Swing points                     | Long trends                   |
| **Target Accuracy** | 88%                  | 85%                              | 82%                           |

---

## ğŸ“Š Training Configuration

### Common Settings

```python
# Data
CURRENCY_PAIRS = ['EUR_USD', 'GBP_USD', 'USD_JPY', 'USD_CAD', 'USD_CHF', 'XAU_USD']
FEATURES = 100+ technical indicators + pair encoding (6)

# Training
BATCH_SIZE = 64
EPOCHS = 50
OPTIMIZER = Adam
LEARNING_RATE = 0.001
DROPOUT = 0.3
```

### Per-Architecture Settings

#### 15-min (Transformer+LSTM)

```python
CONFIG_15MIN = {
    'n_heads': 8,
    'ff_dim': 256,
    'lstm_units': [128, 64],
    'sequence_length': 60
}
```

#### 30-min (Bi-LSTM+Attention)

```python
CONFIG_30MIN = {
    'lstm_units': [128, 64],
    'attention_units': 128,
    'sequence_length': 48
}
```

#### 60-min (CNN-LSTM)

```python
CONFIG_60MIN = {
    'cnn_filters': [64, 128, 64],
    'kernel_size': 3,
    'lstm_units': [128, 64],
    'sequence_length': 48
}
```

---

## ğŸš€ Usage

### Import Models

```python
from ml.models.transformer_lstm import build_transformer_lstm_model
from ml.models.bilstm_attention import build_bilstm_attention_model
from ml.models.cnn_lstm import build_cnn_lstm_model
```

### Build 15-min Model

```python
model_15min = build_transformer_lstm_model(
    sequence_length=60,
    n_features=106,  # 100 indicators + 6 pair encoding
    n_heads=8,
    ff_dim=256,
    lstm_units=[128, 64],
    dropout_rate=0.3,
    n_classes=3
)
```

### Build 30-min Model

```python
model_30min = build_bilstm_attention_model(
    sequence_length=48,
    n_features=106,
    lstm_units=[128, 64],
    attention_units=128,
    dropout_rate=0.3,
    n_classes=3
)
```

### Build 60-min Model

```python
model_60min = build_cnn_lstm_model(
    sequence_length=48,
    n_features=106,
    cnn_filters=[64, 128, 64],
    kernel_size=3,
    lstm_units=[128, 64],
    dropout_rate=0.3,
    n_classes=3
)
```

---

## ğŸ“ˆ Training Process

1. **Data Loading**: Ğ‘Ò¯Ñ… 6 Ğ²Ğ°Ğ»ÑÑ‚Ñ‹Ğ½ Ğ´Ğ°Ñ‚Ğ°Ğ³ Ğ½ÑĞ³Ñ‚Ğ³ÑĞ½Ñ
2. **Resampling**: 15-min, 30-min, 60-min timeframes
3. **Feature Engineering**: 100+ technical indicators
4. **Pair Encoding**: 6 Ğ²Ğ°Ğ»ÑÑ‚ â†’ one-hot features
5. **Sequence Creation**: Sliding windows
6. **Model Training**: Architecture-specific training
7. **Evaluation**: Per-pair accuracy + overall metrics

---

## ğŸ¯ Expected Results

### Overall Accuracy

- **15-min**: 88%+ (Transformer advantage on short-term patterns)
- **30-min**: 85%+ (Attention mechanism on swing points)
- **60-min**: 82%+ (CNN's pattern recognition for trends)

### Per-Pair Performance

Expect variations across currency pairs:

- EUR/USD: Highest (most liquid, predictable)
- GBP/USD: High (volatile but trendy)
- USD/JPY: Medium-High (follows interest rates)
- USD/CAD: Medium (commodity-driven)
- USD/CHF: Medium (safe-haven effects)
- XAU/USD: Variable (gold's unique dynamics)

---

## ğŸ”§ Hyperparameter Tuning

### Transformer+LSTM

- `n_heads`: 4, 8, 16 (8 is sweet spot)
- `ff_dim`: 128, 256, 512 (256 balanced)
- `sequence_length`: 48, 60, 72 (60 for 15-min)

### Bi-LSTM+Attention

- `lstm_units`: [64,32], [128,64], [256,128]
- `attention_units`: 64, 128, 256
- `bidirectional`: True (always for swing trading)

### CNN-LSTM

- `cnn_filters`: [32,64,32], [64,128,64], [128,256,128]
- `kernel_size`: 3, 5, 7 (3 is best for forex)
- `pool_size`: 2, 3 (2 preserves more info)

---

## ğŸ“š References

### Papers

1. **Attention Is All You Need** (Vaswani et al., 2017) - Transformer architecture
2. **Bidirectional LSTM** (Graves & Schmidhuber, 2005) - Bi-LSTM foundations
3. **CNN for Time Series** (Wang et al., 2017) - 1D convolutions
4. **Attention Mechanisms** (Bahdanau et al., 2014) - Attention in RNNs

### Implementation

- TensorFlow/Keras: Deep learning framework
- Custom attention layer: Learnable importance weights
- Hybrid architectures: Combining strengths

---

## ğŸ“ Key Takeaways

1. **Different architectures for different timeframes** - No one-size-fits-all
2. **Transformer excels at pattern recognition** - Best for scalping
3. **Bi-LSTM captures bidirectional context** - Perfect for swings
4. **CNN extracts hierarchical features** - Ideal for trends
5. **Attention focuses on important moments** - Critical for all timeframes
6. **Multi-currency training** - Learns universal forex patterns
7. **Ensemble potential** - Combine all 3 for meta-predictions

---

## ğŸ”œ Next Steps

1. âœ… Train all 3 models on combined data
2. âœ… Evaluate per-pair and overall accuracy
3. â³ Build meta-learner (XGBoost) on top
4. â³ Deploy to Flask API
5. â³ Integrate with mobile app
6. â³ Real-time prediction pipeline

---

**Created**: 2025-10-23  
**Version**: 1.0  
**Status**: Ready for training
