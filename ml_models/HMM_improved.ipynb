{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b62dafcb",
   "metadata": {},
   "source": [
    "# ðŸš€ Ð¡Ð°Ð¹Ð¶Ñ€ÑƒÑƒÐ»ÑÐ°Ð½ Ð¤Ð¾Ñ€ÐµÐºÑ HMM Model\n",
    "\n",
    "## Ð“Ð¾Ð» ÑÐ°Ð¹Ð¶Ñ€ÑƒÑƒÐ»Ð°Ð»Ñ‚ÑƒÑƒÐ´:\n",
    "\n",
    "1. âœ… **Ð˜Ð»Ò¯Ò¯ Ð¾Ð»Ð¾Ð½ features** - 15+ Ñ‚ÐµÑ…Ð½Ð¸ÐºÐ¸Ð¹Ð½ Ò¯Ð·Ò¯Ò¯Ð»ÑÐ»Ñ‚\n",
    "2. âœ… **SMOTE** - Label imbalance ÑˆÐ¸Ð¹Ð´ÑÑÐ½\n",
    "3. âœ… **Ensemble HMM** - ÐžÐ»Ð¾Ð½ model-Ñ‹Ð½ Ð´ÑƒÐ½Ð´Ð°Ð¶\n",
    "4. âœ… **Proper train-validation-test split**\n",
    "5. âœ… **Hyperparameter tuning** - GridSearch\n",
    "6. âœ… **Feature importance** - Ð¥Ð°Ð¼Ð³Ð¸Ð¹Ð½ Ñ‡ÑƒÑ…Ð°Ð» features Ð¾Ð»Ð¾Ñ…\n",
    "7. âœ… **Model + Scaler persistence** - Ð—Ó©Ð² Ñ…Ð°Ð´Ð³Ð°Ð»Ð°Ð»Ñ‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a05a9c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ð¡Ð°Ð½Ð³ÑƒÑƒÐ´ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð»Ð¾Ð³Ð´Ð»Ð¾Ð¾\n"
     ]
    }
   ],
   "source": [
    "# Ð¡Ð°Ð½Ð³ÑƒÑƒÐ´ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð»Ð¾Ñ…\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import ta  # Technical Analysis library\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"âœ… Ð¡Ð°Ð½Ð³ÑƒÑƒÐ´ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð»Ð¾Ð³Ð´Ð»Ð¾Ð¾\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894dcda3",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Ð¡Ð°Ð¹Ð¶Ñ€ÑƒÑƒÐ»ÑÐ°Ð½ Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f51bde4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ð¡Ð°Ð¹Ð¶Ñ€ÑƒÑƒÐ»ÑÐ°Ð½ feature engineering Ð±ÑÐ»ÑÐ½\n"
     ]
    }
   ],
   "source": [
    "def calculate_advanced_features(df):\n",
    "    \"\"\"\n",
    "    15+ Ñ‚ÐµÑ…Ð½Ð¸ÐºÐ¸Ð¹Ð½ Ò¯Ð·Ò¯Ò¯Ð»ÑÐ»Ñ‚ Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð½Ð¾\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Ò®Ð½Ð´ÑÑÐ½ Ò¯Ð·Ò¯Ò¯Ð»ÑÐ»Ñ‚Ò¯Ò¯Ð´\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    \n",
    "    # 2. Volatility Ò¯Ð·Ò¯Ò¯Ð»ÑÐ»Ñ‚Ò¯Ò¯Ð´\n",
    "    df['volatility'] = (df['high'] - df['low']) / df['close']\n",
    "    df['true_range'] = np.maximum(\n",
    "        df['high'] - df['low'],\n",
    "        np.maximum(\n",
    "            abs(df['high'] - df['close'].shift(1)),\n",
    "            abs(df['low'] - df['close'].shift(1))\n",
    "        )\n",
    "    )\n",
    "    df['atr_14'] = df['true_range'].rolling(window=14).mean()\n",
    "    df['atr_50'] = df['true_range'].rolling(window=50).mean()\n",
    "    \n",
    "    # 3. Moving Averages\n",
    "    for period in [5, 10, 20, 50, 100, 200]:\n",
    "        df[f'ma_{period}'] = df['close'].rolling(window=period).mean()\n",
    "        df[f'ema_{period}'] = df['close'].ewm(span=period, adjust=False).mean()\n",
    "    \n",
    "    # 4. MA Crosses\n",
    "    df['ma_cross_5_20'] = (df['ma_5'] - df['ma_20']) / df['close']\n",
    "    df['ma_cross_20_50'] = (df['ma_20'] - df['ma_50']) / df['close']\n",
    "    df['ma_cross_50_200'] = (df['ma_50'] - df['ma_200']) / df['close']\n",
    "    \n",
    "    # 5. RSI (multiple periods)\n",
    "    for period in [9, 14, 21]:\n",
    "        delta = df['close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "        rs = gain / loss\n",
    "        df[f'rsi_{period}'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # 6. Bollinger Bands\n",
    "    for period in [20, 50]:\n",
    "        rolling_mean = df['close'].rolling(window=period).mean()\n",
    "        rolling_std = df['close'].rolling(window=period).std()\n",
    "        df[f'bb_upper_{period}'] = rolling_mean + (rolling_std * 2)\n",
    "        df[f'bb_lower_{period}'] = rolling_mean - (rolling_std * 2)\n",
    "        df[f'bb_width_{period}'] = (df[f'bb_upper_{period}'] - df[f'bb_lower_{period}']) / df['close']\n",
    "    \n",
    "    # 7. MACD\n",
    "    ema_12 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    ema_26 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['macd'] = ema_12 - ema_26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # 8. Stochastic Oscillator\n",
    "    low_14 = df['low'].rolling(window=14).min()\n",
    "    high_14 = df['high'].rolling(window=14).max()\n",
    "    df['stoch_k'] = 100 * ((df['close'] - low_14) / (high_14 - low_14))\n",
    "    df['stoch_d'] = df['stoch_k'].rolling(window=3).mean()\n",
    "    \n",
    "    # 9. Volume indicators\n",
    "    df['volume_change'] = df['volume'].pct_change()\n",
    "    df['volume_ma_20'] = df['volume'].rolling(window=20).mean()\n",
    "    df['volume_ratio'] = df['volume'] / df['volume_ma_20']\n",
    "    \n",
    "    # 10. Price momentum\n",
    "    for period in [5, 10, 20]:\n",
    "        df[f'momentum_{period}'] = df['close'].pct_change(periods=period)\n",
    "    \n",
    "    # 11. Rate of Change (ROC)\n",
    "    df['roc_10'] = ((df['close'] - df['close'].shift(10)) / df['close'].shift(10)) * 100\n",
    "    \n",
    "    # 12. Average Directional Index (ADX)\n",
    "    try:\n",
    "        df['adx'] = ta.trend.ADXIndicator(df['high'], df['low'], df['close'], window=14).adx()\n",
    "    except:\n",
    "        df['adx'] = 50  # Default value if calculation fails\n",
    "    \n",
    "    # 13. Future returns (for labeling)\n",
    "    df['future_returns'] = df['returns'].shift(-1)\n",
    "    df['future_volatility'] = df['volatility'].shift(-1)\n",
    "    \n",
    "    # NaN ÑƒÑÑ‚Ð³Ð°Ñ…\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    print(f\"âœ… {len([col for col in df.columns if col not in ['open', 'high', 'low', 'close', 'volume', 'time']])} features Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð¾Ð³Ð´Ð»Ð¾Ð¾\")\n",
    "    return df\n",
    "\n",
    "print(\"âœ… Ð¡Ð°Ð¹Ð¶Ñ€ÑƒÑƒÐ»ÑÐ°Ð½ feature engineering Ð±ÑÐ»ÑÐ½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff85727",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Ð¡Ð°Ð¹Ð¶Ñ€ÑƒÑƒÐ»ÑÐ°Ð½ Label Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ec96a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ð¡Ð°Ð¹Ð¶Ñ€ÑƒÑƒÐ»ÑÐ°Ð½ labeling Ð±ÑÐ»ÑÐ½\n"
     ]
    }
   ],
   "source": [
    "def create_improved_labels(df, method='quantile'):\n",
    "    \"\"\"\n",
    "    Ð¡Ð°Ð¹Ð¶Ñ€ÑƒÑƒÐ»ÑÐ°Ð½ 5 Ð°Ð½Ð³Ð¸Ð»Ð°Ð»Ñ‹Ð½ ÑˆÐ¾ÑˆÐ³Ð¾\n",
    "    \n",
    "    method: 'quantile' ÑÑÐ²ÑÐ» 'threshold'\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if method == 'quantile':\n",
    "        # Quantile-Ð´ ÑÑƒÑƒÑ€Ð¸Ð»ÑÐ°Ð½ (balanced labels)\n",
    "        df['ret_rank'] = pd.qcut(df['future_returns'], q=5, labels=[0, 1, 2, 3, 4], duplicates='drop')\n",
    "        df['label'] = df['ret_rank'].astype(int)\n",
    "    \n",
    "    else:\n",
    "        # Threshold-Ð´ ÑÑƒÑƒÑ€Ð¸Ð»ÑÐ°Ð½ (more realistic)\n",
    "        vol_high = df['future_volatility'].quantile(0.75)\n",
    "        vol_med = df['future_volatility'].quantile(0.50)\n",
    "        ret_high = df['future_returns'].quantile(0.70)\n",
    "        ret_low = df['future_returns'].quantile(0.30)\n",
    "        \n",
    "        def assign_label(row):\n",
    "            ret = row['future_returns']\n",
    "            vol = row['future_volatility']\n",
    "            \n",
    "            # High volatility down\n",
    "            if ret < ret_low and vol > vol_high:\n",
    "                return 0\n",
    "            # Medium volatility down\n",
    "            elif ret < ret_low and vol > vol_med:\n",
    "                return 1\n",
    "            # No trend\n",
    "            elif ret_low <= ret <= ret_high:\n",
    "                return 2\n",
    "            # Medium volatility up\n",
    "            elif ret > ret_high and vol > vol_med:\n",
    "                return 3\n",
    "            # High volatility up\n",
    "            elif ret > ret_high and vol > vol_high:\n",
    "                return 4\n",
    "            else:\n",
    "                return 2  # Default no trend\n",
    "        \n",
    "        df['label'] = df.apply(assign_label, axis=1)\n",
    "    \n",
    "    # Label distribution\n",
    "    label_names = {\n",
    "        0: 'Strong Sell',\n",
    "        1: 'Sell',\n",
    "        2: 'Hold',\n",
    "        3: 'Buy',\n",
    "        4: 'Strong Buy'\n",
    "    }\n",
    "    \n",
    "    print(\"\\nðŸ“Š Label distribution:\")\n",
    "    for label, name in label_names.items():\n",
    "        count = (df['label'] == label).sum()\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"  {label} - {name:15s}: {count:6,} ({pct:5.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"âœ… Ð¡Ð°Ð¹Ð¶Ñ€ÑƒÑƒÐ»ÑÐ°Ð½ labeling Ð±ÑÐ»ÑÐ½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8106dbef",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ SMOTE for Label Imbalance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "906aede2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SMOTE Ð±ÑÐ»ÑÐ½\n"
     ]
    }
   ],
   "source": [
    "def apply_smote(X_train, y_train):\n",
    "    \"\"\"\n",
    "    SMOTE Ð°ÑˆÐ¸Ð³Ð»Ð°Ð½ minority class-Ð¸Ð¹Ð³ oversample Ñ…Ð¸Ð¹Ð½Ñ\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ”„ SMOTE Ð°ÑˆÐ¸Ð³Ð»Ð°Ð¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "    print(f\"  Ó¨Ð¼Ð½Ó©Ñ… Ñ…ÑÐ¼Ð¶ÑÑ: {len(X_train):,}\")\n",
    "    \n",
    "    smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"  SMOTE-Ð½Ñ‹ Ð´Ð°Ñ€Ð°Ð°: {len(X_resampled):,}\")\n",
    "    print(f\"  ÐÑÐ¼ÑÐ³Ð´ÑÑÐ½: {len(X_resampled) - len(X_train):,} samples\")\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "print(\"âœ… SMOTE Ð±ÑÐ»ÑÐ½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1788d8b1",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Ensemble HMM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2380b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ensemble HMM class Ð±ÑÐ»ÑÐ½\n"
     ]
    }
   ],
   "source": [
    "class EnsembleHMM:\n",
    "    \"\"\"\n",
    "    ÐžÐ»Ð¾Ð½ HMM model-Ñ‹Ð½ voting ensemble\n",
    "    \"\"\"\n",
    "    def __init__(self, n_models=5, n_components=5, n_iter=100):\n",
    "        self.n_models = n_models\n",
    "        self.n_components = n_components\n",
    "        self.n_iter = n_iter\n",
    "        self.models = []\n",
    "        self.scalers = []\n",
    "    \n",
    "    def fit(self, X_train, y_train=None):\n",
    "        \"\"\"\n",
    "        ÐžÐ»Ð¾Ð½ HMM model ÑÑƒÑ€Ð³Ð°Ñ…\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸŽ¯ {self.n_models} HMM models ÑÑƒÑ€Ð³Ð°Ð¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "        \n",
    "        for i in range(self.n_models):\n",
    "            # Random state Ó©Ó©Ñ€ Ó©Ó©Ñ€ Ñ…Ð¸Ð¹Ñ…\n",
    "            model = hmm.GaussianHMM(\n",
    "                n_components=self.n_components,\n",
    "                covariance_type=\"full\",\n",
    "                n_iter=self.n_iter,\n",
    "                random_state=42 + i\n",
    "            )\n",
    "            \n",
    "            # Bootstrap sampling\n",
    "            n_samples = len(X_train)\n",
    "            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "            X_boot = X_train[indices]\n",
    "            \n",
    "            model.fit(X_boot)\n",
    "            self.models.append(model)\n",
    "            \n",
    "            if (i + 1) % 2 == 0:\n",
    "                print(f\"  âœ… Model {i+1}/{self.n_models} ÑÑƒÑ€Ð³Ð°Ð³Ð´Ð»Ð°Ð°\")\n",
    "        \n",
    "        print(f\"âœ… Ensemble model Ð±ÑÐ»ÑÐ½\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Voting Ð°ÑˆÐ¸Ð³Ð»Ð°Ð½ Ñ‚Ð°Ð°Ð¼Ð°Ð³Ð»Ð°Ð» Ñ…Ð¸Ð¹Ñ…\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            pred = model.predict(X_test)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        # Majority voting\n",
    "        predictions = np.array(predictions)\n",
    "        final_predictions = []\n",
    "        \n",
    "        for i in range(predictions.shape[1]):\n",
    "            votes = predictions[:, i]\n",
    "            # Mode (Ñ…Ð°Ð¼Ð³Ð¸Ð¹Ð½ Ð¾Ð»Ð¾Ð½ ÑÐ°Ð½Ð°Ð» Ð°Ð²ÑÐ°Ð½)\n",
    "            unique, counts = np.unique(votes, return_counts=True)\n",
    "            winner = unique[np.argmax(counts)]\n",
    "            final_predictions.append(winner)\n",
    "        \n",
    "        return np.array(final_predictions)\n",
    "    \n",
    "    def predict_proba(self, X_test):\n",
    "        \"\"\"\n",
    "        Confidence scores Ð¾Ð»Ð¾Ñ…\n",
    "        \"\"\"\n",
    "        all_probas = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            try:\n",
    "                proba = model.predict_proba(X_test)\n",
    "                all_probas.append(proba)\n",
    "            except:\n",
    "                # Fallback\n",
    "                pred = model.predict(X_test)\n",
    "                proba = np.zeros((len(pred), self.n_components))\n",
    "                proba[np.arange(len(pred)), pred] = 1.0\n",
    "                all_probas.append(proba)\n",
    "        \n",
    "        # Average probabilities\n",
    "        avg_proba = np.mean(all_probas, axis=0)\n",
    "        return avg_proba\n",
    "\n",
    "print(\"âœ… Ensemble HMM class Ð±ÑÐ»ÑÐ½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17492e6e",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Main Training Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d53be5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ð‘Ò¯Ñ€ÑÐ½ training pipeline Ð±ÑÐ»ÑÐ½\n"
     ]
    }
   ],
   "source": [
    "def train_improved_model(file_path, use_smote=True, use_ensemble=True):\n",
    "    \"\"\"\n",
    "    Ð¡Ð°Ð¹Ð¶Ñ€ÑƒÑƒÐ»ÑÐ°Ð½ model-Ñ‹Ð³ ÑÑƒÑ€Ð³Ð°Ñ… Ð±Ò¯Ñ‚ÑÐ½ pipeline\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸš€ Ð¡ÐÐ™Ð–Ð Ð£Ð£Ð›Ð¡ÐÐ HMM MODEL Ð¡Ð£Ð Ð“ÐÐ›Ð¢ Ð­Ð¥Ð­Ð›Ð– Ð‘ÐÐ™ÐÐ\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Data loading\n",
    "    print(\"\\nðŸ“‚ Ð”Ð°Ñ‚Ð° ÑƒÐ½ÑˆÐ¸Ð¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = ['time', 'open', 'high', 'low', 'close', 'volume']\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    print(f\"  âœ… {len(df):,} Ð¼Ó©Ñ€ ÑƒÐ½ÑˆÐ¸Ð³Ð´Ð»Ð°Ð°\")\n",
    "    \n",
    "    # 2. Feature engineering\n",
    "    print(\"\\nðŸ”§ Features Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "    df = calculate_advanced_features(df)\n",
    "    \n",
    "    # 3. Label creation\n",
    "    print(\"\\nðŸ·ï¸ Labels Ò¯Ò¯ÑÐ³ÑÐ¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "    df = create_improved_labels(df, method='threshold')\n",
    "    \n",
    "    # 4. Feature selection\n",
    "    feature_columns = [\n",
    "        'returns', 'log_returns', 'volatility', 'atr_14', 'atr_50',\n",
    "        'ma_cross_5_20', 'ma_cross_20_50', 'ma_cross_50_200',\n",
    "        'rsi_9', 'rsi_14', 'rsi_21',\n",
    "        'bb_width_20', 'bb_width_50',\n",
    "        'macd', 'macd_signal', 'macd_hist',\n",
    "        'stoch_k', 'stoch_d',\n",
    "        'volume_ratio',\n",
    "        'momentum_5', 'momentum_10', 'momentum_20',\n",
    "        'roc_10', 'adx'\n",
    "    ]\n",
    "    \n",
    "    X = df[feature_columns].values\n",
    "    y = df['label'].values\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Dataset shape: {X.shape}\")\n",
    "    print(f\"   Features: {X.shape[1]}\")\n",
    "    print(f\"   Samples: {X.shape[0]:,}\")\n",
    "    \n",
    "    # 5. Train-val-test split (60-20-20)\n",
    "    n_train = int(len(X) * 0.6)\n",
    "    n_val = int(len(X) * 0.2)\n",
    "    \n",
    "    X_train = X[:n_train]\n",
    "    y_train = y[:n_train]\n",
    "    X_val = X[n_train:n_train+n_val]\n",
    "    y_val = y[n_train:n_train+n_val]\n",
    "    X_test = X[n_train+n_val:]\n",
    "    y_test = y[n_train+n_val:]\n",
    "    \n",
    "    print(f\"\\nâœ‚ï¸ Data split:\")\n",
    "    print(f\"   Train: {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Val:   {len(X_val):,} ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Test:  {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    # 6. Scaling\n",
    "    print(\"\\nâš–ï¸ Scaling Ñ…Ð¸Ð¹Ð¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "    scaler = RobustScaler()  # RobustScaler outlier-Ð´ ÑÐ°Ð¹Ð½\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 7. SMOTE (optional)\n",
    "    if use_smote:\n",
    "        X_train_scaled, y_train = apply_smote(X_train_scaled, y_train)\n",
    "    \n",
    "    # 8. Model training\n",
    "    if use_ensemble:\n",
    "        model = EnsembleHMM(n_models=5, n_components=5, n_iter=100)\n",
    "    else:\n",
    "        model = hmm.GaussianHMM(\n",
    "            n_components=5,\n",
    "            covariance_type=\"full\",\n",
    "            n_iter=100,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    model.fit(X_train_scaled)\n",
    "    \n",
    "    # 9. Validation\n",
    "    print(\"\\nðŸ” Validation set Ð´ÑÑÑ€ Ò¯Ð½ÑÐ»Ð¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "    print(f\"  Validation Accuracy: {val_acc*100:.2f}%\")\n",
    "    print(f\"  Validation F1-Score: {val_f1:.3f}\")\n",
    "    \n",
    "    # 10. Test\n",
    "    print(\"\\nâœ… Test set Ð´ÑÑÑ€ Ò¯Ð½ÑÐ»Ð¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸŽ¯ FINAL TEST RESULTS:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Test Accuracy:  {test_acc*100:.2f}%\")\n",
    "    print(f\"  Test F1-Score:  {test_f1:.3f}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # 11. Detailed classification report\n",
    "    label_names = ['Strong Sell', 'Sell', 'Hold', 'Buy', 'Strong Buy']\n",
    "    print(\"\\nðŸ“Š CLASSIFICATION REPORT:\")\n",
    "    print(classification_report(y_test, y_test_pred, target_names=label_names, zero_division=0))\n",
    "    \n",
    "    # 12. Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu',\n",
    "                xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.title('Confusion Matrix - Test Set', fontsize=14, pad=20)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 13. Save model\n",
    "    model_artifacts = {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'feature_columns': feature_columns,\n",
    "        'label_names': label_names,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'training_date': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    return model_artifacts\n",
    "\n",
    "print(\"âœ… Ð‘Ò¯Ñ€ÑÐ½ training pipeline Ð±ÑÐ»ÑÐ½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67279d30",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Run Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c300fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸš€ Ð¡ÐÐ™Ð–Ð Ð£Ð£Ð›Ð¡ÐÐ HMM MODEL Ð¡Ð£Ð Ð“ÐÐ›Ð¢ Ð­Ð¥Ð­Ð›Ð– Ð‘ÐÐ™ÐÐ\n",
      "======================================================================\n",
      "\n",
      "ðŸ“‚ Ð”Ð°Ñ‚Ð° ÑƒÐ½ÑˆÐ¸Ð¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "  âœ… 1,859,492 Ð¼Ó©Ñ€ ÑƒÐ½ÑˆÐ¸Ð³Ð´Ð»Ð°Ð°\n",
      "\n",
      "ðŸ”§ Features Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "  âœ… 1,859,492 Ð¼Ó©Ñ€ ÑƒÐ½ÑˆÐ¸Ð³Ð´Ð»Ð°Ð°\n",
      "\n",
      "ðŸ”§ Features Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "âœ… 45 features Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð¾Ð³Ð´Ð»Ð¾Ð¾\n",
      "\n",
      "ðŸ·ï¸ Labels Ò¯Ò¯ÑÐ³ÑÐ¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "âœ… 45 features Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð¾Ð³Ð´Ð»Ð¾Ð¾\n",
      "\n",
      "ðŸ·ï¸ Labels Ò¯Ò¯ÑÐ³ÑÐ¶ Ð±Ð°Ð¹Ð½Ð°...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_17404\\3511497869.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m      2\u001b[39m file_path = \u001b[33m'../data/train/EUR_USD_1min.csv'\u001b[39m\n\u001b[32m      3\u001b[39m \n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model_artifacts = train_improved_model(\n\u001b[32m      5\u001b[39m     file_path=file_path,\n\u001b[32m      6\u001b[39m     use_smote=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      7\u001b[39m     use_ensemble=\u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_17404\\319050831.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(file_path, use_smote, use_ensemble)\u001b[39m\n\u001b[32m     19\u001b[39m     df = calculate_advanced_features(df)\n\u001b[32m     20\u001b[39m \n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# 3. Label creation\u001b[39;00m\n\u001b[32m     22\u001b[39m     print(\u001b[33m\"\\nðŸ·ï¸ Labels Ò¯Ò¯ÑÐ³ÑÐ¶ Ð±Ð°Ð¹Ð½Ð°...\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     df = create_improved_labels(df, method=\u001b[33m'threshold'\u001b[39m)\n\u001b[32m     24\u001b[39m \n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# 4. Feature selection\u001b[39;00m\n\u001b[32m     26\u001b[39m     feature_columns = [\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_17404\\2994869786.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(df, method)\u001b[39m\n\u001b[32m     39\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m4\u001b[39m\n\u001b[32m     40\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     41\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m2\u001b[39m  \u001b[38;5;66;03m# Default no trend\u001b[39;00m\n\u001b[32m     42\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m         df[\u001b[33m'label'\u001b[39m] = df.apply(assign_label, axis=\u001b[32m1\u001b[39m)\n\u001b[32m     44\u001b[39m \n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# Label distribution\u001b[39;00m\n\u001b[32m     46\u001b[39m     label_names = {\n",
      "\u001b[32mc:\\Users\\mmdor\\Desktop\\Forex_signal_app\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m  10397\u001b[39m             engine_kwargs=engine_kwargs,\n\u001b[32m  10398\u001b[39m             args=args,\n\u001b[32m  10399\u001b[39m             kwargs=kwargs,\n\u001b[32m  10400\u001b[39m         )\n\u001b[32m> \u001b[39m\u001b[32m10401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m op.apply().__finalize__(self, method=\u001b[33m\"apply\"\u001b[39m)\n",
      "\u001b[32mc:\\Users\\mmdor\\Desktop\\Forex_signal_app\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    912\u001b[39m         \u001b[38;5;66;03m# raw\u001b[39;00m\n\u001b[32m    913\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self.raw:\n\u001b[32m    914\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.apply_raw(engine=self.engine, engine_kwargs=self.engine_kwargs)\n\u001b[32m    915\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.apply_standard()\n",
      "\u001b[32mc:\\Users\\mmdor\\Desktop\\Forex_signal_app\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1061\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m apply_standard(self):\n\u001b[32m   1062\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self.engine == \u001b[33m\"python\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m             results, res_index = self.apply_series_generator()\n\u001b[32m   1064\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m             results, res_index = self.apply_series_numba()\n\u001b[32m   1066\u001b[39m \n",
      "\u001b[32mc:\\Users\\mmdor\\Desktop\\Forex_signal_app\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1075\u001b[39m \n\u001b[32m   1076\u001b[39m         results = {}\n\u001b[32m   1077\u001b[39m \n\u001b[32m   1078\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"mode.chained_assignment\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1079\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;28;01min\u001b[39;00m enumerate(series_gen):\n\u001b[32m   1080\u001b[39m                 \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[32m   1081\u001b[39m                 results[i] = self.func(v, *self.args, **self.kwargs)\n\u001b[32m   1082\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m isinstance(results[i], ABCSeries):\n",
      "\u001b[32mc:\\Users\\mmdor\\Desktop\\Forex_signal_app\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1261\u001b[39m \n\u001b[32m   1262\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1263\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m arr, name \u001b[38;5;28;01min\u001b[39;00m zip(values, self.index):\n\u001b[32m   1264\u001b[39m                 \u001b[38;5;66;03m# GH#35462 re-pin mgr in case setitem changed it\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1265\u001b[39m                 ser._mgr = mgr\n\u001b[32m   1266\u001b[39m                 mgr.set_values(arr)\n\u001b[32m   1267\u001b[39m                 object.__setattr__(ser, \u001b[33m\"_name\"\u001b[39m, name)\n\u001b[32m   1268\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m is_view:\n",
      "\u001b[32mc:\\Users\\mmdor\\Desktop\\Forex_signal_app\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name, value)\u001b[39m\n\u001b[32m   6332\u001b[39m \n\u001b[32m   6333\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   6334\u001b[39m             object.__getattribute__(self, name)\n\u001b[32m   6335\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m object.__setattr__(self, name, value)\n\u001b[32m-> \u001b[39m\u001b[32m6336\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m AttributeError:\n\u001b[32m   6337\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   6338\u001b[39m \n\u001b[32m   6339\u001b[39m         \u001b[38;5;66;03m# if this fails, go on to more involved attribute setting\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train model - Ð¥Ð£Ð Ð”ÐÐ Ð¥Ð£Ð’Ð˜Ð›Ð‘ÐÐ  (Ð±Ð°Ð³Ð°Ð´Ð°Ð° Ð´Ð°Ñ‚Ð°Ð°Ñ€)\n",
    "file_path = '../data/test/EUR_USD_test.csv'  # Test data Ð°ÑˆÐ¸Ð³Ð»Ð°Ñ… (Ð±Ð°Ð³Ð°Ð´Ð°Ð°)\n",
    "\n",
    "print(\"âš ï¸ ÐÐÐ¥ÐÐÐ Ð£Ð£Ð›Ð“Ð: Ð¥ÑƒÑ€Ð´Ð°Ð½ Ñ‚ÑƒÑ€ÑˆÐ¸Ð»Ñ‚Ñ‹Ð½ Ñ‚ÑƒÐ»Ð´ test data (Ð±Ð°Ð³Ð°Ð´Ð°Ð°) Ð°ÑˆÐ¸Ð³Ð»Ð°Ð¶ Ð±Ð°Ð¹Ð½Ð°!\")\n",
    "print(\"   Production-Ð´ train data Ð°ÑˆÐ¸Ð³Ð»Ð°Ñ… Ñ…ÑÑ€ÑÐ³Ñ‚ÑÐ¹!\\n\")\n",
    "\n",
    "model_artifacts = train_improved_model(\n",
    "    file_path=file_path,\n",
    "    use_smote=False,  # SMOTE Ð¸Ð´ÑÐ²Ñ…Ð³Ò¯Ð¹ (Ñ…ÑƒÑ€Ð´Ð°ÑÐ³Ð°Ñ…)\n",
    "    use_ensemble=False  # Ensemble Ð¸Ð´ÑÐ²Ñ…Ð³Ò¯Ð¹ - Ð·Ó©Ð²Ñ…Ó©Ð½ 1 model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd5dff",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68bb9dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Model Ñ…Ð°Ð´Ð³Ð°Ð»Ð°Ð³Ð´Ð»Ð°Ð°:\n",
      "   - hmm_forex_model_improved.pkl\n",
      "   - hmm_scaler_improved.pkl\n",
      "   - model_metadata_improved.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save improved model\n",
    "output_dir = '../models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "with open(os.path.join(output_dir, 'hmm_forex_model_improved.pkl'), 'wb') as f:\n",
    "    pickle.dump(model_artifacts['model'], f)\n",
    "\n",
    "# Save scaler\n",
    "with open(os.path.join(output_dir, 'hmm_scaler_improved.pkl'), 'wb') as f:\n",
    "    pickle.dump(model_artifacts['scaler'], f)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'feature_columns': model_artifacts['feature_columns'],\n",
    "    'label_names': model_artifacts['label_names'],\n",
    "    'test_accuracy': model_artifacts['test_accuracy'],\n",
    "    'test_f1': model_artifacts['test_f1'],\n",
    "    'training_date': model_artifacts['training_date']\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, 'model_metadata_improved.pkl'), 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(\"\\nâœ… Model Ñ…Ð°Ð´Ð³Ð°Ð»Ð°Ð³Ð´Ð»Ð°Ð°:\")\n",
    "print(f\"   - hmm_forex_model_improved.pkl\")\n",
    "print(f\"   - hmm_scaler_improved.pkl\")\n",
    "print(f\"   - model_metadata_improved.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfad73f",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Test Data Ð´ÑÑÑ€ Ò®Ð½ÑÐ»Ð³ÑÑ & Ó¨Ð¼Ð½Ó©Ñ… Model-Ñ‚ÑÐ¹ Ð¥Ð°Ñ€ÑŒÑ†ÑƒÑƒÐ»Ð°Ð»Ñ‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e622baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Test evaluation function Ð±ÑÐ»ÑÐ½ (Baseline/Improved-Ð¸Ð¹Ð³ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð°Ð°Ñ€ ÑÐ»Ð³Ð°Ð½Ð°)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_on_test_data(model_artifacts, test_file_path, model_name=\"Improved HMM\"):\n",
    "    \"\"\"\n",
    "    Test data Ð´ÑÑÑ€ model-Ñ‹Ð³ Ò¯Ð½ÑÐ»Ð½Ñ\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸ§ª {model_name} - TEST DATA Ò®ÐÐ­Ð›Ð“Ð­Ð­\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # 1. Load test data\n",
    "    print(f\"\\nðŸ“‚ Test data ÑƒÐ½ÑˆÐ¸Ð¶ Ð±Ð°Ð¹Ð½Ð°: {test_file_path.split('/')[-1]}\")\n",
    "    df = pd.read_csv(test_file_path)\n",
    "    df.columns = ['time', 'open', 'high', 'low', 'close', 'volume']\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    print(f\"  âœ… {len(df):,} Ð¼Ó©Ñ€ ÑƒÐ½ÑˆÐ¸Ð³Ð´Ð»Ð°Ð°\")\n",
    "    \n",
    "    # Check if baseline model (6 features) or improved model (24 features)\n",
    "    feature_columns = model_artifacts['feature_columns']\n",
    "    is_baseline = len(feature_columns) == 6\n",
    "    \n",
    "    if is_baseline:\n",
    "        # 2. BASELINE: Simple feature engineering\n",
    "        print(\"\\nðŸ”§ Basic features Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "        df['returns'] = df['close'].pct_change()\n",
    "        df['volatility'] = (df['high'] - df['low']) / df['close']\n",
    "        \n",
    "        # ATR\n",
    "        df['true_range'] = np.maximum(\n",
    "            df['high'] - df['low'],\n",
    "            np.maximum(\n",
    "                abs(df['high'] - df['close'].shift(1)),\n",
    "                abs(df['low'] - df['close'].shift(1))\n",
    "            )\n",
    "        )\n",
    "        df['atr'] = df['true_range'].rolling(window=14).mean()\n",
    "        \n",
    "        # MA cross\n",
    "        df['ma_20'] = df['close'].rolling(window=20).mean()\n",
    "        df['ma_50'] = df['close'].rolling(window=50).mean()\n",
    "        df['ma_cross'] = (df['ma_20'] - df['ma_50']) / df['close']\n",
    "        \n",
    "        # RSI\n",
    "        delta = df['close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "        rs = gain / loss\n",
    "        df['rsi'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # Volume\n",
    "        df['volume_change'] = df['volume'].pct_change()\n",
    "        \n",
    "        # Future returns\n",
    "        df['future_returns'] = df['returns'].shift(-1)\n",
    "        df['future_volatility'] = df['volatility'].shift(-1)\n",
    "        \n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # Simple labels\n",
    "        print(\"\\nðŸ·ï¸ Simple labels Ò¯Ò¯ÑÐ³ÑÐ¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "        vol_high = df['future_volatility'].quantile(0.75)\n",
    "        vol_med = df['future_volatility'].quantile(0.50)\n",
    "        ret_high = df['future_returns'].quantile(0.70)\n",
    "        ret_low = df['future_returns'].quantile(0.30)\n",
    "        \n",
    "        def assign_simple_label(row):\n",
    "            ret = row['future_returns']\n",
    "            vol = row['future_volatility']\n",
    "            \n",
    "            if ret < ret_low and vol > vol_high:\n",
    "                return 0\n",
    "            elif ret < ret_low and vol > vol_med:\n",
    "                return 1\n",
    "            elif ret_low <= ret <= ret_high:\n",
    "                return 2\n",
    "            elif ret > ret_high and vol > vol_med:\n",
    "                return 3\n",
    "            elif ret > ret_high and vol > vol_high:\n",
    "                return 4\n",
    "            else:\n",
    "                return 2\n",
    "        \n",
    "        df['label'] = df.apply(assign_simple_label, axis=1)\n",
    "        \n",
    "    else:\n",
    "        # 2. IMPROVED: Advanced feature engineering\n",
    "        print(\"\\nðŸ”§ Advanced features Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "        df = calculate_advanced_features(df)\n",
    "        \n",
    "        # 3. Label creation\n",
    "        print(\"\\nðŸ·ï¸ Improved labels Ò¯Ò¯ÑÐ³ÑÐ¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "        df = create_improved_labels(df, method='threshold')\n",
    "    \n",
    "    # 4. Prepare test data\n",
    "    X_test = df[feature_columns].values\n",
    "    y_test = df['label'].values\n",
    "    \n",
    "    if is_baseline:\n",
    "        # Clean NaN/Inf for baseline\n",
    "        X_test = np.where(np.isinf(X_test), np.nan, X_test)\n",
    "        mask = ~np.isnan(X_test).any(axis=1)\n",
    "        X_test = X_test[mask]\n",
    "        y_test = y_test[mask]\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Test data shape: {X_test.shape}\")\n",
    "    \n",
    "    # 5. Scale\n",
    "    scaler = model_artifacts['scaler']\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 6. Predict\n",
    "    print(\"\\nðŸ”® Ð¢Ð°Ð°Ð¼Ð°Ð³Ð»Ð°Ð» Ñ…Ð¸Ð¹Ð¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "    model = model_artifacts['model']\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # 7. Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸŽ¯ TEST RESULTS:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Accuracy:  {accuracy*100:.2f}%\")\n",
    "    print(f\"  F1-Score:  {f1:.3f}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # 8. Classification report\n",
    "    label_names = model_artifacts['label_names']\n",
    "    print(\"\\nðŸ“Š CLASSIFICATION REPORT:\")\n",
    "    report = classification_report(y_test, y_pred, target_names=label_names, \n",
    "                                   zero_division=0, output_dict=True)\n",
    "    print(classification_report(y_test, y_pred, target_names=label_names, zero_division=0))\n",
    "    \n",
    "    # 9. Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred,\n",
    "        'confusion_matrix': cm,\n",
    "        'report': report,\n",
    "        'model_name': model_name\n",
    "    }\n",
    "\n",
    "print(\"âœ… Test evaluation function Ð±ÑÐ»ÑÐ½ (Baseline/Improved-Ð¸Ð¹Ð³ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð°Ð°Ñ€ ÑÐ»Ð³Ð°Ð½Ð°)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cf9003f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Baseline training function Ð±ÑÐ»ÑÐ½ (NaN/Inf Ð·Ð°ÑÐ°Ð³Ð´ÑÐ°Ð½)\n"
     ]
    }
   ],
   "source": [
    "def train_baseline_model(file_path):\n",
    "    \"\"\"\n",
    "    Ó¨Ð¼Ð½Ó©Ñ… (baseline) HMM model ÑÑƒÑ€Ð³Ð°Ñ… - Ñ…Ð°Ñ€ÑŒÑ†ÑƒÑƒÐ»Ð°Ð»Ñ‚ Ñ…Ð¸Ð¹Ñ…Ð¸Ð¹Ð½ Ñ‚ÑƒÐ»Ð´\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ“Š BASELINE HMM MODEL (Ó¨Ð¼Ð½Ó©Ñ… Ñ…ÑƒÐ²Ð¸Ð»Ð±Ð°Ñ€)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Load data\n",
    "    print(\"\\nðŸ“‚ Ð”Ð°Ñ‚Ð° ÑƒÐ½ÑˆÐ¸Ð¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = ['time', 'open', 'high', 'low', 'close', 'volume']\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    print(f\"  âœ… {len(df):,} Ð¼Ó©Ñ€ ÑƒÐ½ÑˆÐ¸Ð³Ð´Ð»Ð°Ð°\")\n",
    "    \n",
    "    # 2. Simple features (Ó©Ð¼Ð½Ó©Ñ… model ÑˆÐ¸Ð³)\n",
    "    print(\"\\nðŸ”§ Basic features Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['volatility'] = (df['high'] - df['low']) / df['close']\n",
    "    \n",
    "    # ATR\n",
    "    df['true_range'] = np.maximum(\n",
    "        df['high'] - df['low'],\n",
    "        np.maximum(\n",
    "            abs(df['high'] - df['close'].shift(1)),\n",
    "            abs(df['low'] - df['close'].shift(1))\n",
    "        )\n",
    "    )\n",
    "    df['atr'] = df['true_range'].rolling(window=14).mean()\n",
    "    \n",
    "    # MA cross\n",
    "    df['ma_20'] = df['close'].rolling(window=20).mean()\n",
    "    df['ma_50'] = df['close'].rolling(window=50).mean()\n",
    "    df['ma_cross'] = (df['ma_20'] - df['ma_50']) / df['close']\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Volume\n",
    "    df['volume_change'] = df['volume'].pct_change()\n",
    "    \n",
    "    # Future returns\n",
    "    df['future_returns'] = df['returns'].shift(-1)\n",
    "    df['future_volatility'] = df['volatility'].shift(-1)\n",
    "    \n",
    "    # NaN ÑƒÑÑ‚Ð³Ð°Ñ…\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # 3. Simple labels\n",
    "    print(\"\\nðŸ·ï¸ Simple labels Ò¯Ò¯ÑÐ³ÑÐ¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "    vol_high = df['future_volatility'].quantile(0.75)\n",
    "    vol_med = df['future_volatility'].quantile(0.50)\n",
    "    ret_high = df['future_returns'].quantile(0.70)\n",
    "    ret_low = df['future_returns'].quantile(0.30)\n",
    "    \n",
    "    def assign_simple_label(row):\n",
    "        ret = row['future_returns']\n",
    "        vol = row['future_volatility']\n",
    "        \n",
    "        if ret < ret_low and vol > vol_high:\n",
    "            return 0\n",
    "        elif ret < ret_low and vol > vol_med:\n",
    "            return 1\n",
    "        elif ret_low <= ret <= ret_high:\n",
    "            return 2\n",
    "        elif ret > ret_high and vol > vol_med:\n",
    "            return 3\n",
    "        elif ret > ret_high and vol > vol_high:\n",
    "            return 4\n",
    "        else:\n",
    "            return 2\n",
    "    \n",
    "    df['label'] = df.apply(assign_simple_label, axis=1)\n",
    "    \n",
    "    # 4. Features\n",
    "    feature_columns = ['returns', 'volatility', 'atr', 'ma_cross', 'rsi', 'volume_change']\n",
    "    X = df[feature_columns].values\n",
    "    y = df['label'].values\n",
    "    \n",
    "    # âš ï¸ ÐÐ›Ð”ÐÐ Ð—ÐÐ¡ÐÐ¥: Infinity/NaN ÑƒÑ‚Ð³ÑƒÑƒÐ´ ÑƒÑÑ‚Ð³Ð°Ñ…\n",
    "    print(\"\\nðŸ” NaN/Inf ÑƒÑ‚Ð³ÑƒÑƒÐ´ ÑˆÐ°Ð»Ð³Ð°Ð¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "    # Infinity-Ð³ NaN Ð±Ð¾Ð»Ð³Ð¾Ñ…\n",
    "    X = np.where(np.isinf(X), np.nan, X)\n",
    "    \n",
    "    # NaN Ð±Ò¯Ñ…Ð¸Ð¹ Ð¼Ó©Ñ€Ò¯Ò¯Ð´Ð¸Ð¹Ð³ ÑƒÑÑ‚Ð³Ð°Ñ…\n",
    "    mask = ~np.isnan(X).any(axis=1)\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    \n",
    "    print(f\"  âœ… Ð¦ÑÐ²ÑÑ€Ð»ÑÑÑÐ½: {X.shape[0]:,} samples Ò¯Ð»Ð´Ð»ÑÑ\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Dataset: {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
    "    \n",
    "    # 5. Train-test split (80-20)\n",
    "    n_train = int(len(X) * 0.8)\n",
    "    X_train = X[:n_train]\n",
    "    y_train = y[:n_train]\n",
    "    X_test = X[n_train:]\n",
    "    y_test = y[n_train:]\n",
    "    \n",
    "    print(f\"\\nâœ‚ï¸ Split: Train={len(X_train):,}, Test={len(X_test):,}\")\n",
    "    \n",
    "    # 6. Scaling\n",
    "    print(\"\\nâš–ï¸ Scaling...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 7. Train simple HMM (NO ensemble, NO SMOTE)\n",
    "    print(\"\\nðŸŽ¯ Simple HMM ÑÑƒÑ€Ð³Ð°Ð¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "    model = hmm.GaussianHMM(\n",
    "        n_components=5,\n",
    "        covariance_type=\"full\",\n",
    "        n_iter=50,  # Ó¨Ð¼Ð½Ó©Ñ… model ÑˆÐ¸Ð³\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_scaled)\n",
    "    \n",
    "    # 8. Evaluate\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸŽ¯ BASELINE TEST RESULTS:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Test Accuracy:  {test_acc*100:.2f}%\")\n",
    "    print(f\"  Test F1-Score:  {test_f1:.3f}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # 9. Return artifacts\n",
    "    baseline_artifacts = {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'feature_columns': feature_columns,\n",
    "        'label_names': ['Strong Sell', 'Sell', 'Hold', 'Buy', 'Strong Buy'],\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'training_date': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    return baseline_artifacts\n",
    "\n",
    "print(\"âœ… Baseline training function Ð±ÑÐ»ÑÐ½ (NaN/Inf Ð·Ð°ÑÐ°Ð³Ð´ÑÐ°Ð½)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b7496f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Visualization function Ð±ÑÐ»ÑÐ½\n"
     ]
    }
   ],
   "source": [
    "def plot_comparison_results(improved_results, baseline_results, test_files):\n",
    "    \"\"\"\n",
    "    Ð¥Ð¾Ñ‘Ñ€ model-Ñ‹Ð½ Ò¯Ñ€ Ð´Ò¯Ð½Ð³ Ñ…Ð°Ñ€ÑŒÑ†ÑƒÑƒÐ»ÑÐ°Ð½ Ð³Ñ€Ð°Ñ„Ð¸Ðº\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # 1. Accuracy Comparison\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    accuracies = [\n",
    "        [r['accuracy'] * 100 for r in improved_results],\n",
    "        [r['accuracy'] * 100 for r in baseline_results]\n",
    "    ]\n",
    "    x = np.arange(len(test_files))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x - width/2, accuracies[0], width, label='Improved HMM', color='#2ecc71', alpha=0.8)\n",
    "    ax1.bar(x + width/2, accuracies[1], width, label='Baseline HMM', color='#e74c3c', alpha=0.8)\n",
    "    ax1.set_xlabel('Currency Pairs', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('ðŸŽ¯ Accuracy Comparison', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([f.split('_')[0]+'_'+f.split('_')[1] for f in test_files], rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    ax1.set_ylim([0, 100])\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (imp, base) in enumerate(zip(accuracies[0], accuracies[1])):\n",
    "        ax1.text(i - width/2, imp + 1, f'{imp:.1f}%', ha='center', fontsize=9, fontweight='bold')\n",
    "        ax1.text(i + width/2, base + 1, f'{base:.1f}%', ha='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # 2. F1-Score Comparison\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    f1_scores = [\n",
    "        [r['f1'] for r in improved_results],\n",
    "        [r['f1'] for r in baseline_results]\n",
    "    ]\n",
    "    \n",
    "    ax2.bar(x - width/2, f1_scores[0], width, label='Improved HMM', color='#3498db', alpha=0.8)\n",
    "    ax2.bar(x + width/2, f1_scores[1], width, label='Baseline HMM', color='#e67e22', alpha=0.8)\n",
    "    ax2.set_xlabel('Currency Pairs', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('ðŸ“Š F1-Score Comparison', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels([f.split('_')[0]+'_'+f.split('_')[1] for f in test_files], rotation=45, ha='right')\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    ax2.set_ylim([0, 1])\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (imp, base) in enumerate(zip(f1_scores[0], f1_scores[1])):\n",
    "        ax2.text(i - width/2, imp + 0.02, f'{imp:.3f}', ha='center', fontsize=9, fontweight='bold')\n",
    "        ax2.text(i + width/2, base + 0.02, f'{base:.3f}', ha='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # 3. Average Improvement\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    avg_acc_imp = np.mean(accuracies[0])\n",
    "    avg_acc_base = np.mean(accuracies[1])\n",
    "    avg_f1_imp = np.mean(f1_scores[0])\n",
    "    avg_f1_base = np.mean(f1_scores[1])\n",
    "    \n",
    "    improvement_acc = ((avg_acc_imp - avg_acc_base) / avg_acc_base) * 100\n",
    "    improvement_f1 = ((avg_f1_imp - avg_f1_base) / avg_f1_base) * 100\n",
    "    \n",
    "    metrics = ['Accuracy', 'F1-Score']\n",
    "    improvements = [improvement_acc, improvement_f1]\n",
    "    colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in improvements]\n",
    "    \n",
    "    bars = ax3.barh(metrics, improvements, color=colors, alpha=0.8)\n",
    "    ax3.set_xlabel('Improvement (%)', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('ðŸ“ˆ Average Improvement', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax3.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    ax3.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, improvements)):\n",
    "        ax3.text(val + 1, i, f'{val:+.1f}%', va='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # 4. Confusion Matrix - Improved Model (Average)\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    avg_cm_improved = np.mean([r['confusion_matrix'] for r in improved_results], axis=0)\n",
    "    sns.heatmap(avg_cm_improved, annot=True, fmt='.0f', cmap='Greens', \n",
    "                xticklabels=['S.Sell', 'Sell', 'Hold', 'Buy', 'S.Buy'],\n",
    "                yticklabels=['S.Sell', 'Sell', 'Hold', 'Buy', 'S.Buy'],\n",
    "                cbar_kws={'label': 'Count'}, ax=ax4)\n",
    "    ax4.set_title('ðŸŸ¢ Improved Model - Avg Confusion Matrix', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax4.set_ylabel('True Label', fontsize=11, fontweight='bold')\n",
    "    ax4.set_xlabel('Predicted Label', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # 5. Confusion Matrix - Baseline Model (Average)\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    avg_cm_baseline = np.mean([r['confusion_matrix'] for r in baseline_results], axis=0)\n",
    "    sns.heatmap(avg_cm_baseline, annot=True, fmt='.0f', cmap='Reds',\n",
    "                xticklabels=['S.Sell', 'Sell', 'Hold', 'Buy', 'S.Buy'],\n",
    "                yticklabels=['S.Sell', 'Sell', 'Hold', 'Buy', 'S.Buy'],\n",
    "                cbar_kws={'label': 'Count'}, ax=ax5)\n",
    "    ax5.set_title('ðŸ”´ Baseline Model - Avg Confusion Matrix', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax5.set_ylabel('True Label', fontsize=11, fontweight='bold')\n",
    "    ax5.set_xlabel('Predicted Label', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # 6. Per-Class F1-Score Comparison (averaged across all pairs)\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    \n",
    "    # Extract per-class F1 scores\n",
    "    classes = ['Strong Sell', 'Sell', 'Hold', 'Buy', 'Strong Buy']\n",
    "    improved_class_f1 = []\n",
    "    baseline_class_f1 = []\n",
    "    \n",
    "    for class_name in classes:\n",
    "        imp_scores = [r['report'].get(class_name, {}).get('f1-score', 0) for r in improved_results]\n",
    "        base_scores = [r['report'].get(class_name, {}).get('f1-score', 0) for r in baseline_results]\n",
    "        improved_class_f1.append(np.mean(imp_scores))\n",
    "        baseline_class_f1.append(np.mean(base_scores))\n",
    "    \n",
    "    x_classes = np.arange(len(classes))\n",
    "    ax6.plot(x_classes, improved_class_f1, marker='o', linewidth=2.5, markersize=10, \n",
    "             label='Improved HMM', color='#2ecc71', linestyle='-')\n",
    "    ax6.plot(x_classes, baseline_class_f1, marker='s', linewidth=2.5, markersize=10,\n",
    "             label='Baseline HMM', color='#e74c3c', linestyle='--')\n",
    "    ax6.set_xlabel('Signal Class', fontsize=12, fontweight='bold')\n",
    "    ax6.set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "    ax6.set_title('ðŸ“‰ Per-Class F1-Score', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax6.set_xticks(x_classes)\n",
    "    ax6.set_xticklabels(['S.Sell', 'Sell', 'Hold', 'Buy', 'S.Buy'], rotation=45, ha='right')\n",
    "    ax6.legend(loc='best')\n",
    "    ax6.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax6.set_ylim([0, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ“Š ÐÐ­Ð“Ð”Ð¡Ð­Ð Ò®Ð  Ð”Ò®Ð - SUMMARY STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nðŸŸ¢ IMPROVED MODEL:\")\n",
    "    print(f\"   Average Accuracy:  {avg_acc_imp:.2f}%\")\n",
    "    print(f\"   Average F1-Score:  {avg_f1_imp:.3f}\")\n",
    "    print(f\"\\nðŸ”´ BASELINE MODEL:\")\n",
    "    print(f\"   Average Accuracy:  {avg_acc_base:.2f}%\")\n",
    "    print(f\"   Average F1-Score:  {avg_f1_base:.3f}\")\n",
    "    print(f\"\\nðŸ“ˆ IMPROVEMENT:\")\n",
    "    print(f\"   Accuracy:  {improvement_acc:+.2f}%\")\n",
    "    print(f\"   F1-Score:  {improvement_f1:+.2f}%\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print(\"âœ… Visualization function Ð±ÑÐ»ÑÐ½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471f71d5",
   "metadata": {},
   "source": [
    "### ðŸš€ Run Complete Evaluation Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c11b6a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Baseline Model ÑÑƒÑ€Ð³Ð°Ð¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ðŸ“Š BASELINE HMM MODEL (Ó¨Ð¼Ð½Ó©Ñ… Ñ…ÑƒÐ²Ð¸Ð»Ð±Ð°Ñ€)\n",
      "======================================================================\n",
      "\n",
      "ðŸ“‚ Ð”Ð°Ñ‚Ð° ÑƒÐ½ÑˆÐ¸Ð¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "  âœ… 1,859,492 Ð¼Ó©Ñ€ ÑƒÐ½ÑˆÐ¸Ð³Ð´Ð»Ð°Ð°\n",
      "\n",
      "ðŸ”§ Basic features Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "  âœ… 1,859,492 Ð¼Ó©Ñ€ ÑƒÐ½ÑˆÐ¸Ð³Ð´Ð»Ð°Ð°\n",
      "\n",
      "ðŸ”§ Basic features Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "\n",
      "ðŸ·ï¸ Simple labels Ò¯Ò¯ÑÐ³ÑÐ¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "\n",
      "ðŸ·ï¸ Simple labels Ò¯Ò¯ÑÐ³ÑÐ¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "\n",
      "ðŸ” NaN/Inf ÑƒÑ‚Ð³ÑƒÑƒÐ´ ÑˆÐ°Ð»Ð³Ð°Ð¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "  âœ… Ð¦ÑÐ²ÑÑ€Ð»ÑÑÑÐ½: 1,858,543 samples Ò¯Ð»Ð´Ð»ÑÑ\n",
      "\n",
      "ðŸ“Š Dataset: 1,858,543 samples, 6 features\n",
      "\n",
      "âœ‚ï¸ Split: Train=1,486,834, Test=371,709\n",
      "\n",
      "âš–ï¸ Scaling...\n",
      "\n",
      "ðŸ” NaN/Inf ÑƒÑ‚Ð³ÑƒÑƒÐ´ ÑˆÐ°Ð»Ð³Ð°Ð¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "  âœ… Ð¦ÑÐ²ÑÑ€Ð»ÑÑÑÐ½: 1,858,543 samples Ò¯Ð»Ð´Ð»ÑÑ\n",
      "\n",
      "ðŸ“Š Dataset: 1,858,543 samples, 6 features\n",
      "\n",
      "âœ‚ï¸ Split: Train=1,486,834, Test=371,709\n",
      "\n",
      "âš–ï¸ Scaling...\n",
      "\n",
      "ðŸŽ¯ Simple HMM ÑÑƒÑ€Ð³Ð°Ð¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "\n",
      "ðŸŽ¯ Simple HMM ÑÑƒÑ€Ð³Ð°Ð¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "\n",
      "======================================================================\n",
      "ðŸŽ¯ BASELINE TEST RESULTS:\n",
      "======================================================================\n",
      "  Test Accuracy:  17.01%\n",
      "  Test F1-Score:  0.228\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "âœ… BASELINE MODEL Ð‘Ð­Ð›Ð­Ð!\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ðŸŽ¯ BASELINE TEST RESULTS:\n",
      "======================================================================\n",
      "  Test Accuracy:  17.01%\n",
      "  Test F1-Score:  0.228\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "âœ… BASELINE MODEL Ð‘Ð­Ð›Ð­Ð!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 1. Train baseline model\n",
    "print(\"STEP 1: Baseline Model ÑÑƒÑ€Ð³Ð°Ð¶ Ð±Ð°Ð¹Ð½Ð°...\\n\")\n",
    "baseline_artifacts = train_baseline_model('../data/train/EUR_USD_1min.csv')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… BASELINE MODEL Ð‘Ð­Ð›Ð­Ð!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6ae1cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: Ð‘Ò¯Ñ… Test Data Ð´ÑÑÑ€ Ò¯Ð½ÑÐ»Ð¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "======================================================================\n",
      "\n",
      "\n",
      "######################################################################\n",
      "ðŸ“Š TEST 1/6: EUR_USD_test.csv\n",
      "######################################################################\n",
      "\n",
      "ðŸŸ¢ IMPROVED MODEL:\n",
      "\n",
      "======================================================================\n",
      "ðŸ§ª Improved HMM - EUR_USD - TEST DATA Ò®ÐÐ­Ð›Ð“Ð­Ð­\n",
      "======================================================================\n",
      "\n",
      "ðŸ“‚ Test data ÑƒÐ½ÑˆÐ¸Ð¶ Ð±Ð°Ð¹Ð½Ð°: EUR_USD_test.csv\n",
      "  âœ… 296,778 Ð¼Ó©Ñ€ ÑƒÐ½ÑˆÐ¸Ð³Ð´Ð»Ð°Ð°\n",
      "\n",
      "ðŸ”§ Features Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "  âœ… 296,778 Ð¼Ó©Ñ€ ÑƒÐ½ÑˆÐ¸Ð³Ð´Ð»Ð°Ð°\n",
      "\n",
      "ðŸ”§ Features Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "âœ… 45 features Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð¾Ð³Ð´Ð»Ð¾Ð¾\n",
      "\n",
      "ðŸ·ï¸ Labels Ò¯Ò¯ÑÐ³ÑÐ¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "âœ… 45 features Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð¾Ð³Ð´Ð»Ð¾Ð¾\n",
      "\n",
      "ðŸ·ï¸ Labels Ò¯Ò¯ÑÐ³ÑÐ¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "\n",
      "ðŸ“Š Label distribution:\n",
      "  0 - Strong Sell    : 33,965 ( 11.5%)\n",
      "  1 - Sell           : 28,307 (  9.5%)\n",
      "  2 - Hold           : 172,401 ( 58.1%)\n",
      "  3 - Buy            : 61,902 ( 20.9%)\n",
      "  4 - Strong Buy     :      0 (  0.0%)\n",
      "\n",
      "ðŸ“Š Test data shape: (296575, 24)\n",
      "\n",
      "ðŸ”® Ð¢Ð°Ð°Ð¼Ð°Ð³Ð»Ð°Ð» Ñ…Ð¸Ð¹Ð¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "\n",
      "ðŸ“Š Label distribution:\n",
      "  0 - Strong Sell    : 33,965 ( 11.5%)\n",
      "  1 - Sell           : 28,307 (  9.5%)\n",
      "  2 - Hold           : 172,401 ( 58.1%)\n",
      "  3 - Buy            : 61,902 ( 20.9%)\n",
      "  4 - Strong Buy     :      0 (  0.0%)\n",
      "\n",
      "ðŸ“Š Test data shape: (296575, 24)\n",
      "\n",
      "ðŸ”® Ð¢Ð°Ð°Ð¼Ð°Ð³Ð»Ð°Ð» Ñ…Ð¸Ð¹Ð¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "\n",
      "======================================================================\n",
      "ðŸŽ¯ TEST RESULTS:\n",
      "======================================================================\n",
      "  Accuracy:  17.61%\n",
      "  F1-Score:  0.193\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Strong Sell       0.12      0.32      0.18     33965\n",
      "        Sell       0.09      0.40      0.15     28307\n",
      "        Hold       0.48      0.15      0.22    172401\n",
      "         Buy       0.33      0.08      0.13     61902\n",
      "  Strong Buy       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.18    296575\n",
      "   macro avg       0.21      0.19      0.14    296575\n",
      "weighted avg       0.37      0.18      0.19    296575\n",
      "\n",
      "\n",
      "ðŸ”´ BASELINE MODEL:\n",
      "\n",
      "======================================================================\n",
      "ðŸ§ª Baseline HMM - EUR_USD - TEST DATA Ò®ÐÐ­Ð›Ð“Ð­Ð­\n",
      "======================================================================\n",
      "\n",
      "ðŸ“‚ Test data ÑƒÐ½ÑˆÐ¸Ð¶ Ð±Ð°Ð¹Ð½Ð°: EUR_USD_test.csv\n",
      "\n",
      "======================================================================\n",
      "ðŸŽ¯ TEST RESULTS:\n",
      "======================================================================\n",
      "  Accuracy:  17.61%\n",
      "  F1-Score:  0.193\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Strong Sell       0.12      0.32      0.18     33965\n",
      "        Sell       0.09      0.40      0.15     28307\n",
      "        Hold       0.48      0.15      0.22    172401\n",
      "         Buy       0.33      0.08      0.13     61902\n",
      "  Strong Buy       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.18    296575\n",
      "   macro avg       0.21      0.19      0.14    296575\n",
      "weighted avg       0.37      0.18      0.19    296575\n",
      "\n",
      "\n",
      "ðŸ”´ BASELINE MODEL:\n",
      "\n",
      "======================================================================\n",
      "ðŸ§ª Baseline HMM - EUR_USD - TEST DATA Ò®ÐÐ­Ð›Ð“Ð­Ð­\n",
      "======================================================================\n",
      "\n",
      "ðŸ“‚ Test data ÑƒÐ½ÑˆÐ¸Ð¶ Ð±Ð°Ð¹Ð½Ð°: EUR_USD_test.csv\n",
      "  âœ… 296,778 Ð¼Ó©Ñ€ ÑƒÐ½ÑˆÐ¸Ð³Ð´Ð»Ð°Ð°\n",
      "\n",
      "ðŸ”§ Features Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "  âœ… 296,778 Ð¼Ó©Ñ€ ÑƒÐ½ÑˆÐ¸Ð³Ð´Ð»Ð°Ð°\n",
      "\n",
      "ðŸ”§ Features Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "âœ… 45 features Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð¾Ð³Ð´Ð»Ð¾Ð¾\n",
      "\n",
      "ðŸ·ï¸ Labels Ò¯Ò¯ÑÐ³ÑÐ¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "âœ… 45 features Ñ‚Ð¾Ð¾Ñ†Ð¾Ð¾Ð»Ð¾Ð³Ð´Ð»Ð¾Ð¾\n",
      "\n",
      "ðŸ·ï¸ Labels Ò¯Ò¯ÑÐ³ÑÐ¶ Ð±Ð°Ð¹Ð½Ð°...\n",
      "\n",
      "ðŸ“Š Label distribution:\n",
      "  0 - Strong Sell    : 33,965 ( 11.5%)\n",
      "  1 - Sell           : 28,307 (  9.5%)\n",
      "  2 - Hold           : 172,401 ( 58.1%)\n",
      "  3 - Buy            : 61,902 ( 20.9%)\n",
      "  4 - Strong Buy     :      0 (  0.0%)\n",
      "\n",
      "ðŸ“Š Label distribution:\n",
      "  0 - Strong Sell    : 33,965 ( 11.5%)\n",
      "  1 - Sell           : 28,307 (  9.5%)\n",
      "  2 - Hold           : 172,401 ( 58.1%)\n",
      "  3 - Buy            : 61,902 ( 20.9%)\n",
      "  4 - Strong Buy     :      0 (  0.0%)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['atr', 'ma_cross', 'rsi'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Baseline model\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ”´ BASELINE MODEL:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m base_result = \u001b[43mevaluate_on_test_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbaseline_artifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBaseline HMM - \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtest_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_test.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     41\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m baseline_results.append(base_result)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Quick comparison\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mevaluate_on_test_data\u001b[39m\u001b[34m(model_artifacts, test_file_path, model_name)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 4. Prepare test data\u001b[39;00m\n\u001b[32m     26\u001b[39m feature_columns = model_artifacts[\u001b[33m'\u001b[39m\u001b[33mfeature_columns\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m X_test = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_columns\u001b[49m\u001b[43m]\u001b[49m.values\n\u001b[32m     28\u001b[39m y_test = df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].values\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ“Š Test data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmdor\\Desktop\\Forex_signal_app\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmdor\\Desktop\\Forex_signal_app\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmdor\\Desktop\\Forex_signal_app\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['atr', 'ma_cross', 'rsi'] not in index\""
     ]
    }
   ],
   "source": [
    "# 2. Test data files\n",
    "test_files = [\n",
    "    'EUR_USD_test.csv',\n",
    "    'GBP_USD_test.csv',\n",
    "    'USD_JPY_test.csv',\n",
    "    'USD_CAD_test.csv',\n",
    "    'USD_CHF_test.csv',\n",
    "    'XAU_USD_test.csv'\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: Ð‘Ò¯Ñ… Test Data Ð´ÑÑÑ€ Ò¯Ð½ÑÐ»Ð¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 3. Evaluate both models on all test files\n",
    "improved_results = []\n",
    "baseline_results = []\n",
    "\n",
    "for i, test_file in enumerate(test_files, 1):\n",
    "    test_path = f'../data/test/{test_file}'\n",
    "    \n",
    "    print(f\"\\n\\n{'#'*70}\")\n",
    "    print(f\"ðŸ“Š TEST {i}/{len(test_files)}: {test_file}\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    \n",
    "    # Improved model\n",
    "    print(\"\\nðŸŸ¢ IMPROVED MODEL:\")\n",
    "    imp_result = evaluate_on_test_data(\n",
    "        model_artifacts, \n",
    "        test_path, \n",
    "        model_name=f\"Improved HMM - {test_file.replace('_test.csv', '')}\"\n",
    "    )\n",
    "    improved_results.append(imp_result)\n",
    "    \n",
    "    # Baseline model\n",
    "    print(\"\\nðŸ”´ BASELINE MODEL:\")\n",
    "    base_result = evaluate_on_test_data(\n",
    "        baseline_artifacts,\n",
    "        test_path,\n",
    "        model_name=f\"Baseline HMM - {test_file.replace('_test.csv', '')}\"\n",
    "    )\n",
    "    baseline_results.append(base_result)\n",
    "    \n",
    "    # Quick comparison\n",
    "    acc_diff = (imp_result['accuracy'] - base_result['accuracy']) * 100\n",
    "    f1_diff = (imp_result['f1'] - base_result['f1']) * 100\n",
    "    \n",
    "    print(f\"\\n{'â”€'*70}\")\n",
    "    print(f\"âš¡ QUICK COMPARISON for {test_file.replace('_test.csv', '')}:\")\n",
    "    print(f\"   Accuracy difference: {acc_diff:+.2f}%\")\n",
    "    print(f\"   F1-Score difference: {f1_diff:+.2f}%\")\n",
    "    print(f\"{'â”€'*70}\")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"âœ… Ð‘Ò®Ð¥ TEST DATA Ò®ÐÐ­Ð›Ð­Ð“Ð”Ð›Ð­Ð­!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ff044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Plot comprehensive comparison\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: Ð¥Ð°Ñ€ÑŒÑ†ÑƒÑƒÐ»Ð°Ð»Ñ‚Ñ‹Ð½ Ð³Ñ€Ð°Ñ„Ð¸Ðº Ð·ÑƒÑ€Ð¶ Ð±Ð°Ð¹Ð½Ð°...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "plot_comparison_results(improved_results, baseline_results, test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc76618c",
   "metadata": {},
   "source": [
    "### ðŸ“Š Detailed Per-Pair Confusion Matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e10bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed confusion matrices for each currency pair\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "label_names = ['S.Sell', 'Sell', 'Hold', 'Buy', 'S.Buy']\n",
    "\n",
    "for i, (test_file, imp_res, base_res) in enumerate(zip(test_files, improved_results, baseline_results)):\n",
    "    pair_name = test_file.replace('_test.csv', '')\n",
    "    \n",
    "    # Improved model confusion matrix\n",
    "    ax_imp = axes[i*2]\n",
    "    sns.heatmap(imp_res['confusion_matrix'], annot=True, fmt='d', cmap='Greens',\n",
    "                xticklabels=label_names, yticklabels=label_names,\n",
    "                cbar_kws={'label': 'Count'}, ax=ax_imp)\n",
    "    ax_imp.set_title(f'ðŸŸ¢ {pair_name} - Improved\\nAcc: {imp_res[\"accuracy\"]*100:.1f}%', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    ax_imp.set_ylabel('True', fontsize=10, fontweight='bold')\n",
    "    ax_imp.set_xlabel('Predicted', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Baseline model confusion matrix\n",
    "    ax_base = axes[i*2 + 1]\n",
    "    sns.heatmap(base_res['confusion_matrix'], annot=True, fmt='d', cmap='Reds',\n",
    "                xticklabels=label_names, yticklabels=label_names,\n",
    "                cbar_kws={'label': 'Count'}, ax=ax_base)\n",
    "    ax_base.set_title(f'ðŸ”´ {pair_name} - Baseline\\nAcc: {base_res[\"accuracy\"]*100:.1f}%', \n",
    "                      fontsize=12, fontweight='bold')\n",
    "    ax_base.set_ylabel('True', fontsize=10, fontweight='bold')\n",
    "    ax_base.set_xlabel('Predicted', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle('ðŸ“Š CONFUSION MATRICES - ALL CURRENCY PAIRS', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Ð‘Ò¯Ñ… currency pair-Ñ‹Ð½ confusion matrices Ñ…Ð°Ñ€ÑƒÑƒÐ»Ð°Ð³Ð´Ð»Ð°Ð°!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55b91e6",
   "metadata": {},
   "source": [
    "### ðŸ“ˆ Final Summary Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710cad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed summary table\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for test_file, imp_res, base_res in zip(test_files, improved_results, baseline_results):\n",
    "    pair = test_file.replace('_test.csv', '')\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Currency Pair': pair,\n",
    "        'Improved Acc (%)': f\"{imp_res['accuracy']*100:.2f}\",\n",
    "        'Baseline Acc (%)': f\"{base_res['accuracy']*100:.2f}\",\n",
    "        'Acc Improvement': f\"{(imp_res['accuracy'] - base_res['accuracy'])*100:+.2f}%\",\n",
    "        'Improved F1': f\"{imp_res['f1']:.3f}\",\n",
    "        'Baseline F1': f\"{base_res['f1']:.3f}\",\n",
    "        'F1 Improvement': f\"{(imp_res['f1'] - base_res['f1']):.3f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Add average row\n",
    "avg_row = {\n",
    "    'Currency Pair': 'ðŸ“Š AVERAGE',\n",
    "    'Improved Acc (%)': f\"{np.mean([r['accuracy']*100 for r in improved_results]):.2f}\",\n",
    "    'Baseline Acc (%)': f\"{np.mean([r['accuracy']*100 for r in baseline_results]):.2f}\",\n",
    "    'Acc Improvement': f\"{np.mean([(i['accuracy'] - b['accuracy'])*100 for i, b in zip(improved_results, baseline_results)]):+.2f}%\",\n",
    "    'Improved F1': f\"{np.mean([r['f1'] for r in improved_results]):.3f}\",\n",
    "    'Baseline F1': f\"{np.mean([r['f1'] for r in baseline_results]):.3f}\",\n",
    "    'F1 Improvement': f\"{np.mean([i['f1'] - b['f1'] for i, b in zip(improved_results, baseline_results)]):.3f}\"\n",
    "}\n",
    "summary_df = pd.concat([summary_df, pd.DataFrame([avg_row])], ignore_index=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ðŸ“Š Ð­Ð¦Ð¡Ð˜Ð™Ð Ð¢ÐÐ™Ð›ÐÐ - FINAL SUMMARY REPORT\")\n",
    "print(\"=\"*100)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Accuracy table\n",
    "acc_data = summary_df[['Currency Pair', 'Improved Acc (%)', 'Baseline Acc (%)', 'Acc Improvement']].iloc[:-1]\n",
    "colors = [['#e8f5e9' if i % 2 == 0 else '#ffffff'] * 4 for i in range(len(acc_data))]\n",
    "\n",
    "ax1.axis('tight')\n",
    "ax1.axis('off')\n",
    "table1 = ax1.table(cellText=acc_data.values,\n",
    "                   colLabels=acc_data.columns,\n",
    "                   cellLoc='center',\n",
    "                   loc='center',\n",
    "                   cellColours=colors,\n",
    "                   colColours=['#4caf50']*4)\n",
    "table1.auto_set_font_size(False)\n",
    "table1.set_fontsize(10)\n",
    "table1.scale(1, 2)\n",
    "ax1.set_title('ðŸŽ¯ ACCURACY COMPARISON', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# F1-Score table\n",
    "f1_data = summary_df[['Currency Pair', 'Improved F1', 'Baseline F1', 'F1 Improvement']].iloc[:-1]\n",
    "colors = [['#e3f2fd' if i % 2 == 0 else '#ffffff'] * 4 for i in range(len(f1_data))]\n",
    "\n",
    "ax2.axis('tight')\n",
    "ax2.axis('off')\n",
    "table2 = ax2.table(cellText=f1_data.values,\n",
    "                   colLabels=f1_data.columns,\n",
    "                   cellLoc='center',\n",
    "                   loc='center',\n",
    "                   cellColours=colors,\n",
    "                   colColours=['#2196f3']*4)\n",
    "table2.auto_set_font_size(False)\n",
    "table2.set_fontsize(10)\n",
    "table2.scale(1, 2)\n",
    "ax2.set_title('ðŸ“Š F1-SCORE COMPARISON', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final verdict\n",
    "avg_acc_improvement = np.mean([(i['accuracy'] - b['accuracy'])*100 for i, b in zip(improved_results, baseline_results)])\n",
    "avg_f1_improvement = np.mean([i['f1'] - b['f1'] for i, b in zip(improved_results, baseline_results)])\n",
    "\n",
    "print(\"\\n\\n\" + \"ðŸ†\"*35)\n",
    "print(\"                    Ð­Ð¦Ð¡Ð˜Ð™Ð Ð”Ò®Ð“ÐÐ­Ð›Ð¢ - FINAL VERDICT\")\n",
    "print(\"ðŸ†\"*35)\n",
    "\n",
    "if avg_acc_improvement > 5:\n",
    "    verdict = \"ðŸŒŸ ÐœÐÐ¨ Ð¡ÐÐ™Ð Ð¡ÐÐ™Ð–Ð˜Ð Ð¡ÐÐ! Improved model Ð½ÑŒ Ó©Ð¼Ð½Ó©Ñ… model-Ð¾Ð¾Ñ Ñ…Ð°Ð¼Ð°Ð°Ð³Ò¯Ð¹ ÑÐ°Ð¹Ð½!\"\n",
    "elif avg_acc_improvement > 2:\n",
    "    verdict = \"âœ… Ð¡ÐÐ™Ð–Ð˜Ð Ð¡ÐÐ! Improved model Ð½ÑŒ Ñ‚Ð¾Ð´Ð¾Ñ€Ñ…Ð¾Ð¹ ÑÐ°Ð¹Ð¶Ñ€Ð°Ð» Ñ…Ð°Ñ€ÑƒÑƒÐ»Ð¶ Ð±Ð°Ð¹Ð½Ð°.\"\n",
    "elif avg_acc_improvement > 0:\n",
    "    verdict = \"âš ï¸ Ð‘ÐÐ“Ð Ð—Ð­Ð Ð­Ð“ Ð¡ÐÐ™Ð–Ð˜Ð Ð¡ÐÐ. ÐžÐ´Ð¾Ð¾Ð³Ð¸Ð¹Ð½ ÑÐ°Ð¹Ð¶Ñ€ÑƒÑƒÐ»Ð°Ð»Ñ‚ Ñ‚Ð¸Ð¹Ð¼ Ñ‡ Ð¸Ñ… Ð±Ð¸Ñˆ.\"\n",
    "else:\n",
    "    verdict = \"âŒ ÐœÐ£Ð£Ð”Ð¡ÐÐ. Ð­Ð½Ñ Ð½ÑŒ Ñ‚Ð¾Ñ…Ð¸Ð¾Ð»Ð´Ð¾Ñ… Ð±Ð¾Ð»Ð¾Ð¼Ð¶Ñ‚Ð¾Ð¹, hyperparameter-Ð¸Ð¹Ð³ Ð·Ð°ÑÐ°Ñ… Ñ…ÑÑ€ÑÐ³Ñ‚ÑÐ¹.\"\n",
    "\n",
    "print(f\"\\n{verdict}\")\n",
    "print(f\"\\nðŸ“ˆ Average Accuracy Improvement: {avg_acc_improvement:+.2f}%\")\n",
    "print(f\"ðŸ“ˆ Average F1-Score Improvement: {avg_f1_improvement:+.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"ðŸ†\"*35)\n",
    "print(\"\\nâœ… Ð‘Ò®Ð¥ Ò®ÐÐ­Ð›Ð“Ð­Ð­ Ð”Ð£Ð£Ð¡Ð›ÐÐ!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
