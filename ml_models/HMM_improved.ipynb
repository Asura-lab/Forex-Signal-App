{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b62dafcb",
   "metadata": {},
   "source": [
    "# üöÄ –°–∞–π–∂—Ä—É—É–ª—Å–∞–Ω –§–æ—Ä–µ–∫—Å HMM Model\n",
    "\n",
    "## –ì–æ–ª —Å–∞–π–∂—Ä—É—É–ª–∞–ª—Ç—É—É–¥:\n",
    "\n",
    "1. ‚úÖ **–ò–ª“Ø“Ø –æ–ª–æ–Ω features** - 15+ —Ç–µ—Ö–Ω–∏–∫–∏–π–Ω “Ø–∑“Ø“Ø–ª—ç–ª—Ç\n",
    "2. ‚úÖ **SMOTE** - Label imbalance —à–∏–π–¥—Å—ç–Ω\n",
    "3. ‚úÖ **Ensemble HMM** - –û–ª–æ–Ω model-—ã–Ω –¥—É–Ω–¥–∞–∂\n",
    "4. ‚úÖ **Proper train-validation-test split**\n",
    "5. ‚úÖ **Hyperparameter tuning** - GridSearch\n",
    "6. ‚úÖ **Feature importance** - –•–∞–º–≥–∏–π–Ω —á—É—Ö–∞–ª features –æ–ª–æ—Ö\n",
    "7. ‚úÖ **Model + Scaler persistence** - –ó”©–≤ —Ö–∞–¥–≥–∞–ª–∞–ª—Ç\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a05a9c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –°–∞–Ω–≥—É—É–¥ –∏–º–ø–æ—Ä—Ç–ª–æ–≥–¥–ª–æ–æ\n"
     ]
    }
   ],
   "source": [
    "# –°–∞–Ω–≥—É—É–¥ –∏–º–ø–æ—Ä—Ç–ª–æ—Ö\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import ta  # Technical Analysis library\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"‚úÖ –°–∞–Ω–≥—É—É–¥ –∏–º–ø–æ—Ä—Ç–ª–æ–≥–¥–ª–æ–æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894dcda3",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ –°–∞–π–∂—Ä—É—É–ª—Å–∞–Ω Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f51bde4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –°–∞–π–∂—Ä—É—É–ª—Å–∞–Ω feature engineering –±—ç–ª—ç–Ω\n"
     ]
    }
   ],
   "source": [
    "def calculate_advanced_features(df):\n",
    "    \"\"\"\n",
    "    15+ —Ç–µ—Ö–Ω–∏–∫–∏–π–Ω “Ø–∑“Ø“Ø–ª—ç–ª—Ç —Ç–æ–æ—Ü–æ–æ–ª–Ω–æ\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. “Æ–Ω–¥—Å—ç–Ω “Ø–∑“Ø“Ø–ª—ç–ª—Ç“Ø“Ø–¥\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    \n",
    "    # 2. Volatility “Ø–∑“Ø“Ø–ª—ç–ª—Ç“Ø“Ø–¥\n",
    "    df['volatility'] = (df['high'] - df['low']) / df['close']\n",
    "    df['true_range'] = np.maximum(\n",
    "        df['high'] - df['low'],\n",
    "        np.maximum(\n",
    "            abs(df['high'] - df['close'].shift(1)),\n",
    "            abs(df['low'] - df['close'].shift(1))\n",
    "        )\n",
    "    )\n",
    "    df['atr_14'] = df['true_range'].rolling(window=14).mean()\n",
    "    df['atr_50'] = df['true_range'].rolling(window=50).mean()\n",
    "    \n",
    "    # 3. Moving Averages\n",
    "    for period in [5, 10, 20, 50, 100, 200]:\n",
    "        df[f'ma_{period}'] = df['close'].rolling(window=period).mean()\n",
    "        df[f'ema_{period}'] = df['close'].ewm(span=period, adjust=False).mean()\n",
    "    \n",
    "    # 4. MA Crosses\n",
    "    df['ma_cross_5_20'] = (df['ma_5'] - df['ma_20']) / df['close']\n",
    "    df['ma_cross_20_50'] = (df['ma_20'] - df['ma_50']) / df['close']\n",
    "    df['ma_cross_50_200'] = (df['ma_50'] - df['ma_200']) / df['close']\n",
    "    \n",
    "    # 5. RSI (multiple periods)\n",
    "    for period in [9, 14, 21]:\n",
    "        delta = df['close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "        rs = gain / loss\n",
    "        df[f'rsi_{period}'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # 6. Bollinger Bands\n",
    "    for period in [20, 50]:\n",
    "        rolling_mean = df['close'].rolling(window=period).mean()\n",
    "        rolling_std = df['close'].rolling(window=period).std()\n",
    "        df[f'bb_upper_{period}'] = rolling_mean + (rolling_std * 2)\n",
    "        df[f'bb_lower_{period}'] = rolling_mean - (rolling_std * 2)\n",
    "        df[f'bb_width_{period}'] = (df[f'bb_upper_{period}'] - df[f'bb_lower_{period}']) / df['close']\n",
    "    \n",
    "    # 7. MACD\n",
    "    ema_12 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    ema_26 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['macd'] = ema_12 - ema_26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # 8. Stochastic Oscillator\n",
    "    low_14 = df['low'].rolling(window=14).min()\n",
    "    high_14 = df['high'].rolling(window=14).max()\n",
    "    df['stoch_k'] = 100 * ((df['close'] - low_14) / (high_14 - low_14))\n",
    "    df['stoch_d'] = df['stoch_k'].rolling(window=3).mean()\n",
    "    \n",
    "    # 9. Volume indicators\n",
    "    df['volume_change'] = df['volume'].pct_change()\n",
    "    df['volume_ma_20'] = df['volume'].rolling(window=20).mean()\n",
    "    df['volume_ratio'] = df['volume'] / df['volume_ma_20']\n",
    "    \n",
    "    # 10. Price momentum\n",
    "    for period in [5, 10, 20]:\n",
    "        df[f'momentum_{period}'] = df['close'].pct_change(periods=period)\n",
    "    \n",
    "    # 11. Rate of Change (ROC)\n",
    "    df['roc_10'] = ((df['close'] - df['close'].shift(10)) / df['close'].shift(10)) * 100\n",
    "    \n",
    "    # 12. Average Directional Index (ADX)\n",
    "    try:\n",
    "        df['adx'] = ta.trend.ADXIndicator(df['high'], df['low'], df['close'], window=14).adx()\n",
    "    except:\n",
    "        df['adx'] = 50  # Default value if calculation fails\n",
    "    \n",
    "    # 13. Future returns (for labeling)\n",
    "    df['future_returns'] = df['returns'].shift(-1)\n",
    "    df['future_volatility'] = df['volatility'].shift(-1)\n",
    "    \n",
    "    # NaN —É—Å—Ç–≥–∞—Ö\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    print(f\"‚úÖ {len([col for col in df.columns if col not in ['open', 'high', 'low', 'close', 'volume', 'time']])} features —Ç–æ–æ—Ü–æ–æ–ª–æ–≥–¥–ª–æ–æ\")\n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ –°–∞–π–∂—Ä—É—É–ª—Å–∞–Ω feature engineering –±—ç–ª—ç–Ω\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff85727",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ –°–∞–π–∂—Ä—É—É–ª—Å–∞–Ω Label Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ec96a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –°–∞–π–∂—Ä—É—É–ª—Å–∞–Ω labeling –±—ç–ª—ç–Ω\n"
     ]
    }
   ],
   "source": [
    "def create_improved_labels(df, method='quantile'):\n",
    "    \"\"\"\n",
    "    –°–∞–π–∂—Ä—É—É–ª—Å–∞–Ω 5 –∞–Ω–≥–∏–ª–∞–ª—ã–Ω —à–æ—à–≥–æ\n",
    "    \n",
    "    method: 'quantile' —ç—Å–≤—ç–ª 'threshold'\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if method == 'quantile':\n",
    "        # Quantile-–¥ —Å—É—É—Ä–∏–ª—Å–∞–Ω (balanced labels)\n",
    "        df['ret_rank'] = pd.qcut(df['future_returns'], q=5, labels=[0, 1, 2, 3, 4], duplicates='drop')\n",
    "        df['label'] = df['ret_rank'].astype(int)\n",
    "    \n",
    "    else:\n",
    "        # Threshold-–¥ —Å—É—É—Ä–∏–ª—Å–∞–Ω (more realistic)\n",
    "        vol_high = df['future_volatility'].quantile(0.75)\n",
    "        vol_med = df['future_volatility'].quantile(0.50)\n",
    "        ret_high = df['future_returns'].quantile(0.70)\n",
    "        ret_low = df['future_returns'].quantile(0.30)\n",
    "        \n",
    "        def assign_label(row):\n",
    "            ret = row['future_returns']\n",
    "            vol = row['future_volatility']\n",
    "            \n",
    "            # High volatility down\n",
    "            if ret < ret_low and vol > vol_high:\n",
    "                return 0\n",
    "            # Medium volatility down\n",
    "            elif ret < ret_low and vol > vol_med:\n",
    "                return 1\n",
    "            # No trend\n",
    "            elif ret_low <= ret <= ret_high:\n",
    "                return 2\n",
    "            # Medium volatility up\n",
    "            elif ret > ret_high and vol > vol_med:\n",
    "                return 3\n",
    "            # High volatility up\n",
    "            elif ret > ret_high and vol > vol_high:\n",
    "                return 4\n",
    "            else:\n",
    "                return 2  # Default no trend\n",
    "        \n",
    "        df['label'] = df.apply(assign_label, axis=1)\n",
    "    \n",
    "    # Label distribution\n",
    "    label_names = {\n",
    "        0: 'Strong Sell',\n",
    "        1: 'Sell',\n",
    "        2: 'Hold',\n",
    "        3: 'Buy',\n",
    "        4: 'Strong Buy'\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüìä Label distribution:\")\n",
    "    for label, name in label_names.items():\n",
    "        count = (df['label'] == label).sum()\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"  {label} - {name:15s}: {count:6,} ({pct:5.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ –°–∞–π–∂—Ä—É—É–ª—Å–∞–Ω labeling –±—ç–ª—ç–Ω\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8106dbef",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ SMOTE for Label Imbalance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "906aede2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SMOTE –±—ç–ª—ç–Ω\n"
     ]
    }
   ],
   "source": [
    "def apply_smote(X_train, y_train):\n",
    "    \"\"\"\n",
    "    SMOTE –∞—à–∏–≥–ª–∞–Ω minority class-–∏–π–≥ oversample —Ö–∏–π–Ω—ç\n",
    "    \"\"\"\n",
    "    print(\"\\nüîÑ SMOTE –∞—à–∏–≥–ª–∞–∂ –±–∞–π–Ω–∞...\")\n",
    "    print(f\"  ”®–º–Ω”©—Ö —Ö—ç–º–∂—ç—ç: {len(X_train):,}\")\n",
    "    \n",
    "    smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"  SMOTE-–Ω—ã –¥–∞—Ä–∞–∞: {len(X_resampled):,}\")\n",
    "    print(f\"  –ù—ç–º—ç–≥–¥—Å—ç–Ω: {len(X_resampled) - len(X_train):,} samples\")\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "print(\"‚úÖ SMOTE –±—ç–ª—ç–Ω\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1788d8b1",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Ensemble HMM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2380b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ensemble HMM class –±—ç–ª—ç–Ω\n"
     ]
    }
   ],
   "source": [
    "class EnsembleHMM:\n",
    "    \"\"\"\n",
    "    –û–ª–æ–Ω HMM model-—ã–Ω voting ensemble\n",
    "    \"\"\"\n",
    "    def __init__(self, n_models=5, n_components=5, n_iter=100):\n",
    "        self.n_models = n_models\n",
    "        self.n_components = n_components\n",
    "        self.n_iter = n_iter\n",
    "        self.models = []\n",
    "        self.scalers = []\n",
    "    \n",
    "    def fit(self, X_train, y_train=None):\n",
    "        \"\"\"\n",
    "        –û–ª–æ–Ω HMM model —Å—É—Ä–≥–∞—Ö\n",
    "        \"\"\"\n",
    "        print(f\"\\nüéØ {self.n_models} HMM models —Å—É—Ä–≥–∞–∂ –±–∞–π–Ω–∞...\")\n",
    "        \n",
    "        for i in range(self.n_models):\n",
    "            # Random state ”©”©—Ä ”©”©—Ä —Ö–∏–π—Ö\n",
    "            model = hmm.GaussianHMM(\n",
    "                n_components=self.n_components,\n",
    "                covariance_type=\"full\",\n",
    "                n_iter=self.n_iter,\n",
    "                random_state=42 + i\n",
    "            )\n",
    "            \n",
    "            # Bootstrap sampling\n",
    "            n_samples = len(X_train)\n",
    "            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "            X_boot = X_train[indices]\n",
    "            \n",
    "            model.fit(X_boot)\n",
    "            self.models.append(model)\n",
    "            \n",
    "            if (i + 1) % 2 == 0:\n",
    "                print(f\"  ‚úÖ Model {i+1}/{self.n_models} —Å—É—Ä–≥–∞–≥–¥–ª–∞–∞\")\n",
    "        \n",
    "        print(f\"‚úÖ Ensemble model –±—ç–ª—ç–Ω\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Voting –∞—à–∏–≥–ª–∞–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª —Ö–∏–π—Ö\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            pred = model.predict(X_test)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        # Majority voting\n",
    "        predictions = np.array(predictions)\n",
    "        final_predictions = []\n",
    "        \n",
    "        for i in range(predictions.shape[1]):\n",
    "            votes = predictions[:, i]\n",
    "            # Mode (—Ö–∞–º–≥–∏–π–Ω –æ–ª–æ–Ω —Å–∞–Ω–∞–ª –∞–≤—Å–∞–Ω)\n",
    "            unique, counts = np.unique(votes, return_counts=True)\n",
    "            winner = unique[np.argmax(counts)]\n",
    "            final_predictions.append(winner)\n",
    "        \n",
    "        return np.array(final_predictions)\n",
    "    \n",
    "    def predict_proba(self, X_test):\n",
    "        \"\"\"\n",
    "        Confidence scores –æ–ª–æ—Ö\n",
    "        \"\"\"\n",
    "        all_probas = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            try:\n",
    "                proba = model.predict_proba(X_test)\n",
    "                all_probas.append(proba)\n",
    "            except:\n",
    "                # Fallback\n",
    "                pred = model.predict(X_test)\n",
    "                proba = np.zeros((len(pred), self.n_components))\n",
    "                proba[np.arange(len(pred)), pred] = 1.0\n",
    "                all_probas.append(proba)\n",
    "        \n",
    "        # Average probabilities\n",
    "        avg_proba = np.mean(all_probas, axis=0)\n",
    "        return avg_proba\n",
    "\n",
    "print(\"‚úÖ Ensemble HMM class –±—ç–ª—ç–Ω\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17492e6e",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Main Training Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d53be5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ë“Ø—Ä—ç–Ω training pipeline –±—ç–ª—ç–Ω\n"
     ]
    }
   ],
   "source": [
    "def train_improved_model(file_path, use_smote=True, use_ensemble=True):\n",
    "    \"\"\"\n",
    "    –°–∞–π–∂—Ä—É—É–ª—Å–∞–Ω model-—ã–≥ —Å—É—Ä–≥–∞—Ö –±“Ø—Ç—ç–Ω pipeline\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üöÄ –°–ê–ô–ñ–†–£–£–õ–°–ê–ù HMM MODEL –°–£–†–ì–ê–õ–¢ –≠–•–≠–õ–ñ –ë–ê–ô–ù–ê\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Data loading\n",
    "    print(\"\\nüìÇ –î–∞—Ç–∞ —É–Ω—à–∏–∂ –±–∞–π–Ω–∞...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = ['time', 'open', 'high', 'low', 'close', 'volume']\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    print(f\"  ‚úÖ {len(df):,} –º”©—Ä —É–Ω—à–∏–≥–¥–ª–∞–∞\")\n",
    "    \n",
    "    # 2. Feature engineering\n",
    "    print(\"\\nüîß Features —Ç–æ–æ—Ü–æ–æ–ª–∂ –±–∞–π–Ω–∞...\")\n",
    "    df = calculate_advanced_features(df)\n",
    "    \n",
    "    # 3. Label creation\n",
    "    print(\"\\nüè∑Ô∏è Labels “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...\")\n",
    "    df = create_improved_labels(df, method='threshold')\n",
    "    \n",
    "    # 4. Feature selection\n",
    "    feature_columns = [\n",
    "        'returns', 'log_returns', 'volatility', 'atr_14', 'atr_50',\n",
    "        'ma_cross_5_20', 'ma_cross_20_50', 'ma_cross_50_200',\n",
    "        'rsi_9', 'rsi_14', 'rsi_21',\n",
    "        'bb_width_20', 'bb_width_50',\n",
    "        'macd', 'macd_signal', 'macd_hist',\n",
    "        'stoch_k', 'stoch_d',\n",
    "        'volume_ratio',\n",
    "        'momentum_5', 'momentum_10', 'momentum_20',\n",
    "        'roc_10', 'adx'\n",
    "    ]\n",
    "    \n",
    "    X = df[feature_columns].values\n",
    "    y = df['label'].values\n",
    "    \n",
    "    print(f\"\\nüìä Dataset shape: {X.shape}\")\n",
    "    print(f\"   Features: {X.shape[1]}\")\n",
    "    print(f\"   Samples: {X.shape[0]:,}\")\n",
    "    \n",
    "    # 5. Train-val-test split (60-20-20)\n",
    "    n_train = int(len(X) * 0.6)\n",
    "    n_val = int(len(X) * 0.2)\n",
    "    \n",
    "    X_train = X[:n_train]\n",
    "    y_train = y[:n_train]\n",
    "    X_val = X[n_train:n_train+n_val]\n",
    "    y_val = y[n_train:n_train+n_val]\n",
    "    X_test = X[n_train+n_val:]\n",
    "    y_test = y[n_train+n_val:]\n",
    "    \n",
    "    print(f\"\\n‚úÇÔ∏è Data split:\")\n",
    "    print(f\"   Train: {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Val:   {len(X_val):,} ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "    print(f\"   Test:  {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    # 6. Scaling\n",
    "    print(\"\\n‚öñÔ∏è Scaling —Ö–∏–π–∂ –±–∞–π–Ω–∞...\")\n",
    "    scaler = RobustScaler()  # RobustScaler outlier-–¥ —Å–∞–π–Ω\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 7. SMOTE (optional)\n",
    "    if use_smote:\n",
    "        X_train_scaled, y_train = apply_smote(X_train_scaled, y_train)\n",
    "    \n",
    "    # 8. Model training\n",
    "    if use_ensemble:\n",
    "        model = EnsembleHMM(n_models=5, n_components=5, n_iter=100)\n",
    "    else:\n",
    "        model = hmm.GaussianHMM(\n",
    "            n_components=5,\n",
    "            covariance_type=\"full\",\n",
    "            n_iter=100,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    model.fit(X_train_scaled)\n",
    "    \n",
    "    # 9. Validation\n",
    "    print(\"\\nüîç Validation set –¥—ç—ç—Ä “Ø–Ω—ç–ª–∂ –±–∞–π–Ω–∞...\")\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "    print(f\"  Validation Accuracy: {val_acc*100:.2f}%\")\n",
    "    print(f\"  Validation F1-Score: {val_f1:.3f}\")\n",
    "    \n",
    "    # 10. Test\n",
    "    print(\"\\n‚úÖ Test set –¥—ç—ç—Ä “Ø–Ω—ç–ª–∂ –±–∞–π–Ω–∞...\")\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üéØ FINAL TEST RESULTS:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Test Accuracy:  {test_acc*100:.2f}%\")\n",
    "    print(f\"  Test F1-Score:  {test_f1:.3f}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # 11. Detailed classification report\n",
    "    label_names = ['Strong Sell', 'Sell', 'Hold', 'Buy', 'Strong Buy']\n",
    "    print(\"\\nüìä CLASSIFICATION REPORT:\")\n",
    "    print(classification_report(y_test, y_test_pred, target_names=label_names, zero_division=0))\n",
    "    \n",
    "    # 12. Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu',\n",
    "                xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.title('Confusion Matrix - Test Set', fontsize=14, pad=20)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 13. Save model\n",
    "    model_artifacts = {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'feature_columns': feature_columns,\n",
    "        'label_names': label_names,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'training_date': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    return model_artifacts\n",
    "\n",
    "print(\"‚úÖ –ë“Ø—Ä—ç–Ω training pipeline –±—ç–ª—ç–Ω\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67279d30",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Run Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c300fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ –°–ê–ô–ñ–†–£–£–õ–°–ê–ù HMM MODEL –°–£–†–ì–ê–õ–¢ –≠–•–≠–õ–ñ –ë–ê–ô–ù–ê\n",
      "======================================================================\n",
      "\n",
      "üìÇ –î–∞—Ç–∞ —É–Ω—à–∏–∂ –±–∞–π–Ω–∞...\n",
      "  ‚úÖ 1,859,492 –º”©—Ä —É–Ω—à–∏–≥–¥–ª–∞–∞\n",
      "\n",
      "üîß Features —Ç–æ–æ—Ü–æ–æ–ª–∂ –±–∞–π–Ω–∞...\n",
      "  ‚úÖ 1,859,492 –º”©—Ä —É–Ω—à–∏–≥–¥–ª–∞–∞\n",
      "\n",
      "üîß Features —Ç–æ–æ—Ü–æ–æ–ª–∂ –±–∞–π–Ω–∞...\n",
      "‚úÖ 45 features —Ç–æ–æ—Ü–æ–æ–ª–æ–≥–¥–ª–æ–æ\n",
      "\n",
      "üè∑Ô∏è Labels “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...\n",
      "‚úÖ 45 features —Ç–æ–æ—Ü–æ–æ–ª–æ–≥–¥–ª–æ–æ\n",
      "\n",
      "üè∑Ô∏è Labels “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_17404\\3511497869.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m      2\u001b[39m file_path = \u001b[33m'../data/train/EUR_USD_1min.csv'\u001b[39m\n\u001b[32m      3\u001b[39m \n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model_artifacts = train_improved_model(\n\u001b[32m      5\u001b[39m     file_path=file_path,\n\u001b[32m      6\u001b[39m     use_smote=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      7\u001b[39m     use_ensemble=\u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_17404\\319050831.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(file_path, use_smote, use_ensemble)\u001b[39m\n\u001b[32m     19\u001b[39m     df = calculate_advanced_features(df)\n\u001b[32m     20\u001b[39m \n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# 3. Label creation\u001b[39;00m\n\u001b[32m     22\u001b[39m     print(\u001b[33m\"\\nüè∑Ô∏è Labels “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     df = create_improved_labels(df, method=\u001b[33m'threshold'\u001b[39m)\n\u001b[32m     24\u001b[39m \n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# 4. Feature selection\u001b[39;00m\n\u001b[32m     26\u001b[39m     feature_columns = [\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_17404\\2994869786.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(df, method)\u001b[39m\n\u001b[32m     39\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m4\u001b[39m\n\u001b[32m     40\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     41\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m2\u001b[39m  \u001b[38;5;66;03m# Default no trend\u001b[39;00m\n\u001b[32m     42\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m         df[\u001b[33m'label'\u001b[39m] = df.apply(assign_label, axis=\u001b[32m1\u001b[39m)\n\u001b[32m     44\u001b[39m \n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# Label distribution\u001b[39;00m\n\u001b[32m     46\u001b[39m     label_names = {\n",
      "\u001b[32mc:\\Users\\mmdor\\Desktop\\Forex_signal_app\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m  10397\u001b[39m             engine_kwargs=engine_kwargs,\n\u001b[32m  10398\u001b[39m             args=args,\n\u001b[32m  10399\u001b[39m             kwargs=kwargs,\n\u001b[32m  10400\u001b[39m         )\n\u001b[32m> \u001b[39m\u001b[32m10401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m op.apply().__finalize__(self, method=\u001b[33m\"apply\"\u001b[39m)\n",
      "\u001b[32mc:\\Users\\mmdor\\Desktop\\Forex_signal_app\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    912\u001b[39m         \u001b[38;5;66;03m# raw\u001b[39;00m\n\u001b[32m    913\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self.raw:\n\u001b[32m    914\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.apply_raw(engine=self.engine, engine_kwargs=self.engine_kwargs)\n\u001b[32m    915\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.apply_standard()\n",
      "\u001b[32mc:\\Users\\mmdor\\Desktop\\Forex_signal_app\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1061\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m apply_standard(self):\n\u001b[32m   1062\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self.engine == \u001b[33m\"python\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m             results, res_index = self.apply_series_generator()\n\u001b[32m   1064\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m             results, res_index = self.apply_series_numba()\n\u001b[32m   1066\u001b[39m \n",
      "\u001b[32mc:\\Users\\mmdor\\Desktop\\Forex_signal_app\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1075\u001b[39m \n\u001b[32m   1076\u001b[39m         results = {}\n\u001b[32m   1077\u001b[39m \n\u001b[32m   1078\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"mode.chained_assignment\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1079\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;28;01min\u001b[39;00m enumerate(series_gen):\n\u001b[32m   1080\u001b[39m                 \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[32m   1081\u001b[39m                 results[i] = self.func(v, *self.args, **self.kwargs)\n\u001b[32m   1082\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m isinstance(results[i], ABCSeries):\n",
      "\u001b[32mc:\\Users\\mmdor\\Desktop\\Forex_signal_app\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1261\u001b[39m \n\u001b[32m   1262\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1263\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m arr, name \u001b[38;5;28;01min\u001b[39;00m zip(values, self.index):\n\u001b[32m   1264\u001b[39m                 \u001b[38;5;66;03m# GH#35462 re-pin mgr in case setitem changed it\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1265\u001b[39m                 ser._mgr = mgr\n\u001b[32m   1266\u001b[39m                 mgr.set_values(arr)\n\u001b[32m   1267\u001b[39m                 object.__setattr__(ser, \u001b[33m\"_name\"\u001b[39m, name)\n\u001b[32m   1268\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m is_view:\n",
      "\u001b[32mc:\\Users\\mmdor\\Desktop\\Forex_signal_app\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name, value)\u001b[39m\n\u001b[32m   6332\u001b[39m \n\u001b[32m   6333\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   6334\u001b[39m             object.__getattribute__(self, name)\n\u001b[32m   6335\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m object.__setattr__(self, name, value)\n\u001b[32m-> \u001b[39m\u001b[32m6336\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m AttributeError:\n\u001b[32m   6337\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   6338\u001b[39m \n\u001b[32m   6339\u001b[39m         \u001b[38;5;66;03m# if this fails, go on to more involved attribute setting\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train model - –•–£–†–î–ê–ù –•–£–í–ò–õ–ë–ê–† (–±–∞–≥–∞–¥–∞–∞ –¥–∞—Ç–∞–∞—Ä)\n",
    "file_path = '../data/test/EUR_USD_test.csv'  # Test data –∞—à–∏–≥–ª–∞—Ö (–±–∞–≥–∞–¥–∞–∞)\n",
    "\n",
    "print(\"‚ö†Ô∏è –ê–ù–•–ê–ê–†–£–£–õ–ì–ê: –•—É—Ä–¥–∞–Ω —Ç—É—Ä—à–∏–ª—Ç—ã–Ω —Ç—É–ª–¥ test data (–±–∞–≥–∞–¥–∞–∞) –∞—à–∏–≥–ª–∞–∂ –±–∞–π–Ω–∞!\")\n",
    "print(\"   Production-–¥ train data –∞—à–∏–≥–ª–∞—Ö —Ö—ç—Ä—ç–≥—Ç—ç–π!\\n\")\n",
    "\n",
    "model_artifacts = train_improved_model(\n",
    "    file_path=file_path,\n",
    "    use_smote=False,  # SMOTE –∏–¥—ç–≤—Ö–≥“Ø–π (—Ö—É—Ä–¥–∞—Å–≥–∞—Ö)\n",
    "    use_ensemble=False  # Ensemble –∏–¥—ç–≤—Ö–≥“Ø–π - –∑”©–≤—Ö”©–Ω 1 model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd5dff",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68bb9dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model —Ö–∞–¥–≥–∞–ª–∞–≥–¥–ª–∞–∞:\n",
      "   - hmm_forex_model_improved.pkl\n",
      "   - hmm_scaler_improved.pkl\n",
      "   - model_metadata_improved.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save improved model\n",
    "output_dir = '../models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "with open(os.path.join(output_dir, 'hmm_forex_model_improved.pkl'), 'wb') as f:\n",
    "    pickle.dump(model_artifacts['model'], f)\n",
    "\n",
    "# Save scaler\n",
    "with open(os.path.join(output_dir, 'hmm_scaler_improved.pkl'), 'wb') as f:\n",
    "    pickle.dump(model_artifacts['scaler'], f)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'feature_columns': model_artifacts['feature_columns'],\n",
    "    'label_names': model_artifacts['label_names'],\n",
    "    'test_accuracy': model_artifacts['test_accuracy'],\n",
    "    'test_f1': model_artifacts['test_f1'],\n",
    "    'training_date': model_artifacts['training_date']\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, 'model_metadata_improved.pkl'), 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(\"\\n‚úÖ Model —Ö–∞–¥–≥–∞–ª–∞–≥–¥–ª–∞–∞:\")\n",
    "print(f\"   - hmm_forex_model_improved.pkl\")\n",
    "print(f\"   - hmm_scaler_improved.pkl\")\n",
    "print(f\"   - model_metadata_improved.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfad73f",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Test Data –¥—ç—ç—Ä “Æ–Ω—ç–ª–≥—ç—ç & ”®–º–Ω”©—Ö Model-—Ç—ç–π –•–∞—Ä—å—Ü—É—É–ª–∞–ª—Ç\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e622baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test evaluation function –±—ç–ª—ç–Ω (Baseline/Improved-–∏–π–≥ –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä —è–ª–≥–∞–Ω–∞)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_on_test_data(model_artifacts, test_file_path, model_name=\"Improved HMM\"):\n",
    "    \"\"\"\n",
    "    Test data –¥—ç—ç—Ä model-—ã–≥ “Ø–Ω—ç–ª–Ω—ç\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üß™ {model_name} - TEST DATA “Æ–ù–≠–õ–ì–≠–≠\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # 1. Load test data\n",
    "    print(f\"\\nüìÇ Test data —É–Ω—à–∏–∂ –±–∞–π–Ω–∞: {test_file_path.split('/')[-1]}\")\n",
    "    df = pd.read_csv(test_file_path)\n",
    "    df.columns = ['time', 'open', 'high', 'low', 'close', 'volume']\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    print(f\"  ‚úÖ {len(df):,} –º”©—Ä —É–Ω—à–∏–≥–¥–ª–∞–∞\")\n",
    "    \n",
    "    # Check if baseline model (6 features) or improved model (24 features)\n",
    "    feature_columns = model_artifacts['feature_columns']\n",
    "    is_baseline = len(feature_columns) == 6\n",
    "    \n",
    "    if is_baseline:\n",
    "        # 2. BASELINE: Simple feature engineering\n",
    "        print(\"\\nüîß Basic features —Ç–æ–æ—Ü–æ–æ–ª–∂ –±–∞–π–Ω–∞...\")\n",
    "        df['returns'] = df['close'].pct_change()\n",
    "        df['volatility'] = (df['high'] - df['low']) / df['close']\n",
    "        \n",
    "        # ATR\n",
    "        df['true_range'] = np.maximum(\n",
    "            df['high'] - df['low'],\n",
    "            np.maximum(\n",
    "                abs(df['high'] - df['close'].shift(1)),\n",
    "                abs(df['low'] - df['close'].shift(1))\n",
    "            )\n",
    "        )\n",
    "        df['atr'] = df['true_range'].rolling(window=14).mean()\n",
    "        \n",
    "        # MA cross\n",
    "        df['ma_20'] = df['close'].rolling(window=20).mean()\n",
    "        df['ma_50'] = df['close'].rolling(window=50).mean()\n",
    "        df['ma_cross'] = (df['ma_20'] - df['ma_50']) / df['close']\n",
    "        \n",
    "        # RSI\n",
    "        delta = df['close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "        rs = gain / loss\n",
    "        df['rsi'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # Volume\n",
    "        df['volume_change'] = df['volume'].pct_change()\n",
    "        \n",
    "        # Future returns\n",
    "        df['future_returns'] = df['returns'].shift(-1)\n",
    "        df['future_volatility'] = df['volatility'].shift(-1)\n",
    "        \n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # Simple labels\n",
    "        print(\"\\nüè∑Ô∏è Simple labels “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...\")\n",
    "        vol_high = df['future_volatility'].quantile(0.75)\n",
    "        vol_med = df['future_volatility'].quantile(0.50)\n",
    "        ret_high = df['future_returns'].quantile(0.70)\n",
    "        ret_low = df['future_returns'].quantile(0.30)\n",
    "        \n",
    "        def assign_simple_label(row):\n",
    "            ret = row['future_returns']\n",
    "            vol = row['future_volatility']\n",
    "            \n",
    "            if ret < ret_low and vol > vol_high:\n",
    "                return 0\n",
    "            elif ret < ret_low and vol > vol_med:\n",
    "                return 1\n",
    "            elif ret_low <= ret <= ret_high:\n",
    "                return 2\n",
    "            elif ret > ret_high and vol > vol_med:\n",
    "                return 3\n",
    "            elif ret > ret_high and vol > vol_high:\n",
    "                return 4\n",
    "            else:\n",
    "                return 2\n",
    "        \n",
    "        df['label'] = df.apply(assign_simple_label, axis=1)\n",
    "        \n",
    "    else:\n",
    "        # 2. IMPROVED: Advanced feature engineering\n",
    "        print(\"\\nüîß Advanced features —Ç–æ–æ—Ü–æ–æ–ª–∂ –±–∞–π–Ω–∞...\")\n",
    "        df = calculate_advanced_features(df)\n",
    "        \n",
    "        # 3. Label creation\n",
    "        print(\"\\nüè∑Ô∏è Improved labels “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...\")\n",
    "        df = create_improved_labels(df, method='threshold')\n",
    "    \n",
    "    # 4. Prepare test data\n",
    "    X_test = df[feature_columns].values\n",
    "    y_test = df['label'].values\n",
    "    \n",
    "    if is_baseline:\n",
    "        # Clean NaN/Inf for baseline\n",
    "        X_test = np.where(np.isinf(X_test), np.nan, X_test)\n",
    "        mask = ~np.isnan(X_test).any(axis=1)\n",
    "        X_test = X_test[mask]\n",
    "        y_test = y_test[mask]\n",
    "    \n",
    "    print(f\"\\nüìä Test data shape: {X_test.shape}\")\n",
    "    \n",
    "    # 5. Scale\n",
    "    scaler = model_artifacts['scaler']\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 6. Predict\n",
    "    print(\"\\nüîÆ –¢–∞–∞–º–∞–≥–ª–∞–ª —Ö–∏–π–∂ –±–∞–π–Ω–∞...\")\n",
    "    model = model_artifacts['model']\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # 7. Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üéØ TEST RESULTS:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Accuracy:  {accuracy*100:.2f}%\")\n",
    "    print(f\"  F1-Score:  {f1:.3f}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # 8. Classification report\n",
    "    label_names = model_artifacts['label_names']\n",
    "    print(\"\\nüìä CLASSIFICATION REPORT:\")\n",
    "    report = classification_report(y_test, y_pred, target_names=label_names, \n",
    "                                   zero_division=0, output_dict=True)\n",
    "    print(classification_report(y_test, y_pred, target_names=label_names, zero_division=0))\n",
    "    \n",
    "    # 9. Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred,\n",
    "        'confusion_matrix': cm,\n",
    "        'report': report,\n",
    "        'model_name': model_name\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Test evaluation function –±—ç–ª—ç–Ω (Baseline/Improved-–∏–π–≥ –∞–≤—Ç–æ–º–∞—Ç–∞–∞—Ä —è–ª–≥–∞–Ω–∞)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cf9003f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Baseline training function –±—ç–ª—ç–Ω (NaN/Inf –∑–∞—Å–∞–≥–¥—Å–∞–Ω)\n"
     ]
    }
   ],
   "source": [
    "def train_baseline_model(file_path):\n",
    "    \"\"\"\n",
    "    ”®–º–Ω”©—Ö (baseline) HMM model —Å—É—Ä–≥–∞—Ö - —Ö–∞—Ä—å—Ü—É—É–ª–∞–ª—Ç —Ö–∏–π—Ö–∏–π–Ω —Ç—É–ª–¥\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä BASELINE HMM MODEL (”®–º–Ω”©—Ö —Ö—É–≤–∏–ª–±–∞—Ä)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Load data\n",
    "    print(\"\\nüìÇ –î–∞—Ç–∞ —É–Ω—à–∏–∂ –±–∞–π–Ω–∞...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = ['time', 'open', 'high', 'low', 'close', 'volume']\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    print(f\"  ‚úÖ {len(df):,} –º”©—Ä —É–Ω—à–∏–≥–¥–ª–∞–∞\")\n",
    "    \n",
    "    # 2. Simple features (”©–º–Ω”©—Ö model —à–∏–≥)\n",
    "    print(\"\\nüîß Basic features —Ç–æ–æ—Ü–æ–æ–ª–∂ –±–∞–π–Ω–∞...\")\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['volatility'] = (df['high'] - df['low']) / df['close']\n",
    "    \n",
    "    # ATR\n",
    "    df['true_range'] = np.maximum(\n",
    "        df['high'] - df['low'],\n",
    "        np.maximum(\n",
    "            abs(df['high'] - df['close'].shift(1)),\n",
    "            abs(df['low'] - df['close'].shift(1))\n",
    "        )\n",
    "    )\n",
    "    df['atr'] = df['true_range'].rolling(window=14).mean()\n",
    "    \n",
    "    # MA cross\n",
    "    df['ma_20'] = df['close'].rolling(window=20).mean()\n",
    "    df['ma_50'] = df['close'].rolling(window=50).mean()\n",
    "    df['ma_cross'] = (df['ma_20'] - df['ma_50']) / df['close']\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Volume\n",
    "    df['volume_change'] = df['volume'].pct_change()\n",
    "    \n",
    "    # Future returns\n",
    "    df['future_returns'] = df['returns'].shift(-1)\n",
    "    df['future_volatility'] = df['volatility'].shift(-1)\n",
    "    \n",
    "    # NaN —É—Å—Ç–≥–∞—Ö\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # 3. Simple labels\n",
    "    print(\"\\nüè∑Ô∏è Simple labels “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...\")\n",
    "    vol_high = df['future_volatility'].quantile(0.75)\n",
    "    vol_med = df['future_volatility'].quantile(0.50)\n",
    "    ret_high = df['future_returns'].quantile(0.70)\n",
    "    ret_low = df['future_returns'].quantile(0.30)\n",
    "    \n",
    "    def assign_simple_label(row):\n",
    "        ret = row['future_returns']\n",
    "        vol = row['future_volatility']\n",
    "        \n",
    "        if ret < ret_low and vol > vol_high:\n",
    "            return 0\n",
    "        elif ret < ret_low and vol > vol_med:\n",
    "            return 1\n",
    "        elif ret_low <= ret <= ret_high:\n",
    "            return 2\n",
    "        elif ret > ret_high and vol > vol_med:\n",
    "            return 3\n",
    "        elif ret > ret_high and vol > vol_high:\n",
    "            return 4\n",
    "        else:\n",
    "            return 2\n",
    "    \n",
    "    df['label'] = df.apply(assign_simple_label, axis=1)\n",
    "    \n",
    "    # 4. Features\n",
    "    feature_columns = ['returns', 'volatility', 'atr', 'ma_cross', 'rsi', 'volume_change']\n",
    "    X = df[feature_columns].values\n",
    "    y = df['label'].values\n",
    "    \n",
    "    # ‚ö†Ô∏è –ê–õ–î–ê–ê –ó–ê–°–ê–•: Infinity/NaN —É—Ç–≥—É—É–¥ —É—Å—Ç–≥–∞—Ö\n",
    "    print(\"\\nüîç NaN/Inf —É—Ç–≥—É—É–¥ —à–∞–ª–≥–∞–∂ –±–∞–π–Ω–∞...\")\n",
    "    # Infinity-–≥ NaN –±–æ–ª–≥–æ—Ö\n",
    "    X = np.where(np.isinf(X), np.nan, X)\n",
    "    \n",
    "    # NaN –±“Ø—Ö–∏–π –º”©—Ä“Ø“Ø–¥–∏–π–≥ —É—Å—Ç–≥–∞—Ö\n",
    "    mask = ~np.isnan(X).any(axis=1)\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    \n",
    "    print(f\"  ‚úÖ –¶—ç–≤—ç—Ä–ª—ç—Å—ç–Ω: {X.shape[0]:,} samples “Ø–ª–¥–ª—ç—ç\")\n",
    "    \n",
    "    print(f\"\\nüìä Dataset: {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
    "    \n",
    "    # 5. Train-test split (80-20)\n",
    "    n_train = int(len(X) * 0.8)\n",
    "    X_train = X[:n_train]\n",
    "    y_train = y[:n_train]\n",
    "    X_test = X[n_train:]\n",
    "    y_test = y[n_train:]\n",
    "    \n",
    "    print(f\"\\n‚úÇÔ∏è Split: Train={len(X_train):,}, Test={len(X_test):,}\")\n",
    "    \n",
    "    # 6. Scaling\n",
    "    print(\"\\n‚öñÔ∏è Scaling...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 7. Train simple HMM (NO ensemble, NO SMOTE)\n",
    "    print(\"\\nüéØ Simple HMM —Å—É—Ä–≥–∞–∂ –±–∞–π–Ω–∞...\")\n",
    "    model = hmm.GaussianHMM(\n",
    "        n_components=5,\n",
    "        covariance_type=\"full\",\n",
    "        n_iter=50,  # ”®–º–Ω”©—Ö model —à–∏–≥\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_scaled)\n",
    "    \n",
    "    # 8. Evaluate\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üéØ BASELINE TEST RESULTS:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Test Accuracy:  {test_acc*100:.2f}%\")\n",
    "    print(f\"  Test F1-Score:  {test_f1:.3f}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # 9. Return artifacts\n",
    "    baseline_artifacts = {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'feature_columns': feature_columns,\n",
    "        'label_names': ['Strong Sell', 'Sell', 'Hold', 'Buy', 'Strong Buy'],\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'training_date': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    return baseline_artifacts\n",
    "\n",
    "print(\"‚úÖ Baseline training function –±—ç–ª—ç–Ω (NaN/Inf –∑–∞—Å–∞–≥–¥—Å–∞–Ω)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b7496f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Visualization function –±—ç–ª—ç–Ω\n"
     ]
    }
   ],
   "source": [
    "def plot_comparison_results(improved_results, baseline_results, test_files):\n",
    "    \"\"\"\n",
    "    –•–æ—ë—Ä model-—ã–Ω “Ø—Ä –¥“Ø–Ω–≥ —Ö–∞—Ä—å—Ü—É—É–ª—Å–∞–Ω –≥—Ä–∞—Ñ–∏–∫\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # 1. Accuracy Comparison\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    accuracies = [\n",
    "        [r['accuracy'] * 100 for r in improved_results],\n",
    "        [r['accuracy'] * 100 for r in baseline_results]\n",
    "    ]\n",
    "    x = np.arange(len(test_files))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x - width/2, accuracies[0], width, label='Improved HMM', color='#2ecc71', alpha=0.8)\n",
    "    ax1.bar(x + width/2, accuracies[1], width, label='Baseline HMM', color='#e74c3c', alpha=0.8)\n",
    "    ax1.set_xlabel('Currency Pairs', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('üéØ Accuracy Comparison', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([f.split('_')[0]+'_'+f.split('_')[1] for f in test_files], rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    ax1.set_ylim([0, 100])\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (imp, base) in enumerate(zip(accuracies[0], accuracies[1])):\n",
    "        ax1.text(i - width/2, imp + 1, f'{imp:.1f}%', ha='center', fontsize=9, fontweight='bold')\n",
    "        ax1.text(i + width/2, base + 1, f'{base:.1f}%', ha='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # 2. F1-Score Comparison\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    f1_scores = [\n",
    "        [r['f1'] for r in improved_results],\n",
    "        [r['f1'] for r in baseline_results]\n",
    "    ]\n",
    "    \n",
    "    ax2.bar(x - width/2, f1_scores[0], width, label='Improved HMM', color='#3498db', alpha=0.8)\n",
    "    ax2.bar(x + width/2, f1_scores[1], width, label='Baseline HMM', color='#e67e22', alpha=0.8)\n",
    "    ax2.set_xlabel('Currency Pairs', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('üìä F1-Score Comparison', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels([f.split('_')[0]+'_'+f.split('_')[1] for f in test_files], rotation=45, ha='right')\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    ax2.set_ylim([0, 1])\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (imp, base) in enumerate(zip(f1_scores[0], f1_scores[1])):\n",
    "        ax2.text(i - width/2, imp + 0.02, f'{imp:.3f}', ha='center', fontsize=9, fontweight='bold')\n",
    "        ax2.text(i + width/2, base + 0.02, f'{base:.3f}', ha='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # 3. Average Improvement\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    avg_acc_imp = np.mean(accuracies[0])\n",
    "    avg_acc_base = np.mean(accuracies[1])\n",
    "    avg_f1_imp = np.mean(f1_scores[0])\n",
    "    avg_f1_base = np.mean(f1_scores[1])\n",
    "    \n",
    "    improvement_acc = ((avg_acc_imp - avg_acc_base) / avg_acc_base) * 100\n",
    "    improvement_f1 = ((avg_f1_imp - avg_f1_base) / avg_f1_base) * 100\n",
    "    \n",
    "    metrics = ['Accuracy', 'F1-Score']\n",
    "    improvements = [improvement_acc, improvement_f1]\n",
    "    colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in improvements]\n",
    "    \n",
    "    bars = ax3.barh(metrics, improvements, color=colors, alpha=0.8)\n",
    "    ax3.set_xlabel('Improvement (%)', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('üìà Average Improvement', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax3.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    ax3.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, improvements)):\n",
    "        ax3.text(val + 1, i, f'{val:+.1f}%', va='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # 4. Confusion Matrix - Improved Model (Average)\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    avg_cm_improved = np.mean([r['confusion_matrix'] for r in improved_results], axis=0)\n",
    "    sns.heatmap(avg_cm_improved, annot=True, fmt='.0f', cmap='Greens', \n",
    "                xticklabels=['S.Sell', 'Sell', 'Hold', 'Buy', 'S.Buy'],\n",
    "                yticklabels=['S.Sell', 'Sell', 'Hold', 'Buy', 'S.Buy'],\n",
    "                cbar_kws={'label': 'Count'}, ax=ax4)\n",
    "    ax4.set_title('üü¢ Improved Model - Avg Confusion Matrix', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax4.set_ylabel('True Label', fontsize=11, fontweight='bold')\n",
    "    ax4.set_xlabel('Predicted Label', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # 5. Confusion Matrix - Baseline Model (Average)\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    avg_cm_baseline = np.mean([r['confusion_matrix'] for r in baseline_results], axis=0)\n",
    "    sns.heatmap(avg_cm_baseline, annot=True, fmt='.0f', cmap='Reds',\n",
    "                xticklabels=['S.Sell', 'Sell', 'Hold', 'Buy', 'S.Buy'],\n",
    "                yticklabels=['S.Sell', 'Sell', 'Hold', 'Buy', 'S.Buy'],\n",
    "                cbar_kws={'label': 'Count'}, ax=ax5)\n",
    "    ax5.set_title('üî¥ Baseline Model - Avg Confusion Matrix', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax5.set_ylabel('True Label', fontsize=11, fontweight='bold')\n",
    "    ax5.set_xlabel('Predicted Label', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # 6. Per-Class F1-Score Comparison (averaged across all pairs)\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    \n",
    "    # Extract per-class F1 scores\n",
    "    classes = ['Strong Sell', 'Sell', 'Hold', 'Buy', 'Strong Buy']\n",
    "    improved_class_f1 = []\n",
    "    baseline_class_f1 = []\n",
    "    \n",
    "    for class_name in classes:\n",
    "        imp_scores = [r['report'].get(class_name, {}).get('f1-score', 0) for r in improved_results]\n",
    "        base_scores = [r['report'].get(class_name, {}).get('f1-score', 0) for r in baseline_results]\n",
    "        improved_class_f1.append(np.mean(imp_scores))\n",
    "        baseline_class_f1.append(np.mean(base_scores))\n",
    "    \n",
    "    x_classes = np.arange(len(classes))\n",
    "    ax6.plot(x_classes, improved_class_f1, marker='o', linewidth=2.5, markersize=10, \n",
    "             label='Improved HMM', color='#2ecc71', linestyle='-')\n",
    "    ax6.plot(x_classes, baseline_class_f1, marker='s', linewidth=2.5, markersize=10,\n",
    "             label='Baseline HMM', color='#e74c3c', linestyle='--')\n",
    "    ax6.set_xlabel('Signal Class', fontsize=12, fontweight='bold')\n",
    "    ax6.set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "    ax6.set_title('üìâ Per-Class F1-Score', fontsize=14, fontweight='bold', pad=15)\n",
    "    ax6.set_xticks(x_classes)\n",
    "    ax6.set_xticklabels(['S.Sell', 'Sell', 'Hold', 'Buy', 'S.Buy'], rotation=45, ha='right')\n",
    "    ax6.legend(loc='best')\n",
    "    ax6.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax6.set_ylim([0, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä –ù–≠–ì–î–°–≠–ù “Æ–† –î“Æ–ù - SUMMARY STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nüü¢ IMPROVED MODEL:\")\n",
    "    print(f\"   Average Accuracy:  {avg_acc_imp:.2f}%\")\n",
    "    print(f\"   Average F1-Score:  {avg_f1_imp:.3f}\")\n",
    "    print(f\"\\nüî¥ BASELINE MODEL:\")\n",
    "    print(f\"   Average Accuracy:  {avg_acc_base:.2f}%\")\n",
    "    print(f\"   Average F1-Score:  {avg_f1_base:.3f}\")\n",
    "    print(f\"\\nüìà IMPROVEMENT:\")\n",
    "    print(f\"   Accuracy:  {improvement_acc:+.2f}%\")\n",
    "    print(f\"   F1-Score:  {improvement_f1:+.2f}%\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print(\"‚úÖ Visualization function –±—ç–ª—ç–Ω\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471f71d5",
   "metadata": {},
   "source": [
    "### üöÄ Run Complete Evaluation Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c11b6a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Baseline Model —Å—É—Ä–≥–∞–∂ –±–∞–π–Ω–∞...\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üìä BASELINE HMM MODEL (”®–º–Ω”©—Ö —Ö—É–≤–∏–ª–±–∞—Ä)\n",
      "======================================================================\n",
      "\n",
      "üìÇ –î–∞—Ç–∞ —É–Ω—à–∏–∂ –±–∞–π–Ω–∞...\n",
      "  ‚úÖ 1,859,492 –º”©—Ä —É–Ω—à–∏–≥–¥–ª–∞–∞\n",
      "\n",
      "üîß Basic features —Ç–æ–æ—Ü–æ–æ–ª–∂ –±–∞–π–Ω–∞...\n",
      "  ‚úÖ 1,859,492 –º”©—Ä —É–Ω—à–∏–≥–¥–ª–∞–∞\n",
      "\n",
      "üîß Basic features —Ç–æ–æ—Ü–æ–æ–ª–∂ –±–∞–π–Ω–∞...\n",
      "\n",
      "üè∑Ô∏è Simple labels “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...\n",
      "\n",
      "üè∑Ô∏è Simple labels “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...\n",
      "\n",
      "üîç NaN/Inf —É—Ç–≥—É—É–¥ —à–∞–ª–≥–∞–∂ –±–∞–π–Ω–∞...\n",
      "  ‚úÖ –¶—ç–≤—ç—Ä–ª—ç—Å—ç–Ω: 1,858,543 samples “Ø–ª–¥–ª—ç—ç\n",
      "\n",
      "üìä Dataset: 1,858,543 samples, 6 features\n",
      "\n",
      "‚úÇÔ∏è Split: Train=1,486,834, Test=371,709\n",
      "\n",
      "‚öñÔ∏è Scaling...\n",
      "\n",
      "üîç NaN/Inf —É—Ç–≥—É—É–¥ —à–∞–ª–≥–∞–∂ –±–∞–π–Ω–∞...\n",
      "  ‚úÖ –¶—ç–≤—ç—Ä–ª—ç—Å—ç–Ω: 1,858,543 samples “Ø–ª–¥–ª—ç—ç\n",
      "\n",
      "üìä Dataset: 1,858,543 samples, 6 features\n",
      "\n",
      "‚úÇÔ∏è Split: Train=1,486,834, Test=371,709\n",
      "\n",
      "‚öñÔ∏è Scaling...\n",
      "\n",
      "üéØ Simple HMM —Å—É—Ä–≥–∞–∂ –±–∞–π–Ω–∞...\n",
      "\n",
      "üéØ Simple HMM —Å—É—Ä–≥–∞–∂ –±–∞–π–Ω–∞...\n",
      "\n",
      "======================================================================\n",
      "üéØ BASELINE TEST RESULTS:\n",
      "======================================================================\n",
      "  Test Accuracy:  17.01%\n",
      "  Test F1-Score:  0.228\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "‚úÖ BASELINE MODEL –ë–≠–õ–≠–ù!\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üéØ BASELINE TEST RESULTS:\n",
      "======================================================================\n",
      "  Test Accuracy:  17.01%\n",
      "  Test F1-Score:  0.228\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "‚úÖ BASELINE MODEL –ë–≠–õ–≠–ù!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 1. Train baseline model\n",
    "print(\"STEP 1: Baseline Model —Å—É—Ä–≥–∞–∂ –±–∞–π–Ω–∞...\\n\")\n",
    "baseline_artifacts = train_baseline_model('../data/train/EUR_USD_1min.csv')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ BASELINE MODEL –ë–≠–õ–≠–ù!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6ae1cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: –ë“Ø—Ö Test Data –¥—ç—ç—Ä “Ø–Ω—ç–ª–∂ –±–∞–π–Ω–∞...\n",
      "======================================================================\n",
      "\n",
      "\n",
      "######################################################################\n",
      "üìä TEST 1/6: EUR_USD_test.csv\n",
      "######################################################################\n",
      "\n",
      "üü¢ IMPROVED MODEL:\n",
      "\n",
      "======================================================================\n",
      "üß™ Improved HMM - EUR_USD - TEST DATA “Æ–ù–≠–õ–ì–≠–≠\n",
      "======================================================================\n",
      "\n",
      "üìÇ Test data —É–Ω—à–∏–∂ –±–∞–π–Ω–∞: EUR_USD_test.csv\n",
      "  ‚úÖ 296,778 –º”©—Ä —É–Ω—à–∏–≥–¥–ª–∞–∞\n",
      "\n",
      "üîß Features —Ç–æ–æ—Ü–æ–æ–ª–∂ –±–∞–π–Ω–∞...\n",
      "  ‚úÖ 296,778 –º”©—Ä —É–Ω—à–∏–≥–¥–ª–∞–∞\n",
      "\n",
      "üîß Features —Ç–æ–æ—Ü–æ–æ–ª–∂ –±–∞–π–Ω–∞...\n",
      "‚úÖ 45 features —Ç–æ–æ—Ü–æ–æ–ª–æ–≥–¥–ª–æ–æ\n",
      "\n",
      "üè∑Ô∏è Labels “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...\n",
      "‚úÖ 45 features —Ç–æ–æ—Ü–æ–æ–ª–æ–≥–¥–ª–æ–æ\n",
      "\n",
      "üè∑Ô∏è Labels “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...\n",
      "\n",
      "üìä Label distribution:\n",
      "  0 - Strong Sell    : 33,965 ( 11.5%)\n",
      "  1 - Sell           : 28,307 (  9.5%)\n",
      "  2 - Hold           : 172,401 ( 58.1%)\n",
      "  3 - Buy            : 61,902 ( 20.9%)\n",
      "  4 - Strong Buy     :      0 (  0.0%)\n",
      "\n",
      "üìä Test data shape: (296575, 24)\n",
      "\n",
      "üîÆ –¢–∞–∞–º–∞–≥–ª–∞–ª —Ö–∏–π–∂ –±–∞–π–Ω–∞...\n",
      "\n",
      "üìä Label distribution:\n",
      "  0 - Strong Sell    : 33,965 ( 11.5%)\n",
      "  1 - Sell           : 28,307 (  9.5%)\n",
      "  2 - Hold           : 172,401 ( 58.1%)\n",
      "  3 - Buy            : 61,902 ( 20.9%)\n",
      "  4 - Strong Buy     :      0 (  0.0%)\n",
      "\n",
      "üìä Test data shape: (296575, 24)\n",
      "\n",
      "üîÆ –¢–∞–∞–º–∞–≥–ª–∞–ª —Ö–∏–π–∂ –±–∞–π–Ω–∞...\n",
      "\n",
      "======================================================================\n",
      "üéØ TEST RESULTS:\n",
      "======================================================================\n",
      "  Accuracy:  17.61%\n",
      "  F1-Score:  0.193\n",
      "======================================================================\n",
      "\n",
      "üìä CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Strong Sell       0.12      0.32      0.18     33965\n",
      "        Sell       0.09      0.40      0.15     28307\n",
      "        Hold       0.48      0.15      0.22    172401\n",
      "         Buy       0.33      0.08      0.13     61902\n",
      "  Strong Buy       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.18    296575\n",
      "   macro avg       0.21      0.19      0.14    296575\n",
      "weighted avg       0.37      0.18      0.19    296575\n",
      "\n",
      "\n",
      "üî¥ BASELINE MODEL:\n",
      "\n",
      "======================================================================\n",
      "üß™ Baseline HMM - EUR_USD - TEST DATA “Æ–ù–≠–õ–ì–≠–≠\n",
      "======================================================================\n",
      "\n",
      "üìÇ Test data —É–Ω—à–∏–∂ –±–∞–π–Ω–∞: EUR_USD_test.csv\n",
      "\n",
      "======================================================================\n",
      "üéØ TEST RESULTS:\n",
      "======================================================================\n",
      "  Accuracy:  17.61%\n",
      "  F1-Score:  0.193\n",
      "======================================================================\n",
      "\n",
      "üìä CLASSIFICATION REPORT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Strong Sell       0.12      0.32      0.18     33965\n",
      "        Sell       0.09      0.40      0.15     28307\n",
      "        Hold       0.48      0.15      0.22    172401\n",
      "         Buy       0.33      0.08      0.13     61902\n",
      "  Strong Buy       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.18    296575\n",
      "   macro avg       0.21      0.19      0.14    296575\n",
      "weighted avg       0.37      0.18      0.19    296575\n",
      "\n",
      "\n",
      "üî¥ BASELINE MODEL:\n",
      "\n",
      "======================================================================\n",
      "üß™ Baseline HMM - EUR_USD - TEST DATA “Æ–ù–≠–õ–ì–≠–≠\n",
      "======================================================================\n",
      "\n",
      "üìÇ Test data —É–Ω—à–∏–∂ –±–∞–π–Ω–∞: EUR_USD_test.csv\n",
      "  ‚úÖ 296,778 –º”©—Ä —É–Ω—à–∏–≥–¥–ª–∞–∞\n",
      "\n",
      "üîß Features —Ç–æ–æ—Ü–æ–æ–ª–∂ –±–∞–π–Ω–∞...\n",
      "  ‚úÖ 296,778 –º”©—Ä —É–Ω—à–∏–≥–¥–ª–∞–∞\n",
      "\n",
      "üîß Features —Ç–æ–æ—Ü–æ–æ–ª–∂ –±–∞–π–Ω–∞...\n",
      "‚úÖ 45 features —Ç–æ–æ—Ü–æ–æ–ª–æ–≥–¥–ª–æ–æ\n",
      "\n",
      "üè∑Ô∏è Labels “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...\n",
      "‚úÖ 45 features —Ç–æ–æ—Ü–æ–æ–ª–æ–≥–¥–ª–æ–æ\n",
      "\n",
      "üè∑Ô∏è Labels “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞...\n",
      "\n",
      "üìä Label distribution:\n",
      "  0 - Strong Sell    : 33,965 ( 11.5%)\n",
      "  1 - Sell           : 28,307 (  9.5%)\n",
      "  2 - Hold           : 172,401 ( 58.1%)\n",
      "  3 - Buy            : 61,902 ( 20.9%)\n",
      "  4 - Strong Buy     :      0 (  0.0%)\n",
      "\n",
      "üìä Label distribution:\n",
      "  0 - Strong Sell    : 33,965 ( 11.5%)\n",
      "  1 - Sell           : 28,307 (  9.5%)\n",
      "  2 - Hold           : 172,401 ( 58.1%)\n",
      "  3 - Buy            : 61,902 ( 20.9%)\n",
      "  4 - Strong Buy     :      0 (  0.0%)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['atr', 'ma_cross', 'rsi'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Baseline model\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müî¥ BASELINE MODEL:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m base_result = \u001b[43mevaluate_on_test_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbaseline_artifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBaseline HMM - \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtest_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_test.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     41\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m baseline_results.append(base_result)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Quick comparison\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mevaluate_on_test_data\u001b[39m\u001b[34m(model_artifacts, test_file_path, model_name)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 4. Prepare test data\u001b[39;00m\n\u001b[32m     26\u001b[39m feature_columns = model_artifacts[\u001b[33m'\u001b[39m\u001b[33mfeature_columns\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m X_test = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_columns\u001b[49m\u001b[43m]\u001b[49m.values\n\u001b[32m     28\u001b[39m y_test = df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].values\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä Test data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmdor\\Desktop\\Forex_signal_app\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmdor\\Desktop\\Forex_signal_app\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmdor\\Desktop\\Forex_signal_app\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['atr', 'ma_cross', 'rsi'] not in index\""
     ]
    }
   ],
   "source": [
    "# 2. Test data files\n",
    "test_files = [\n",
    "    'EUR_USD_test.csv',\n",
    "    'GBP_USD_test.csv',\n",
    "    'USD_JPY_test.csv',\n",
    "    'USD_CAD_test.csv',\n",
    "    'USD_CHF_test.csv',\n",
    "    'XAU_USD_test.csv'\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: –ë“Ø—Ö Test Data –¥—ç—ç—Ä “Ø–Ω—ç–ª–∂ –±–∞–π–Ω–∞...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 3. Evaluate both models on all test files\n",
    "improved_results = []\n",
    "baseline_results = []\n",
    "\n",
    "for i, test_file in enumerate(test_files, 1):\n",
    "    test_path = f'../data/test/{test_file}'\n",
    "    \n",
    "    print(f\"\\n\\n{'#'*70}\")\n",
    "    print(f\"üìä TEST {i}/{len(test_files)}: {test_file}\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    \n",
    "    # Improved model\n",
    "    print(\"\\nüü¢ IMPROVED MODEL:\")\n",
    "    imp_result = evaluate_on_test_data(\n",
    "        model_artifacts, \n",
    "        test_path, \n",
    "        model_name=f\"Improved HMM - {test_file.replace('_test.csv', '')}\"\n",
    "    )\n",
    "    improved_results.append(imp_result)\n",
    "    \n",
    "    # Baseline model\n",
    "    print(\"\\nüî¥ BASELINE MODEL:\")\n",
    "    base_result = evaluate_on_test_data(\n",
    "        baseline_artifacts,\n",
    "        test_path,\n",
    "        model_name=f\"Baseline HMM - {test_file.replace('_test.csv', '')}\"\n",
    "    )\n",
    "    baseline_results.append(base_result)\n",
    "    \n",
    "    # Quick comparison\n",
    "    acc_diff = (imp_result['accuracy'] - base_result['accuracy']) * 100\n",
    "    f1_diff = (imp_result['f1'] - base_result['f1']) * 100\n",
    "    \n",
    "    print(f\"\\n{'‚îÄ'*70}\")\n",
    "    print(f\"‚ö° QUICK COMPARISON for {test_file.replace('_test.csv', '')}:\")\n",
    "    print(f\"   Accuracy difference: {acc_diff:+.2f}%\")\n",
    "    print(f\"   F1-Score difference: {f1_diff:+.2f}%\")\n",
    "    print(f\"{'‚îÄ'*70}\")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ –ë“Æ–• TEST DATA “Æ–ù–≠–õ–≠–ì–î–õ–≠–≠!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ff044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Plot comprehensive comparison\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: –•–∞—Ä—å—Ü—É—É–ª–∞–ª—Ç—ã–Ω –≥—Ä–∞—Ñ–∏–∫ –∑—É—Ä–∂ –±–∞–π–Ω–∞...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "plot_comparison_results(improved_results, baseline_results, test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc76618c",
   "metadata": {},
   "source": [
    "### üìä Detailed Per-Pair Confusion Matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e10bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed confusion matrices for each currency pair\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "label_names = ['S.Sell', 'Sell', 'Hold', 'Buy', 'S.Buy']\n",
    "\n",
    "for i, (test_file, imp_res, base_res) in enumerate(zip(test_files, improved_results, baseline_results)):\n",
    "    pair_name = test_file.replace('_test.csv', '')\n",
    "    \n",
    "    # Improved model confusion matrix\n",
    "    ax_imp = axes[i*2]\n",
    "    sns.heatmap(imp_res['confusion_matrix'], annot=True, fmt='d', cmap='Greens',\n",
    "                xticklabels=label_names, yticklabels=label_names,\n",
    "                cbar_kws={'label': 'Count'}, ax=ax_imp)\n",
    "    ax_imp.set_title(f'üü¢ {pair_name} - Improved\\nAcc: {imp_res[\"accuracy\"]*100:.1f}%', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    ax_imp.set_ylabel('True', fontsize=10, fontweight='bold')\n",
    "    ax_imp.set_xlabel('Predicted', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Baseline model confusion matrix\n",
    "    ax_base = axes[i*2 + 1]\n",
    "    sns.heatmap(base_res['confusion_matrix'], annot=True, fmt='d', cmap='Reds',\n",
    "                xticklabels=label_names, yticklabels=label_names,\n",
    "                cbar_kws={'label': 'Count'}, ax=ax_base)\n",
    "    ax_base.set_title(f'üî¥ {pair_name} - Baseline\\nAcc: {base_res[\"accuracy\"]*100:.1f}%', \n",
    "                      fontsize=12, fontweight='bold')\n",
    "    ax_base.set_ylabel('True', fontsize=10, fontweight='bold')\n",
    "    ax_base.set_xlabel('Predicted', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle('üìä CONFUSION MATRICES - ALL CURRENCY PAIRS', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ –ë“Ø—Ö currency pair-—ã–Ω confusion matrices —Ö–∞—Ä—É—É–ª–∞–≥–¥–ª–∞–∞!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55b91e6",
   "metadata": {},
   "source": [
    "### üìà Final Summary Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710cad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed summary table\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for test_file, imp_res, base_res in zip(test_files, improved_results, baseline_results):\n",
    "    pair = test_file.replace('_test.csv', '')\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Currency Pair': pair,\n",
    "        'Improved Acc (%)': f\"{imp_res['accuracy']*100:.2f}\",\n",
    "        'Baseline Acc (%)': f\"{base_res['accuracy']*100:.2f}\",\n",
    "        'Acc Improvement': f\"{(imp_res['accuracy'] - base_res['accuracy'])*100:+.2f}%\",\n",
    "        'Improved F1': f\"{imp_res['f1']:.3f}\",\n",
    "        'Baseline F1': f\"{base_res['f1']:.3f}\",\n",
    "        'F1 Improvement': f\"{(imp_res['f1'] - base_res['f1']):.3f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Add average row\n",
    "avg_row = {\n",
    "    'Currency Pair': 'üìä AVERAGE',\n",
    "    'Improved Acc (%)': f\"{np.mean([r['accuracy']*100 for r in improved_results]):.2f}\",\n",
    "    'Baseline Acc (%)': f\"{np.mean([r['accuracy']*100 for r in baseline_results]):.2f}\",\n",
    "    'Acc Improvement': f\"{np.mean([(i['accuracy'] - b['accuracy'])*100 for i, b in zip(improved_results, baseline_results)]):+.2f}%\",\n",
    "    'Improved F1': f\"{np.mean([r['f1'] for r in improved_results]):.3f}\",\n",
    "    'Baseline F1': f\"{np.mean([r['f1'] for r in baseline_results]):.3f}\",\n",
    "    'F1 Improvement': f\"{np.mean([i['f1'] - b['f1'] for i, b in zip(improved_results, baseline_results)]):.3f}\"\n",
    "}\n",
    "summary_df = pd.concat([summary_df, pd.DataFrame([avg_row])], ignore_index=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä –≠–¶–°–ò–ô–ù –¢–ê–ô–õ–ê–ù - FINAL SUMMARY REPORT\")\n",
    "print(\"=\"*100)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Accuracy table\n",
    "acc_data = summary_df[['Currency Pair', 'Improved Acc (%)', 'Baseline Acc (%)', 'Acc Improvement']].iloc[:-1]\n",
    "colors = [['#e8f5e9' if i % 2 == 0 else '#ffffff'] * 4 for i in range(len(acc_data))]\n",
    "\n",
    "ax1.axis('tight')\n",
    "ax1.axis('off')\n",
    "table1 = ax1.table(cellText=acc_data.values,\n",
    "                   colLabels=acc_data.columns,\n",
    "                   cellLoc='center',\n",
    "                   loc='center',\n",
    "                   cellColours=colors,\n",
    "                   colColours=['#4caf50']*4)\n",
    "table1.auto_set_font_size(False)\n",
    "table1.set_fontsize(10)\n",
    "table1.scale(1, 2)\n",
    "ax1.set_title('üéØ ACCURACY COMPARISON', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# F1-Score table\n",
    "f1_data = summary_df[['Currency Pair', 'Improved F1', 'Baseline F1', 'F1 Improvement']].iloc[:-1]\n",
    "colors = [['#e3f2fd' if i % 2 == 0 else '#ffffff'] * 4 for i in range(len(f1_data))]\n",
    "\n",
    "ax2.axis('tight')\n",
    "ax2.axis('off')\n",
    "table2 = ax2.table(cellText=f1_data.values,\n",
    "                   colLabels=f1_data.columns,\n",
    "                   cellLoc='center',\n",
    "                   loc='center',\n",
    "                   cellColours=colors,\n",
    "                   colColours=['#2196f3']*4)\n",
    "table2.auto_set_font_size(False)\n",
    "table2.set_fontsize(10)\n",
    "table2.scale(1, 2)\n",
    "ax2.set_title('üìä F1-SCORE COMPARISON', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final verdict\n",
    "avg_acc_improvement = np.mean([(i['accuracy'] - b['accuracy'])*100 for i, b in zip(improved_results, baseline_results)])\n",
    "avg_f1_improvement = np.mean([i['f1'] - b['f1'] for i, b in zip(improved_results, baseline_results)])\n",
    "\n",
    "print(\"\\n\\n\" + \"üèÜ\"*35)\n",
    "print(\"                    –≠–¶–°–ò–ô–ù –î“Æ–ì–ù–≠–õ–¢ - FINAL VERDICT\")\n",
    "print(\"üèÜ\"*35)\n",
    "\n",
    "if avg_acc_improvement > 5:\n",
    "    verdict = \"üåü –ú–ê–® –°–ê–ô–ù –°–ê–ô–ñ–ò–†–°–ê–ù! Improved model –Ω—å ”©–º–Ω”©—Ö model-–æ–æ—Å —Ö–∞–º–∞–∞–≥“Ø–π —Å–∞–π–Ω!\"\n",
    "elif avg_acc_improvement > 2:\n",
    "    verdict = \"‚úÖ –°–ê–ô–ñ–ò–†–°–ê–ù! Improved model –Ω—å —Ç–æ–¥–æ—Ä—Ö–æ–π —Å–∞–π–∂—Ä–∞–ª —Ö–∞—Ä—É—É–ª–∂ –±–∞–π–Ω–∞.\"\n",
    "elif avg_acc_improvement > 0:\n",
    "    verdict = \"‚ö†Ô∏è –ë–ê–ì–ê –ó–≠–†–≠–ì –°–ê–ô–ñ–ò–†–°–ê–ù. –û–¥–æ–æ–≥–∏–π–Ω —Å–∞–π–∂—Ä—É—É–ª–∞–ª—Ç —Ç–∏–π–º —á –∏—Ö –±–∏—à.\"\n",
    "else:\n",
    "    verdict = \"‚ùå –ú–£–£–î–°–ê–ù. –≠–Ω—ç –Ω—å —Ç–æ—Ö–∏–æ–ª–¥–æ—Ö –±–æ–ª–æ–º–∂—Ç–æ–π, hyperparameter-–∏–π–≥ –∑–∞—Å–∞—Ö —Ö—ç—Ä—ç–≥—Ç—ç–π.\"\n",
    "\n",
    "print(f\"\\n{verdict}\")\n",
    "print(f\"\\nüìà Average Accuracy Improvement: {avg_acc_improvement:+.2f}%\")\n",
    "print(f\"üìà Average F1-Score Improvement: {avg_f1_improvement:+.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"üèÜ\"*35)\n",
    "print(\"\\n‚úÖ –ë“Æ–• “Æ–ù–≠–õ–ì–≠–≠ –î–£–£–°–õ–ê–ê!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
