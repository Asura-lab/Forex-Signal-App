{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf955497",
   "metadata": {},
   "source": [
    "# 🚀 Multi-Currency Forex Trading Model\n",
    "\n",
    "**Training:** All 6 currency pairs from `data/train/`\n",
    "**Testing:** All 6 currency pairs from `data/test/`\n",
    "\n",
    "Pairs:\n",
    "- EUR_USD\n",
    "- GBP_USD\n",
    "- USD_JPY\n",
    "- USD_CAD\n",
    "- USD_CHF\n",
    "- XAU_USD (Gold)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bef6b1c",
   "metadata": {},
   "source": [
    "## 📦 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e87c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorFlow: 2.20.0\n",
      "✅ GPU: []\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"✅ TensorFlow: {tf.__version__}\")\n",
    "print(f\"✅ GPU: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fb72fe",
   "metadata": {},
   "source": [
    "## 📥 2. Load All Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90cf984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Found 6 training files:\n",
      "   • EUR_USD_1min.csv\n",
      "   • GBP_USD_1min.csv\n",
      "   • USD_CAD_1min.csv\n",
      "   • USD_CHF_1min.csv\n",
      "   • USD_JPY_1min.csv\n",
      "   • XAU_USD_1min.csv\n",
      "✅ Loaded EUR_USD: 1,859,492 rows\n",
      "✅ Loaded GBP_USD: 1,861,292 rows\n",
      "✅ Loaded USD_CAD: 1,855,369 rows\n"
     ]
    }
   ],
   "source": [
    "# Get all CSV files from train folder\n",
    "train_files = glob('../data/train/*_1min.csv')\n",
    "print(f\"📂 Found {len(train_files)} training files:\")\n",
    "for f in train_files:\n",
    "    print(f\"   • {Path(f).name}\")\n",
    "\n",
    "# Load all training data\n",
    "train_dfs = []\n",
    "for file in train_files:\n",
    "    pair_name = Path(file).stem.replace('_1min', '')\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Standardize column names\n",
    "    df.columns = df.columns.str.lower()\n",
    "    if 'tick_volume' in df.columns:\n",
    "        df.rename(columns={'tick_volume': 'volume'}, inplace=True)\n",
    "    \n",
    "    df['pair'] = pair_name\n",
    "    train_dfs.append(df)\n",
    "    print(f\"✅ Loaded {pair_name}: {len(df):,} rows\")\n",
    "\n",
    "# Combine all training data\n",
    "df_train_all = pd.concat(train_dfs, ignore_index=True)\n",
    "print(f\"\\n📊 Total training data: {len(df_train_all):,} rows\")\n",
    "print(f\"   Columns: {list(df_train_all.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beacb3f",
   "metadata": {},
   "source": [
    "## 📥 3. Load All Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d925609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all CSV files from test folder\n",
    "test_files = glob('../data/test/*_test.csv')\n",
    "print(f\"📂 Found {len(test_files)} test files:\")\n",
    "for f in test_files:\n",
    "    print(f\"   • {Path(f).name}\")\n",
    "\n",
    "# Load all test data\n",
    "test_dfs = []\n",
    "for file in test_files:\n",
    "    pair_name = Path(file).stem.replace('_test', '')\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Standardize column names\n",
    "    df.columns = df.columns.str.lower()\n",
    "    if 'tick_volume' in df.columns:\n",
    "        df.rename(columns={'tick_volume': 'volume'}, inplace=True)\n",
    "    \n",
    "    df['pair'] = pair_name\n",
    "    test_dfs.append(df)\n",
    "    print(f\"✅ Loaded {pair_name}: {len(df):,} rows\")\n",
    "\n",
    "# Combine all test data\n",
    "df_test_all = pd.concat(test_dfs, ignore_index=True)\n",
    "print(f\"\\n📊 Total test data: {len(df_test_all):,} rows\")\n",
    "print(f\"   Columns: {list(df_test_all.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410a6d5a",
   "metadata": {},
   "source": [
    "## 🔧 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa2a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"Create technical indicators\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Price features\n",
    "    df['price_range'] = df['high'] - df['low']\n",
    "    df['body'] = df['close'] - df['open']\n",
    "    df['upper_wick'] = df['high'] - df[['open', 'close']].max(axis=1)\n",
    "    df['lower_wick'] = df[['open', 'close']].min(axis=1) - df['low']\n",
    "    \n",
    "    # Moving Averages\n",
    "    for period in [5, 10, 20, 50]:\n",
    "        df[f'sma_{period}'] = df['close'].rolling(period).mean()\n",
    "        df[f'ema_{period}'] = df['close'].ewm(span=period).mean()\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / loss\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    ema12 = df['close'].ewm(span=12).mean()\n",
    "    ema26 = df['close'].ewm(span=26).mean()\n",
    "    df['macd'] = ema12 - ema26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    sma20 = df['close'].rolling(20).mean()\n",
    "    std20 = df['close'].rolling(20).std()\n",
    "    df['bb_upper'] = sma20 + (std20 * 2)\n",
    "    df['bb_lower'] = sma20 - (std20 * 2)\n",
    "    df['bb_width'] = df['bb_upper'] - df['bb_lower']\n",
    "    \n",
    "    # ATR\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = (df['high'] - df['close'].shift()).abs()\n",
    "    low_close = (df['low'] - df['close'].shift()).abs()\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['atr'] = true_range.rolling(14).mean()\n",
    "    \n",
    "    # Volume\n",
    "    df['volume_sma'] = df['volume'].rolling(20).mean()\n",
    "    df['volume_ratio'] = df['volume'] / df['volume_sma']\n",
    "    \n",
    "    # Returns\n",
    "    df['return_1'] = df['close'].pct_change(1)\n",
    "    df['return_5'] = df['close'].pct_change(5)\n",
    "    df['return_10'] = df['close'].pct_change(10)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"🔧 Creating features for training data...\")\n",
    "df_train_all = create_features(df_train_all)\n",
    "\n",
    "print(\"🔧 Creating features for test data...\")\n",
    "df_test_all = create_features(df_test_all)\n",
    "\n",
    "print(f\"✅ Training features: {df_train_all.shape}\")\n",
    "print(f\"✅ Test features: {df_test_all.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7a2af0",
   "metadata": {},
   "source": [
    "## 🏷️ 5. Create Labels\n",
    "\n",
    "**Strategy:** Predict if price will go UP/DOWN/NEUTRAL in next 15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f099ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(df, horizon=15, threshold=0.0005):\n",
    "    \"\"\"\n",
    "    Create labels: 0=SELL, 1=NEUTRAL, 2=BUY\n",
    "    \n",
    "    Args:\n",
    "        horizon: Minutes to look ahead\n",
    "        threshold: Minimum price change (0.05% = ~5 pips)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Future price change\n",
    "    df['future_price'] = df.groupby('pair')['close'].shift(-horizon)\n",
    "    df['price_change'] = (df['future_price'] - df['close']) / df['close']\n",
    "    \n",
    "    # Create labels\n",
    "    df['label'] = 1  # Default: NEUTRAL\n",
    "    df.loc[df['price_change'] > threshold, 'label'] = 2   # BUY\n",
    "    df.loc[df['price_change'] < -threshold, 'label'] = 0  # SELL\n",
    "    \n",
    "    return df\n",
    "\n",
    "HORIZON = 15\n",
    "THRESHOLD = 0.0005  # 0.05%\n",
    "\n",
    "print(f\"🏷️ Creating labels (horizon={HORIZON}min, threshold={THRESHOLD*100:.2f}%)...\")\n",
    "df_train_all = create_labels(df_train_all, HORIZON, THRESHOLD)\n",
    "df_test_all = create_labels(df_test_all, HORIZON, THRESHOLD)\n",
    "\n",
    "# Check label distribution\n",
    "print(\"\\n📊 Training Label Distribution:\")\n",
    "train_label_dist = df_train_all['label'].value_counts().sort_index()\n",
    "for label, count in train_label_dist.items():\n",
    "    label_name = ['SELL', 'NEUTRAL', 'BUY'][int(label)]\n",
    "    pct = count / len(df_train_all) * 100\n",
    "    print(f\"   {label_name:8}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n📊 Test Label Distribution:\")\n",
    "test_label_dist = df_test_all['label'].value_counts().sort_index()\n",
    "for label, count in test_label_dist.items():\n",
    "    label_name = ['SELL', 'NEUTRAL', 'BUY'][int(label)]\n",
    "    pct = count / len(df_test_all) * 100\n",
    "    print(f\"   {label_name:8}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a941797",
   "metadata": {},
   "source": [
    "## 📦 6. Prepare Sequences\n",
    "\n",
    "Create time series sequences (lookback window = 60 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1af02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(df, sequence_length=60):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM\n",
    "    \"\"\"\n",
    "    # Select feature columns (exclude non-numeric and target)\n",
    "    feature_cols = [col for col in df.columns if col not in \n",
    "                   ['time', 'pair', 'label', 'future_price', 'price_change']]\n",
    "    feature_cols = [col for col in feature_cols if df[col].dtype in ['float64', 'int64']]\n",
    "    \n",
    "    # Drop NaN\n",
    "    df_clean = df[feature_cols + ['label']].dropna()\n",
    "    \n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    # Create sequences\n",
    "    for i in range(sequence_length, len(df_clean)):\n",
    "        X_list.append(df_clean[feature_cols].iloc[i-sequence_length:i].values)\n",
    "        y_list.append(df_clean['label'].iloc[i])\n",
    "    \n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "SEQUENCE_LENGTH = 60\n",
    "\n",
    "print(f\"📦 Creating sequences (lookback={SEQUENCE_LENGTH} minutes)...\\n\")\n",
    "\n",
    "X_train, y_train, feature_cols = prepare_sequences(df_train_all, SEQUENCE_LENGTH)\n",
    "X_test, y_test, _ = prepare_sequences(df_test_all, SEQUENCE_LENGTH)\n",
    "\n",
    "print(f\"✅ Training data: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"✅ Test data: X={X_test.shape}, y={y_test.shape}\")\n",
    "print(f\"\\n📋 Features ({len(feature_cols)}): {feature_cols[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809ff59",
   "metadata": {},
   "source": [
    "## 🔄 7. Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a41664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for scaling\n",
    "n_samples_train, n_timesteps, n_features = X_train.shape\n",
    "n_samples_test = X_test.shape[0]\n",
    "\n",
    "X_train_reshaped = X_train.reshape(-1, n_features)\n",
    "X_test_reshaped = X_test.reshape(-1, n_features)\n",
    "\n",
    "# Fit scaler on training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "X_test_scaled = scaler.transform(X_test_reshaped)\n",
    "\n",
    "# Reshape back\n",
    "X_train_scaled = X_train_scaled.reshape(n_samples_train, n_timesteps, n_features)\n",
    "X_test_scaled = X_test_scaled.reshape(n_samples_test, n_timesteps, n_features)\n",
    "\n",
    "print(f\"✅ Data scaled: X_train={X_train_scaled.shape}, X_test={X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2257aa51",
   "metadata": {},
   "source": [
    "## 🏗️ 8. Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59baf0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(input_shape, n_classes=3):\n",
    "    \"\"\"\n",
    "    Build LSTM model for multi-class classification\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(128, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.3),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dropout(0.3),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "input_shape = (X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
    "model = build_lstm_model(input_shape)\n",
    "\n",
    "print(\"🏗️ Model Architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f038a",
   "metadata": {},
   "source": [
    "## 🎯 9. Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427cbfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "Path('../models/multi_currency').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        '../models/multi_currency/best_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"✅ Callbacks configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07c3519",
   "metadata": {},
   "source": [
    "## 🚀 10. Train Model\n",
    "\n",
    "Training on **all currency pairs** from train folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f0669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "print(\"🚀 Starting training...\\n\")\n",
    "print(f\"⚙️  Configuration:\")\n",
    "print(f\"   • Training samples: {len(X_train_scaled):,}\")\n",
    "print(f\"   • Validation split: {VALIDATION_SPLIT*100:.0f}%\")\n",
    "print(f\"   • Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   • Max epochs: {EPOCHS}\")\n",
    "print(f\"   • Early stopping: patience=10\\n\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ Training complete!\")\n",
    "print(f\"   • Best val accuracy: {max(history.history['val_accuracy'])*100:.2f}%\")\n",
    "print(f\"   • Total epochs: {len(history.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb38f57",
   "metadata": {},
   "source": [
    "## 📈 11. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01d29e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history.history['accuracy'], label='Train')\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation')\n",
    "axes[0].set_title('Model Accuracy', fontweight='bold', fontsize=14)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history.history['loss'], label='Train')\n",
    "axes[1].plot(history.history['val_loss'], label='Validation')\n",
    "axes[1].set_title('Model Loss', fontweight='bold', fontsize=14)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/multi_currency/training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 Training history saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88dd901",
   "metadata": {},
   "source": [
    "## 🧪 12. Test on All Currency Pairs\n",
    "\n",
    "Testing on **all test data** from test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model = keras.models.load_model('../models/multi_currency/best_model.keras')\n",
    "\n",
    "print(\"🧪 Testing on all currency pairs...\\n\")\n",
    "\n",
    "# Predict\n",
    "y_pred_proba = best_model.predict(X_test_scaled)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Overall accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"🎯 Overall Test Accuracy: {accuracy*100:.2f}%\\n\")\n",
    "\n",
    "# Classification report\n",
    "print(\"📊 Classification Report:\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names=['SELL', 'NEUTRAL', 'BUY'],\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['SELL', 'NEUTRAL', 'BUY'],\n",
    "            yticklabels=['SELL', 'NEUTRAL', 'BUY'])\n",
    "plt.title('Confusion Matrix - All Currency Pairs', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/multi_currency/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98deca7c",
   "metadata": {},
   "source": [
    "## 🔍 13. Signal Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c159f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 SIGNAL DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Count predictions\n",
    "pred_counts = pd.Series(y_pred).value_counts().sort_index()\n",
    "true_counts = pd.Series(y_test).value_counts().sort_index()\n",
    "\n",
    "print(\"\\n📊 TRUE LABELS:\")\n",
    "for label in [0, 1, 2]:\n",
    "    label_name = ['SELL', 'NEUTRAL', 'BUY'][label]\n",
    "    count = true_counts.get(label, 0)\n",
    "    pct = count / len(y_test) * 100\n",
    "    print(f\"   {label_name:8}: {count:6,} ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\n🎯 PREDICTED LABELS:\")\n",
    "for label in [0, 1, 2]:\n",
    "    label_name = ['SELL', 'NEUTRAL', 'BUY'][label]\n",
    "    count = pred_counts.get(label, 0)\n",
    "    pct = count / len(y_pred) * 100\n",
    "    print(f\"   {label_name:8}: {count:6,} ({pct:5.1f}%)\")\n",
    "\n",
    "# Signal percentage\n",
    "signal_pct = (pred_counts.get(0, 0) + pred_counts.get(2, 0)) / len(y_pred) * 100\n",
    "print(f\"\\n📡 ACTIONABLE SIGNALS: {signal_pct:.1f}%\")\n",
    "\n",
    "if signal_pct < 20:\n",
    "    print(\"   ⚠️  Too few signals - mostly predicts NEUTRAL\")\n",
    "elif signal_pct > 40:\n",
    "    print(\"   ✅ Good signal generation!\")\n",
    "else:\n",
    "    print(\"   ⚠️  Moderate signal generation\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af07b76d",
   "metadata": {},
   "source": [
    "## 💾 14. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618cd47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, '../models/multi_currency/scaler.pkl')\n",
    "print(\"✅ Scaler saved\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_name': 'Multi_Currency_LSTM',\n",
    "    'currency_pairs': ['EUR_USD', 'GBP_USD', 'USD_JPY', 'USD_CAD', 'USD_CHF', 'XAU_USD'],\n",
    "    'horizon': HORIZON,\n",
    "    'threshold': THRESHOLD,\n",
    "    'sequence_length': SEQUENCE_LENGTH,\n",
    "    'n_features': len(feature_cols),\n",
    "    'feature_columns': feature_cols,\n",
    "    'training_samples': int(len(X_train)),\n",
    "    'test_samples': int(len(X_test)),\n",
    "    'test_accuracy': float(accuracy),\n",
    "    'best_val_accuracy': float(max(history.history['val_accuracy'])),\n",
    "    'signal_percentage': float(signal_pct),\n",
    "    'trained_at': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('../models/multi_currency/model_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"✅ Metadata saved\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🎉 ALL DONE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n📁 Model saved to: ../models/multi_currency/\")\n",
    "print(f\"   • best_model.keras\")\n",
    "print(f\"   • scaler.pkl\")\n",
    "print(f\"   • model_metadata.json\")\n",
    "print(f\"   • training_history.png\")\n",
    "print(f\"   • confusion_matrix.png\")\n",
    "print(f\"\\n🎯 Final Results:\")\n",
    "print(f\"   • Test Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"   • Signal Coverage: {signal_pct:.1f}%\")\n",
    "print(f\"   • Training samples: {len(X_train):,}\")\n",
    "print(f\"   • Test samples: {len(X_test):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf51615",
   "metadata": {},
   "source": [
    "## 🎊 Summary\n",
    "\n",
    "✅ **Training Data:** All 6 currency pairs from `data/train/`\n",
    "✅ **Test Data:** All 6 currency pairs from `data/test/`\n",
    "✅ **Model:** LSTM with 128→64 units\n",
    "✅ **Features:** 30+ technical indicators\n",
    "✅ **Prediction:** 15-minute ahead (BUY/SELL/NEUTRAL)\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "1. Deploy model to Flask API\n",
    "2. Integrate with mobile app\n",
    "3. Add real-time MT5 data feed\n",
    "4. Implement signal filtering (confidence > 80%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
