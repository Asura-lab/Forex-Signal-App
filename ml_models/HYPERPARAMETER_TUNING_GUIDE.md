# üéØ Hyperparameter Tuning Guide for EUR/USD Model

## ”®–º–Ω”©—Ö –∞—Å—É—É–¥–∞–ª (Problem Analysis)

**Accuracy: 33.46%** - –≠–Ω—ç –Ω—å 3 –∞–Ω–≥–∏–ª–ª—ã–Ω —Ö—É–≤—å–¥ —Å–∞–Ω–∞–º—Å–∞—Ä–≥“Ø–π —Ç–∞–∞–º–∞–≥–ª–∞–ª–∞–∞—Å (33.33%) –±–∞–≥–∞ –±–∞–π–Ω–∞. –≠–Ω—ç –Ω—å –º–æ–¥–µ–ª—å —è–º–∞—Ä —á —É—Ç–≥–∞ –±“Ø—Ö–∏–π –∑“Ø–π —Ç–æ–≥—Ç–æ–ª—ã–≥ —Å—É—Ä—á —á–∞–¥–∞—Ö–≥“Ø–π –±–∞–π–≥–∞–∞–≥ —Ö–∞—Ä—É—É–ª–∂ –±–∞–π–Ω–∞.

## ‚úÖ –®–∏–Ω—ç –°–∞–π–∂—Ä—É—É–ª–∞–ª—Ç (Implemented Improvements)

### 1. –ú–æ–¥–µ–ª–∏–π–Ω —Ö“Ø—á–∏–Ω —á–∞–¥–ª—ã–≥ –Ω—ç–º—ç–≥–¥“Ø“Ø–ª—Å—ç–Ω (Increased Model Capacity)

**”®–º–Ω”© (Before):**
```python
lstm_units = [128, 64]
ff_dim = 256
```

**–û–¥–æ–æ (Now):**
```python
lstm_units = [256, 128]  # 2x –∏–ª“Ø“Ø –ø–∞—Ä–∞–º–µ—Ç—Ä
ff_dim = 512            # 2x –∏–ª“Ø“Ø –ø–∞—Ä–∞–º–µ—Ç—Ä
```

**–Ø–∞–≥–∞–∞–¥?** –ò–ª“Ø“Ø —Ç–æ–º –º–æ–¥–µ–ª—å –Ω—å –∏–ª“Ø“Ø —Ç”©–≤”©–≥—Ç—ç–π –∑“Ø–π —Ç–æ–≥—Ç–æ–ª—ã–≥ –æ–ª–∂ –∏–ª—Ä“Ø“Ø–ª–∂ —á–∞–¥–Ω–∞.

---

### 2. –£—Ä—Ç —Ö—É–≥–∞—Ü–∞–∞–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç (Extended Temporal Context)

**”®–º–Ω”©:**
```python
SEQUENCE_LENGTH = 60  # 60 –º–∏–Ω—É—Ç—ã–Ω ”©–≥”©–≥–¥”©–ª
```

**–û–¥–æ–æ:**
```python
SEQUENCE_LENGTH = 120  # 120 –º–∏–Ω—É—Ç—ã–Ω ”©–≥”©–≥–¥”©–ª
```

**–Ø–∞–≥–∞–∞–¥?** –£—Ä—Ç –¥–∞—Ä–∞–∞–ª–∞–ª –Ω—å –∏–ª“Ø“Ø –∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç ”©–≥—á, –∏–ª“Ø“Ø —Å–∞–π–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª —Ö–∏–π—Ö –±–æ–ª–æ–º–∂–∏–π–≥ –æ–ª–≥–æ–Ω–æ.

---

### 3. –ò–ª“Ø“Ø –æ–ª–æ–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω ”©–≥”©–≥–¥”©–ª (More Training Data)

**”®–º–Ω”©:**
```python
STEP = 5  # –ê–ª—Ö–∞–º
```

**–û–¥–æ–æ:**
```python
STEP = 3  # –ë–∞–≥–∞ –∞–ª—Ö–∞–º = –∏–ª“Ø“Ø –æ–ª–æ–Ω –¥–∞—Ä–∞–∞–ª–∞–ª
```

**–Ø–∞–≥–∞–∞–¥?** –ë–∞–≥–∞ –∞–ª—Ö–∞–º –Ω—å –∏–ª“Ø“Ø –æ–ª–æ–Ω –¥–∞–≤—Ö–∞—Ä–¥—Å–∞–Ω –¥–∞—Ä–∞–∞–ª–∞–ª “Ø“Ø—Å–≥—ç–∂, –∏–ª“Ø“Ø –∏—Ö —Å—É—Ä–≥–∞–ª—Ç—ã–Ω ”©–≥”©–≥–¥”©–ª ”©–≥–Ω”©.

---

### 4. –ê–Ω–≥–∏–ª–ª—ã–Ω —Ç—ç–Ω—Ü–≤—ç—Ä (Class Balancing)

**–®–∏–Ω—ç –æ–Ω—Ü–ª–æ–≥:**
```python
class_weights = {
    0: 1.2,  # SELL
    1: 0.8,  # NEUTRAL  
    2: 1.2   # BUY
}
```

**–Ø–∞–≥–∞–∞–¥?** –§–æ—Ä–µ–∫—Å –∑–∞—Ö –∑—ç—ç–ª –¥—ç—ç—Ä –∏—Ö—ç–≤—á–ª—ç–Ω NEUTRAL –±–∞–π–¥–∞–ª –∏—Ö –±–∞–π–¥–∞–≥. Class weights –Ω—å –º–æ–¥–µ–ª—å –±“Ø—Ö –∞–Ω–≥–∏–ª–ª—ã–≥ —Ç—ç–Ω—Ü“Ø“Ø –∞–Ω—Ö–∞–∞—Ä–∞—Ö–∞–¥ —Ç—É—Å–∞–ª–Ω–∞.

---

### 5. Dropout –æ–Ω–æ–≤—á—Ç–æ–π –±–æ–ª–≥–æ—Ö (Optimized Dropout)

**”®–º–Ω”©:**
```python
dropout_rate = 0.5  # –•—ç—Ç –∏—Ö
```

**–û–¥–æ–æ:**
```python
dropout_rate = 0.4  # –û–Ω–æ–≤—á—Ç–æ–π
```

**–Ø–∞–≥–∞–∞–¥?** 0.5 –Ω—å —Ö—ç—Ç –∏—Ö –±–∞–π–∂ –±–æ–ª–æ—Ö —é–º. 0.3-0.4 –Ω—å –∏—Ö—ç–≤—á–ª—ç–Ω –∏–ª“Ø“Ø —Å–∞–π–Ω –∞–∂–∏–ª–ª–∞–¥–∞–≥.

---

### 6. Learning Rate —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö (Learning Rate Tuning)

**–û–¥–æ–æ:**
```python
LEARNING_RATE = 0.0001  # –û–Ω–æ–≤—á—Ç–æ–π —É—Ç–≥–∞
```

**–¢–∞–π–ª–±–∞—Ä:**
- 0.0001 –Ω—å Transformer + LSTM-–¥ —Ö–∞–º–≥–∏–π–Ω —Å–∞–π–Ω –∞–∂–∏–ª–ª–∞–¥–∞–≥
- –•—ç—Ç –∏—Ö –±–∞–π–≤–∞–ª —Ç–æ–≥—Ç–≤–æ—Ä–≥“Ø–π —Å—É—Ä–≥–∞–ª—Ç
- –•—ç—Ç –±–∞–≥–∞ –±–∞–π–≤–∞–ª —É–¥–∞–∞–Ω —Å—É—Ä–≥–∞–ª—Ç

---

### 7. Batch Size –æ–Ω–æ–≤—á—Ç–æ–π –±–æ–ª–≥–æ—Ö (Optimal Batch Size)

**–û–¥–æ–æ:**
```python
BATCH_SIZE = 64
```

**–¢–∞–π–ª–±–∞—Ä:**
- 32 –Ω—å —Ö—ç—Ç –±–∞–≥–∞ (—à—É—É–≥–∏–∞–Ω—Ç–∞–π –≥—Ä–∞–¥–∏–µ–Ω—Ç)
- 64 –Ω—å –æ–Ω–æ–≤—á—Ç–æ–π (—Ç–æ–≥—Ç–≤–æ—Ä—Ç–æ–π + —Ö—É—Ä–¥–∞–Ω)
- 128 –Ω—å —Ö—ç—Ç –∏—Ö (—Å–∞–Ω–∞—Ö –æ–π –∏—Ö—Ç—ç–π, —É–¥–∞–∞–Ω —Å—É—Ä–≥–∞–ª—Ç)

---

### 8. Label-–∏–π–Ω threshold —Å–∞–π–∂—Ä—É—É–ª–∞—Ö (Better Label Threshold)

**”®–º–Ω”©:**
```python
THRESHOLD = 0.0005  # 0.05% - —Ö—ç—Ç –∏—Ö
```

**–û–¥–æ–æ:**
```python
THRESHOLD = 0.0003  # 0.03% - –∏–ª“Ø“Ø –º—ç–¥—Ä—ç–º—Ç–≥–∏–π
```

**–Ø–∞–≥–∞–∞–¥?** –ë–∞–≥–∞ threshold –Ω—å –∏–ª“Ø“Ø –æ–ª–æ–Ω —Å–∏–≥–Ω–∞–ª “Ø“Ø—Å–≥—ç–∂, –º–æ–¥–µ–ª—å –∏–ª“Ø“Ø –∏—Ö ”©–≥”©–≥–¥”©–ª—Ç—ç–π –∞–∂–∏–ª–ª–∞—Ö –±–æ–ª–æ–º–∂—Ç–æ–π.

---

## üîß Hyperparameter-“Ø“Ø–¥–∏–π–≥ –¥–∞—Ö–∏–Ω —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö –∑–∞–∞–≤–∞—Ä

–•—ç—Ä—ç–≤ accuracy-–≥ —Ü–∞–∞—à —Å–∞–π–∂—Ä—É—É–ª–∞—Ö —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π –±–æ–ª:

### 1. Model Capacity (–ú–æ–¥–µ–ª–∏–π–Ω —Ö“Ø—á–∏–Ω —á–∞–¥–∞–ª)

```python
# –ò–ª“Ø“Ø –±–∞–≥–∞ - —Ö—É—Ä–¥–∞–Ω, –≥—ç—Ö–¥—ç—ç –±–∞–≥–∞ —Ö“Ø—á–∏–Ω —á–∞–¥–∞–ª—Ç–∞–π
lstm_units = [128, 64]
ff_dim = 256

# –û–Ω–æ–≤—á—Ç–æ–π - –æ–¥–æ–æ–≥–∏–π–Ω —Ç–æ—Ö–∏—Ä–≥–æ–æ
lstm_units = [256, 128]
ff_dim = 512

# –ò–ª“Ø“Ø –∏—Ö - –∏–ª“Ø“Ø —Ö“Ø—á—Ç—ç–π, –≥—ç—Ö–¥—ç—ç –∏–ª“Ø“Ø —É–¥–∞–∞–Ω
lstm_units = [512, 256, 128]
ff_dim = 1024
```

### 2. Dropout Rate

```python
# –ò–ª“Ø“Ø –±–∞–≥–∞ overfitting = –∏–ª“Ø“Ø –∏—Ö dropout
dropout_rate = 0.5

# –û–Ω–æ–≤—á—Ç–æ–π - –æ–¥–æ–æ–≥–∏–π–Ω —Ç–æ—Ö–∏—Ä–≥–æ–æ
dropout_rate = 0.4

# –ë–∞–≥–∞ overfitting = –±–∞–≥–∞ dropout
dropout_rate = 0.3
```

### 3. Learning Rate

```python
# –•—É—Ä–¥–∞–Ω —Å—É—Ä–≥–∞–ª—Ç, –≥—ç—Ö–¥—ç—ç —Ç–æ–≥—Ç–≤–æ—Ä–≥“Ø–π
LEARNING_RATE = 0.0005

# –û–Ω–æ–≤—á—Ç–æ–π - –æ–¥–æ–æ–≥–∏–π–Ω —Ç–æ—Ö–∏—Ä–≥–æ–æ
LEARNING_RATE = 0.0001

# –£–¥–∞–∞–Ω, –≥—ç—Ö–¥—ç—ç –∏–ª“Ø“Ø –Ω–∞—Ä–∏–π–≤—á–ª–∞–ª—Ç–∞–π
LEARNING_RATE = 0.00005
```

### 4. Sequence Length

```python
# –ë–æ–≥–∏–Ω–æ - –±–∞–≥–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç
SEQUENCE_LENGTH = 60

# –û–Ω–æ–≤—á—Ç–æ–π - –æ–¥–æ–æ–≥–∏–π–Ω —Ç–æ—Ö–∏—Ä–≥–æ–æ
SEQUENCE_LENGTH = 120

# –£—Ä—Ç - –∏–ª“Ø“Ø –∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç, –≥—ç—Ö–¥—ç—ç —É–¥–∞–∞–Ω
SEQUENCE_LENGTH = 240
```

### 5. Batch Size

```python
# –ë–∞–≥–∞ - —à—É—É–≥–∏–∞–Ω—Ç–∞–π –≥—Ä–∞–¥–∏–µ–Ω—Ç
BATCH_SIZE = 32

# –û–Ω–æ–≤—á—Ç–æ–π - –æ–¥–æ–æ–≥–∏–π–Ω —Ç–æ—Ö–∏—Ä–≥–æ–æ
BATCH_SIZE = 64

# –ò—Ö - –∏–ª“Ø“Ø —Ç–æ–≥—Ç–≤–æ—Ä—Ç–æ–π, –≥—ç—Ö–¥—ç—ç —Å–∞–Ω–∞—Ö –æ–π –∏—Ö —à–∞–∞—Ä–¥–¥–∞–≥
BATCH_SIZE = 128
```

---

## üìä Accuracy-–≥ —Ö—ç—Ä—Ö—ç–Ω —Å–∞–π–∂—Ä—É—É–ª–∞—Ö –≤—ç?

### –•—É–≤–∏–ª–±–∞—Ä 1: –ò–ª“Ø“Ø ”©–≥”©–≥–¥”©–ª —Ö—ç—Ä—ç–≥—Ç—ç–π –±–æ–ª

```python
# –ò–ª“Ø“Ø –æ–ª–æ–Ω —Ç“Ø“Ø—Ö—ç–Ω ”©–≥”©–≥–¥”©–ª –∞–≤–∞—Ö
df_features = load_data(months=6)  # 3 —Å–∞—Ä—ã–Ω –æ—Ä–æ–Ω–¥ 6 —Å–∞—Ä

# –ò–ª“Ø“Ø –±–∞–≥–∞ –∞–ª—Ö–∞–º - –∏–ª“Ø“Ø –æ–ª–æ–Ω –¥–∞—Ä–∞–∞–ª–∞–ª
STEP = 2  # 3-–Ω –æ—Ä–æ–Ω–¥ 2
```

### –•—É–≤–∏–ª–±–∞—Ä 2: –ò–ª“Ø“Ø —Ö“Ø—á—Ç—ç–π –º–æ–¥–µ–ª—å —Ö—ç—Ä—ç–≥—Ç—ç–π –±–æ–ª

```python
# –ò–ª“Ø“Ø —Ç–æ–º LSTM
lstm_units = [512, 256, 128]  # 3 –¥–∞–≤—Ö–∞—Ä–≥–∞

# –ò–ª“Ø“Ø –æ–ª–æ–Ω attention head
n_heads = 16  # 8-–Ω –æ—Ä–æ–Ω–¥ 16

# –¢–æ–º feed-forward
ff_dim = 1024  # 512-–Ω –æ—Ä–æ–Ω–¥ 1024
```

### –•—É–≤–∏–ª–±–∞—Ä 3: Feature Engineering

```python
# –ò–ª“Ø“Ø –æ–ª–æ–Ω —Ç–µ—Ö–Ω–∏–∫–∏–π–Ω –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –Ω—ç–º—ç—Ö
# backend/ml/features/technical_indicators.py —Ñ–∞–π–ª–¥:

# Ichimoku Cloud
# Fibonacci Retracement
# Support/Resistance levels
# Order flow indicators
# Market microstructure features
```

### –•—É–≤–∏–ª–±–∞—Ä 4: Ensemble Models

```python
# –û–ª–æ–Ω –º–æ–¥–µ–ª–∏–π–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª—ã–≥ –Ω—ç–≥—Ç–≥—ç—Ö
model_1 = Transformer_LSTM()
model_2 = BiLSTM_Attention()
model_3 = CNN_LSTM()

# –î—É–Ω–¥–∞–∂ –∞–≤–∞—Ö —ç—Å–≤—ç–ª —Å–∞–Ω–∞–ª ”©–≥”©—Ö —Å–∏—Å—Ç–µ–º–∏–π–≥ –∞—à–∏–≥–ª–∞—Ö
final_prediction = average([model_1, model_2, model_3])
```

---

## üöÄ –¢—É—Ä—à–∏–ª—Ç —Ö–∏–π—Ö –¥–∞—Ä–∞–∞–ª–∞–ª (Experimentation Order)

1. **–≠—Ö–ª—ç—ç–¥** - –û–¥–æ–æ–≥–∏–π–Ω —Ç–æ—Ö–∏—Ä–≥–æ–æ–≥–æ–æ—Ä —Å—É—Ä–≥–∞—Ö
2. **–•—ç—Ä—ç–≤ accuracy < 50%** - Batch size = 32, Learning rate = 0.0005
3. **–•—ç—Ä—ç–≤ accuracy 50-60%** - LSTM units –Ω—ç–º—ç—Ö: [512, 256]
4. **–•—ç—Ä—ç–≤ accuracy 60-70%** - Sequence length –Ω—ç–º—ç—Ö: 240
5. **–•—ç—Ä—ç–≤ accuracy 70-75%** - Ensemble models —Ç—É—Ä—à–∏–∂ “Ø–∑—ç—Ö
6. **–•—ç—Ä—ç–≤ accuracy 75%+** - Production-–¥ –±—ç–ª—ç–Ω! üéâ

---

## ‚ö†Ô∏è –ê–Ω—Ö–∞–∞—Ä—É—É–ª–≥–∞ (Warnings)

### Overfitting-—Å —Å—ç—Ä–≥–∏–π–ª—ç—Ö

–•—ç—Ä—ç–≤ training accuracy > validation accuracy:
```python
# Dropout-–≥ –Ω—ç–º—ç–≥–¥“Ø“Ø–ª—ç—Ö
dropout_rate = 0.5

# –≠—Å–≤—ç–ª L2 regularization –Ω—ç–º—ç—Ö
keras.regularizers.l2(0.01)
```

### Underfitting-—Å —Å—ç—Ä–≥–∏–π–ª—ç—Ö

–•—ç—Ä—ç–≤ —Ö–æ—ë—É–ª–∞–Ω–¥ –Ω—å accuracy –±–∞–≥–∞:
```python
# –ú–æ–¥–µ–ª—å —Ç–æ–º –±–æ–ª–≥–æ—Ö
lstm_units = [512, 256, 128]

# –≠—Å–≤—ç–ª –∏–ª“Ø“Ø –∏—Ö —Å—É—Ä–≥–∞—Ö
EPOCHS = 200
```

---

## üìà “Æ—Ä –¥“Ø–Ω–≥ —Ö—ç–º–∂–∏—Ö (Measuring Success)

### –•“Ø–ª—ç—ç–≥–¥—ç–∂ –±—É–π accuracy:

| Accuracy | “Æ–Ω—ç–ª–≥—ç—ç | –®–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π “Ø–π–ª–¥—ç–ª |
|----------|---------|---------------------|
| < 40%    | –ú–∞—à –º—É—É | –ú–æ–¥–µ–ª–∏–π–≥ –±“Ø—Ö—ç–ª–¥ –Ω—å –¥–∞—Ö–∏–Ω –±–æ–¥–æ—Ö |
| 40-50%   | –ú—É—É     | Hyperparameter-“Ø“Ø–¥–∏–π–≥ –∏—Ö ”©”©—Ä—á–ª”©—Ö |
| 50-60%   | –î—É–Ω–¥    | –°–∞–π–∂—Ä—É—É–ª–∞–ª—Ç —Ö–∏–π—Ö |
| 60-70%   | –°–∞–π–Ω    | –ñ–∏–∂–∏–≥ —Å–∞–π–∂—Ä—É—É–ª–∞–ª—Ç |
| 70-80%   | –ú–∞—à —Å–∞–π–Ω | Production-–¥ –∞—à–∏–≥–ª–∞–∂ –±–æ–ª–Ω–æ |
| > 80%    | –ì–∞–π—Ö–∞–ª—Ç–∞–π | Overfitting —à–∞–ª–≥–∞—Ö! |

---

## üéì –¢—É—Ä—à–∏–ª—Ç —Ö–∏–π—Ö–¥—ç—ç —Å–∞–Ω–∞—Ö –∑“Ø–π–ª

1. **–ù—ç–≥ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã–≥ –Ω—ç–≥ —É–¥–∞–∞ ”©”©—Ä—á–ª”©—Ö** - –•—ç–¥—ç–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä –∑—ç—Ä—ç–≥ ”©”©—Ä—á–ª”©–≤”©–ª —é—É –∞–∂–∏–ª–ª–∞—Å–∞–Ω –º—ç–¥—ç—Ö–≥“Ø–π
2. **“Æ—Ä –¥“Ø–Ω–≥ –±“Ø—Ä—Ç–≥—ç—Ö** - –¢–æ—Ö–∏—Ä–≥–æ–æ –±–æ–ª–≥–æ–Ω–¥ —ç—Ö–ª—ç—Ö accuracy, —Ç”©–≥—Å–≥”©–ª–∏–π–Ω accuracy –±–∏—á–∏—Ö
3. **Random seed-–≥ —Ç–æ–≥—Ç–æ–æ—Å–æ–Ω –±–∞–π—Ö** - –ò–∂–∏–ª “Ø—Ä –¥“Ø–Ω–≥ –¥–∞—Ö–∏–Ω –∞–≤–∞—Ö –±–æ–ª–æ–º–∂—Ç–æ–π
4. **Validation set –¥—ç—ç—Ä —à–∞–ª–≥–∞—Ö** - Test set-–≥ –∑”©–≤—Ö”©–Ω —ç—Ü—Å–∏–π–Ω “Ø–Ω—ç–ª–≥—ç—ç–Ω–¥ –∞—à–∏–≥–ª–∞—Ö

---

## üí° –ù—ç–º—ç–ª—Ç –∑”©–≤–ª”©–º–∂“Ø“Ø–¥

### Data Preprocessing
- Outlier-“Ø“Ø–¥–∏–π–≥ —Ü—ç–≤—ç—Ä–ª—ç—Ö
- Feature scaling (StandardScaler —ç—Å–≤—ç–ª MinMaxScaler)
- –ê–ª–≥–∞ ”©–≥”©–≥–¥–ª–∏–π–≥ –∑–∞—Å–∞—Ö

### Training Strategy
- Cross-validation –∞—à–∏–≥–ª–∞—Ö
- Walk-forward validation (time series-–¥ —Ç–æ—Ö–∏—Ä–æ–º–∂—Ç–æ–π)
- Early stopping patience –Ω—ç–º—ç–≥–¥“Ø“Ø–ª—ç—Ö

### Model Architecture
- Residual connections —Ç—É—Ä—à–∏–∂ “Ø–∑—ç—Ö
- Batch normalization –Ω—ç–º—ç—Ö
- Different attention mechanisms

---

## üìö –ù–æ–º –∑“Ø–π (References)

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer architecture
- [LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) - LSTM explained
- [Keras Tuner](https://keras.io/keras_tuner/) - Automated hyperparameter tuning

---

**Good luck! –ê–º–∂–∏–ª—Ç —Ö“Ø—Å—å–µ! üöÄ**
