{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "558ed385",
   "metadata": {},
   "source": [
    "# ğŸš€ Multi-Currency Multi-Timeframe Deep Learning Training\n",
    "\n",
    "## Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ¸Ğ¹Ğ½ Ñ‚Ğ¾Ğ´Ğ¾Ñ€Ñ…Ğ¾Ğ¹Ğ»Ğ¾Ğ»Ñ‚\n",
    "\n",
    "Ğ­Ğ½Ñ notebook Ğ½ÑŒ **3 Ó©Ó©Ñ€ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ°Ñ€** Ğ³ÑƒÑ€Ğ²Ğ°Ğ½ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑƒÑ€Ğ³Ğ°Ğ½Ğ°:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  TRAINING DATA (ALL PAIRS COMBINED)             â”‚\n",
    "â”‚  - EUR/USD, GBP/USD, USD/JPY                    â”‚\n",
    "â”‚  - USD/CAD, USD/CHF, XAU/USD                    â”‚\n",
    "â”‚  Total: ~3-4M 1-minute candles                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  3 PARALLEL MODELS WITH DIFFERENT ARCHITECTURES â”‚\n",
    "â”‚                                                 â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚ 15-min Model   â”‚ Transformer + LSTM    â”‚    â”‚\n",
    "â”‚  â”‚ Expected: 88%  â”‚ Focus: Quick scalping â”‚    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â”‚                                                 â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚ 30-min Model   â”‚ Bi-LSTM + Attention   â”‚    â”‚\n",
    "â”‚  â”‚ Expected: 85%  â”‚ Focus: Swing trades   â”‚    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â”‚                                                 â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚ 60-min Model   â”‚ CNN-LSTM Hybrid       â”‚    â”‚\n",
    "â”‚  â”‚ Expected: 82%  â”‚ Focus: Trend followingâ”‚    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  TEST ON ALL PAIRS                              â”‚\n",
    "â”‚  - Overall accuracy per model                   â”‚\n",
    "â”‚  - Per-pair performance breakdown               â”‚\n",
    "â”‚  - Architecture comparison                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Ğ—Ğ¾Ñ€Ğ¸Ğ»Ğ³Ğ¾\n",
    "\n",
    "- ğŸ¯ **15-Ğ¼Ğ¸Ğ½ÑƒÑ‚ (Transformer+LSTM):** 88%+ accuracy (scalping)\n",
    "- ğŸ¯ **30-Ğ¼Ğ¸Ğ½ÑƒÑ‚ (Bi-LSTM+Attention):** 85%+ accuracy (swing trading)\n",
    "- ğŸ¯ **60-Ğ¼Ğ¸Ğ½ÑƒÑ‚ (CNN-LSTM):** 82%+ accuracy (trend following)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa13087",
   "metadata": {},
   "source": [
    "## ğŸ“¦ 1. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87573b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add backend to path\n",
    "sys.path.append(str(Path.cwd().parent / 'backend'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks, optimizers\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Custom modules\n",
    "from ml.features.technical_indicators import calculate_all_features\n",
    "from ml.preprocessing.data_loader import ForexDataLoader\n",
    "from ml.preprocessing.sequence_generator import create_sequences\n",
    "\n",
    "# Import different model architectures\n",
    "from ml.models.transformer_lstm import build_transformer_lstm_model\n",
    "from ml.models.bilstm_attention import build_bilstm_attention_model\n",
    "from ml.models.cnn_lstm import build_cnn_lstm_model\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"âœ… TensorFlow version: {tf.__version__}\")\n",
    "print(f\"âœ… GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"âœ… Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd0722f",
   "metadata": {},
   "source": [
    "## âš™ï¸ 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9806fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = Path.cwd().parent / 'data'\n",
    "TRAIN_DIR = DATA_DIR / 'train'\n",
    "TEST_DIR = DATA_DIR / 'test'\n",
    "MODELS_DIR = Path.cwd().parent / 'models'\n",
    "LOGS_DIR = Path.cwd().parent / 'logs'\n",
    "\n",
    "# Create directories\n",
    "for timeframe in ['15min', '30min', '60min']:\n",
    "    (MODELS_DIR / timeframe).mkdir(parents=True, exist_ok=True)\n",
    "    (LOGS_DIR / timeframe / 'train').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Currency pairs\n",
    "CURRENCY_PAIRS = ['EUR_USD', 'GBP_USD', 'USD_JPY', 'USD_CAD', 'USD_CHF', 'XAU_USD']\n",
    "\n",
    "# Training configuration - DIFFERENT ARCHITECTURES\n",
    "CONFIG = {\n",
    "    '15min': {\n",
    "        'architecture': 'transformer_lstm',  # Scalping strategy\n",
    "        'resample_period': '15T',\n",
    "        'sequence_length': 60,\n",
    "        'prediction_steps': 1,\n",
    "        'n_heads': 8,\n",
    "        'ff_dim': 256,\n",
    "        'lstm_units': [128, 64],\n",
    "        'dropout': 0.3,\n",
    "        'batch_size': 128,  # Increased for memory efficiency\n",
    "        'epochs': 30,  # Reduced for faster training\n",
    "        'learning_rate': 0.001,\n",
    "        'description': 'Transformer + LSTM for quick scalping',\n",
    "        'sample_ratio': 1\n",
    "    },\n",
    "    '30min': {\n",
    "        'architecture': 'bilstm_attention',  # Swing trading\n",
    "        'resample_period': '30T',\n",
    "        'sequence_length': 48,\n",
    "        'prediction_steps': 1,\n",
    "        'lstm_units': [128, 64],\n",
    "        'attention_units': 128,\n",
    "        'dropout': 0.3,\n",
    "        'batch_size': 128,  # Increased for memory efficiency\n",
    "        'epochs': 30,  # Reduced for faster training\n",
    "        'learning_rate': 0.001,\n",
    "        'description': 'Bi-LSTM + Attention for swing trades',\n",
    "        'sample_ratio': 1\n",
    "    },\n",
    "    '60min': {\n",
    "        'architecture': 'cnn_lstm',  # Trend following\n",
    "        'resample_period': '60T',\n",
    "        'sequence_length': 48,\n",
    "        'prediction_steps': 1,\n",
    "        'cnn_filters': [64, 128, 64],\n",
    "        'kernel_size': 3,\n",
    "        'lstm_units': [128, 64],\n",
    "        'dropout': 0.3,\n",
    "        'batch_size': 128,  # Increased for memory efficiency\n",
    "        'epochs': 30,  # Reduced for faster training\n",
    "        'learning_rate': 0.001,\n",
    "        'description': 'CNN-LSTM for trend following',\n",
    "        'sample_ratio': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ… Configuration loaded\")\n",
    "print(f\"ğŸ“ Data directory: {DATA_DIR}\")\n",
    "print(f\"ğŸ’¾ Models directory: {MODELS_DIR}\")\n",
    "print(f\"ğŸ“Š Currency pairs: {', '.join(CURRENCY_PAIRS)}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ—ï¸  ARCHITECTURE CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "for tf, cfg in CONFIG.items():\n",
    "    print(f\"\\n{tf}:\")\n",
    "    print(f\"  Architecture: {cfg['architecture']}\")\n",
    "    print(f\"  Strategy: {cfg['description']}\")\n",
    "    print(f\"  Sample ratio: {cfg['sample_ratio']*100:.0f}%\")\n",
    "    print(f\"  Batch size: {cfg['batch_size']}\")\n",
    "    print(f\"  Epochs: {cfg['epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916327a",
   "metadata": {},
   "source": [
    "## ğŸ“¥ 3. Load ALL Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_training_data():\n",
    "    \"\"\"\n",
    "    Load and combine ALL currency pairs from data/train/\n",
    "    \n",
    "    Returns:\n",
    "        combined_df: DataFrame with all pairs combined\n",
    "        pair_stats: Statistics per pair\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“¥ LOADING ALL TRAINING DATA\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    loader = ForexDataLoader(data_dir=DATA_DIR)\n",
    "    all_dfs = []\n",
    "    pair_stats = []\n",
    "    \n",
    "    for pair in CURRENCY_PAIRS:\n",
    "        print(f\"Loading {pair}...\")\n",
    "        df = loader.load_train_data(pair=pair)\n",
    "        \n",
    "        if df is not None:\n",
    "            # Add pair identifier\n",
    "            df['pair'] = pair\n",
    "            all_dfs.append(df)\n",
    "            \n",
    "            pair_stats.append({\n",
    "                'pair': pair,\n",
    "                'rows': len(df),\n",
    "                'start': df.index.min(),\n",
    "                'end': df.index.max()\n",
    "            })\n",
    "            print(f\"  âœ… {len(df):,} rows\")\n",
    "        else:\n",
    "            print(f\"  âŒ Failed to load\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(all_dfs, axis=0)\n",
    "    combined_df = combined_df.sort_index()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"âœ… COMBINED TRAINING DATA\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total rows: {len(combined_df):,}\")\n",
    "    print(f\"Date range: {combined_df.index.min()} to {combined_df.index.max()}\")\n",
    "    print(f\"Columns: {combined_df.columns.tolist()}\")\n",
    "    \n",
    "    # Statistics table\n",
    "    stats_df = pd.DataFrame(pair_stats)\n",
    "    print(f\"\\nğŸ“Š Per-Pair Statistics:\")\n",
    "    print(stats_df.to_string(index=False))\n",
    "    \n",
    "    return combined_df, stats_df\n",
    "\n",
    "# Load all training data\n",
    "df_train_all, train_stats = load_all_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429e507",
   "metadata": {},
   "source": [
    "## ğŸ“¥ 4. Load ALL Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b8c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_test_data():\n",
    "    \"\"\"\n",
    "    Load and combine ALL currency pairs from data/test/\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“¥ LOADING ALL TEST DATA\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    loader = ForexDataLoader(data_dir=DATA_DIR)\n",
    "    all_dfs = []\n",
    "    pair_stats = []\n",
    "    \n",
    "    for pair in CURRENCY_PAIRS:\n",
    "        print(f\"Loading {pair} test data...\")\n",
    "        df = loader.load_test_data(pair=pair)\n",
    "        \n",
    "        if df is not None:\n",
    "            df['pair'] = pair\n",
    "            all_dfs.append(df)\n",
    "            \n",
    "            pair_stats.append({\n",
    "                'pair': pair,\n",
    "                'rows': len(df)\n",
    "            })\n",
    "            print(f\"  âœ… {len(df):,} rows\")\n",
    "        else:\n",
    "            print(f\"  âŒ Failed to load\")\n",
    "    \n",
    "    combined_df = pd.concat(all_dfs, axis=0)\n",
    "    combined_df = combined_df.sort_index()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"âœ… COMBINED TEST DATA\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total rows: {len(combined_df):,}\")\n",
    "    \n",
    "    stats_df = pd.DataFrame(pair_stats)\n",
    "    print(f\"\\nğŸ“Š Per-Pair Statistics:\")\n",
    "    print(stats_df.to_string(index=False))\n",
    "    \n",
    "    return combined_df, stats_df\n",
    "\n",
    "# Load all test data\n",
    "df_test_all, test_stats = load_all_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bef60b",
   "metadata": {},
   "source": [
    "## ğŸ”§ 5. Feature Engineering for Multi-Currency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f8e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_multi_currency_data(df, timeframe_config, fit_scaler=None, fit_encoder=None):\n",
    "    \"\"\"\n",
    "    Prepare data for specific timeframe with multi-currency support\n",
    "    \n",
    "    Key difference: Adds pair encoding as feature\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ”„ Processing multi-currency data for {timeframe_config['resample_period']}...\")\n",
    "    \n",
    "    all_pairs_data = []\n",
    "    \n",
    "    # Process each pair separately then combine\n",
    "    for pair in CURRENCY_PAIRS:\n",
    "        print(f\"  Processing {pair}...\")\n",
    "        \n",
    "        # Filter data for this pair\n",
    "        df_pair = df[df['pair'] == pair].copy()\n",
    "        df_pair = df_pair.drop('pair', axis=1)\n",
    "        \n",
    "        if len(df_pair) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Resample\n",
    "        df_resampled = df_pair.resample(timeframe_config['resample_period']).agg({\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'close': 'last',\n",
    "            'tick_volume': 'sum'\n",
    "        }).dropna()\n",
    "        \n",
    "        # Calculate features\n",
    "        df_features = calculate_all_features(df_resampled)\n",
    "        \n",
    "        # Create labels\n",
    "        prediction_steps = timeframe_config['prediction_steps']\n",
    "        df_features['future_return'] = df_features['close'].shift(-prediction_steps) / df_features['close'] - 1\n",
    "        \n",
    "        threshold = 0.0005\n",
    "        conditions = [\n",
    "            df_features['future_return'] < -threshold,\n",
    "            (df_features['future_return'] >= -threshold) & (df_features['future_return'] <= threshold),\n",
    "            df_features['future_return'] > threshold\n",
    "        ]\n",
    "        df_features['label'] = np.select(conditions, [0, 1, 2], default=1)\n",
    "        \n",
    "        # Add pair identifier back\n",
    "        df_features['pair'] = pair\n",
    "        \n",
    "        df_features = df_features.dropna()\n",
    "        all_pairs_data.append(df_features)\n",
    "        print(f\"    âœ… {len(df_features):,} rows\")\n",
    "    \n",
    "    # Combine all pairs\n",
    "    df_combined = pd.concat(all_pairs_data, axis=0)\n",
    "    df_combined = df_combined.sort_index()\n",
    "    \n",
    "    print(f\"\\nâœ… Combined: {len(df_combined):,} rows\")\n",
    "    print(f\"\\nğŸ“Š Label distribution:\")\n",
    "    print(df_combined['label'].value_counts())\n",
    "    print(f\"\\nClass percentages:\")\n",
    "    print(df_combined['label'].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    # Encode pair as one-hot\n",
    "    if fit_encoder is None:\n",
    "        pair_encoder = LabelEncoder()\n",
    "        pair_encoded = pair_encoder.fit_transform(df_combined['pair'])\n",
    "    else:\n",
    "        pair_encoder = fit_encoder\n",
    "        pair_encoded = pair_encoder.transform(df_combined['pair'])\n",
    "    \n",
    "    # One-hot encode pair (6 columns)\n",
    "    pair_onehot = pd.get_dummies(pair_encoded, prefix='pair')\n",
    "    \n",
    "    # Separate features and labels\n",
    "    feature_cols = [col for col in df_combined.columns \n",
    "                   if col not in ['label', 'future_return', 'open', 'high', 'low', 'close', 'pair']]\n",
    "    \n",
    "    X = df_combined[feature_cols].values\n",
    "    y = df_combined['label'].values\n",
    "    \n",
    "    # Add pair encoding to features\n",
    "    X = np.concatenate([X, pair_onehot.values], axis=1)\n",
    "    feature_cols_with_pair = feature_cols + [f'pair_{i}' for i in range(len(CURRENCY_PAIRS))]\n",
    "    \n",
    "    # Normalize\n",
    "    print(\"\\nğŸ”„ Normalizing features...\")\n",
    "    if fit_scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "    else:\n",
    "        scaler = fit_scaler\n",
    "        X_scaled = scaler.transform(X)\n",
    "    \n",
    "    print(f\"âœ… Final shape: X={X_scaled.shape}, y={y.shape}\")\n",
    "    print(f\"âœ… Total features: {len(feature_cols_with_pair)}\")\n",
    "    \n",
    "    return X_scaled, y, scaler, pair_encoder, feature_cols_with_pair, df_combined['pair'].values\n",
    "\n",
    "print(\"âœ… Multi-currency feature engineering function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b434ccb6",
   "metadata": {},
   "source": [
    "## ğŸ‹ï¸ 6. Train Models on ALL Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47842cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multi_currency_model(timeframe):\n",
    "    \"\"\"\n",
    "    Train model on ALL currency pairs combined\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ğŸš€ TRAINING {timeframe.upper()} MODEL ON ALL PAIRS\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    config = CONFIG[timeframe]\n",
    "    sample_ratio = config.get('sample_ratio', 1.0)\n",
    "    \n",
    "    # Prepare training data\n",
    "    print(f\"ğŸ”§ Preparing training data (all pairs, {sample_ratio*100:.0f}% sample)...\")\n",
    "    X_train, y_train, scaler, pair_encoder, feature_cols, train_pairs = prepare_multi_currency_data(\n",
    "        df_train_all, config\n",
    "    )\n",
    "    \n",
    "    # Sample data if needed\n",
    "    if sample_ratio < 1.0:\n",
    "        n_samples = int(len(X_train) * sample_ratio)\n",
    "        indices = np.random.choice(len(X_train), n_samples, replace=False)\n",
    "        indices.sort()  # Keep temporal order\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "        train_pairs = train_pairs[indices]\n",
    "        print(f\"ğŸ“‰ Sampled to {len(X_train):,} rows ({sample_ratio*100:.0f}%)\")\n",
    "    \n",
    "    # Prepare test data\n",
    "    print(f\"\\nğŸ”§ Preparing test data (all pairs)...\")\n",
    "    X_test, y_test, _, _, _, test_pairs = prepare_multi_currency_data(\n",
    "        df_test_all, config, fit_scaler=scaler, fit_encoder=pair_encoder\n",
    "    )\n",
    "    \n",
    "    # Create sequences\n",
    "    print(f\"\\nğŸ”„ Creating sequences (length={config['sequence_length']})...\")\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train, y_train, config['sequence_length'])\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test, y_test, config['sequence_length'])\n",
    "    \n",
    "    # Keep track of pairs for test sequences\n",
    "    test_pairs_seq = test_pairs[config['sequence_length']:]\n",
    "    \n",
    "    print(f\"âœ… Training sequences: {X_train_seq.shape}\")\n",
    "    print(f\"âœ… Test sequences: {X_test_seq.shape}\")\n",
    "    \n",
    "    # Convert to float32 to save memory\n",
    "    X_train_seq = X_train_seq.astype(np.float32)\n",
    "    X_test_seq = X_test_seq.astype(np.float32)\n",
    "    y_train_seq = y_train_seq.astype(np.int32)\n",
    "    y_test_seq = y_test_seq.astype(np.int32)\n",
    "    print(f\"âœ… Converted to float32/int32 for memory efficiency\")\n",
    "    \n",
    "    # Split validation\n",
    "    X_train_seq, X_val_seq, y_train_seq, y_val_seq = train_test_split(\n",
    "        X_train_seq, y_train_seq, test_size=0.2, random_state=42, stratify=y_train_seq\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Final splits:\")\n",
    "    print(f\"   Train: {X_train_seq.shape}\")\n",
    "    print(f\"   Val:   {X_val_seq.shape}\")\n",
    "    print(f\"   Test:  {X_test_seq.shape}\")\n",
    "    \n",
    "    # Build model - DIFFERENT ARCHITECTURE PER TIMEFRAME\n",
    "    print(f\"\\nğŸ—ï¸  Building {config['architecture']} model...\")\n",
    "    print(f\"ğŸ“ Strategy: {config['description']}\")\n",
    "    n_features = X_train_seq.shape[2]\n",
    "    \n",
    "    if timeframe == '15min':\n",
    "        # Transformer + LSTM for scalping\n",
    "        model = build_transformer_lstm_model(\n",
    "            sequence_length=config['sequence_length'],\n",
    "            n_features=n_features,\n",
    "            n_heads=config['n_heads'],\n",
    "            ff_dim=config['ff_dim'],\n",
    "            lstm_units=config['lstm_units'],\n",
    "            dropout_rate=config['dropout'],\n",
    "            n_classes=3\n",
    "        )\n",
    "    elif timeframe == '30min':\n",
    "        # Bi-LSTM + Attention for swing trading\n",
    "        model = build_bilstm_attention_model(\n",
    "            sequence_length=config['sequence_length'],\n",
    "            n_features=n_features,\n",
    "            lstm_units=config['lstm_units'],\n",
    "            attention_units=config['attention_units'],\n",
    "            dropout_rate=config['dropout'],\n",
    "            n_classes=3\n",
    "        )\n",
    "    else:  # 60min\n",
    "        # CNN-LSTM for trend following\n",
    "        model = build_cnn_lstm_model(\n",
    "            sequence_length=config['sequence_length'],\n",
    "            n_features=n_features,\n",
    "            cnn_filters=config['cnn_filters'],\n",
    "            kernel_size=config['kernel_size'],\n",
    "            lstm_units=config['lstm_units'],\n",
    "            dropout_rate=config['dropout'],\n",
    "            n_classes=3\n",
    "        )\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=config['learning_rate']),\n",
    "        loss={\n",
    "            'direction': 'sparse_categorical_crossentropy',\n",
    "            'confidence': 'mse'\n",
    "        },\n",
    "        loss_weights={'direction': 1.0, 'confidence': 0.5},\n",
    "        metrics={\n",
    "            'direction': ['accuracy'],\n",
    "            'confidence': ['mae']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    # Callbacks\n",
    "    model_path = MODELS_DIR / timeframe / f\"multi_currency_{timeframe}_best.keras\"\n",
    "    log_dir = LOGS_DIR / timeframe / 'train' / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    callbacks_list = [\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath=str(model_path),\n",
    "            monitor='val_direction_accuracy',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='val_direction_accuracy',\n",
    "            patience=7,  # Reduced patience\n",
    "            mode='max',\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,  # Reduced patience\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        callbacks.TensorBoard(\n",
    "            log_dir=str(log_dir),\n",
    "            histogram_freq=0  # Disabled for speed\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nğŸ‹ï¸  Training model on ALL currency pairs...\")\n",
    "    print(f\"ğŸ“Š Batch size: {config['batch_size']}\")\n",
    "    print(f\"ğŸ“Š Epochs: {config['epochs']}\")\n",
    "    print(f\"âš¡ Optimized for memory and speed\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_seq,\n",
    "        {\n",
    "            'direction': y_train_seq,\n",
    "            'confidence': np.ones_like(y_train_seq, dtype=np.float32)\n",
    "        },\n",
    "        validation_data=(\n",
    "            X_val_seq,\n",
    "            {\n",
    "                'direction': y_val_seq,\n",
    "                'confidence': np.ones_like(y_val_seq, dtype=np.float32)\n",
    "            }\n",
    "        ),\n",
    "        batch_size=config['batch_size'],\n",
    "        epochs=config['epochs'],\n",
    "        callbacks=callbacks_list,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Test\n",
    "    print(f\"\\nğŸ“Š Testing on all pairs...\")\n",
    "    predictions = model.predict(X_test_seq, batch_size=config['batch_size'])\n",
    "    y_pred = np.argmax(predictions[0], axis=1)\n",
    "    y_confidence = predictions[1].flatten()\n",
    "    \n",
    "    # Overall accuracy\n",
    "    accuracy = accuracy_score(y_test_seq, y_pred)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ğŸ“ˆ OVERALL TEST RESULTS FOR {timeframe.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nğŸ¯ Overall Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(\"\\nğŸ“Š Classification Report:\")\n",
    "    print(classification_report(y_test_seq, y_pred, target_names=['SELL', 'NEUTRAL', 'BUY']))\n",
    "    \n",
    "    # Per-pair accuracy\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“Š PER-PAIR ACCURACY BREAKDOWN\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    pair_results = []\n",
    "    for pair in CURRENCY_PAIRS:\n",
    "        mask = test_pairs_seq == pair\n",
    "        if mask.sum() > 0:\n",
    "            pair_acc = accuracy_score(y_test_seq[mask], y_pred[mask])\n",
    "            pair_results.append({\n",
    "                'Pair': pair,\n",
    "                'Samples': mask.sum(),\n",
    "                'Accuracy': f\"{pair_acc*100:.2f}%\"\n",
    "            })\n",
    "            print(f\"{pair:8s}: {pair_acc*100:.2f}% ({mask.sum():,} samples)\")\n",
    "    \n",
    "    # Save artifacts\n",
    "    scaler_path = MODELS_DIR / timeframe / f\"multi_currency_{timeframe}_scaler.pkl\"\n",
    "    encoder_path = MODELS_DIR / timeframe / f\"multi_currency_{timeframe}_encoder.pkl\"\n",
    "    \n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    joblib.dump(pair_encoder, encoder_path)\n",
    "    \n",
    "    metadata = {\n",
    "        'timeframe': timeframe,\n",
    "        'architecture': config['architecture'],\n",
    "        'strategy': config['description'],\n",
    "        'training_mode': 'multi_currency',\n",
    "        'sample_ratio': sample_ratio,\n",
    "        'pairs': CURRENCY_PAIRS,\n",
    "        'n_features': n_features,\n",
    "        'sequence_length': config['sequence_length'],\n",
    "        'feature_columns': feature_cols,\n",
    "        'train_samples': len(X_train_seq),\n",
    "        'test_samples': len(X_test_seq),\n",
    "        'overall_accuracy': float(accuracy),\n",
    "        'per_pair_results': pair_results,\n",
    "        'training_date': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    metadata_path = MODELS_DIR / timeframe / f\"multi_currency_{timeframe}_metadata.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Model saved: {model_path}\")\n",
    "    print(f\"ğŸ’¾ Scaler saved: {scaler_path}\")\n",
    "    print(f\"ğŸ’¾ Encoder saved: {encoder_path}\")\n",
    "    print(f\"ğŸ’¾ Metadata saved: {metadata_path}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred,\n",
    "        'confidence': y_confidence,\n",
    "        'true_labels': y_test_seq,\n",
    "        'test_pairs': test_pairs_seq,\n",
    "        'pair_results': pair_results\n",
    "    }\n",
    "\n",
    "print(\"âœ… Multi-currency training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df449102",
   "metadata": {},
   "source": [
    "## ğŸš€ 7. Train 15-Minute Model (Transformer + LSTM)\n",
    "\n",
    "**Strategy:** Quick scalping  \n",
    "**Architecture:** Multi-head attention + LSTM layers  \n",
    "**Target Accuracy:** 88%+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10266aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 15-minute model\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ¯ TRAINING 15-MINUTE MODEL (TRANSFORMER + LSTM)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    result_15min = train_multi_currency_model('15min')\n",
    "    print(f\"\\nâœ… 15-minute model training completed!\")\n",
    "    print(f\"ğŸ¯ Accuracy: {result_15min['accuracy']*100:.2f}%\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error training 15-minute model: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    result_15min = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16694593",
   "metadata": {},
   "source": [
    "## ğŸš€ 8. Train 30-Minute Model (Bi-LSTM + Attention)\n",
    "\n",
    "**Strategy:** Swing trading  \n",
    "**Architecture:** Bidirectional LSTM + Custom attention mechanism  \n",
    "**Target Accuracy:** 85%+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 30-minute model\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ¯ TRAINING 30-MINUTE MODEL (BI-LSTM + ATTENTION)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    result_30min = train_multi_currency_model('30min')\n",
    "    print(f\"\\nâœ… 30-minute model training completed!\")\n",
    "    print(f\"ğŸ¯ Accuracy: {result_30min['accuracy']*100:.2f}%\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error training 30-minute model: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    result_30min = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f4f1d3",
   "metadata": {},
   "source": [
    "## ğŸš€ 9. Train 60-Minute Model (CNN-LSTM Hybrid)\n",
    "\n",
    "**Strategy:** Trend following  \n",
    "**Architecture:** Convolutional layers + LSTM for temporal modeling  \n",
    "**Target Accuracy:** 82%+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8271bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 60-minute model\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ¯ TRAINING 60-MINUTE MODEL (CNN-LSTM HYBRID)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    result_60min = train_multi_currency_model('60min')\n",
    "    print(f\"\\nâœ… 60-minute model training completed!\")\n",
    "    print(f\"ğŸ¯ Accuracy: {result_60min['accuracy']*100:.2f}%\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error training 60-minute model: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    result_60min = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05953e9e",
   "metadata": {},
   "source": [
    "## ğŸ“Š 10. Collect All Results\n",
    "\n",
    "Combine results from all 3 models for visualization and comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd5733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "results = {}\n",
    "\n",
    "if 'result_15min' in locals() and result_15min is not None:\n",
    "    results['15min'] = result_15min\n",
    "    print(f\"âœ… 15-minute model: {result_15min['accuracy']*100:.2f}%\")\n",
    "else:\n",
    "    print(\"âŒ 15-minute model not trained\")\n",
    "\n",
    "if 'result_30min' in locals() and result_30min is not None:\n",
    "    results['30min'] = result_30min\n",
    "    print(f\"âœ… 30-minute model: {result_30min['accuracy']*100:.2f}%\")\n",
    "else:\n",
    "    print(\"âŒ 30-minute model not trained\")\n",
    "\n",
    "if 'result_60min' in locals() and result_60min is not None:\n",
    "    results['60min'] = result_60min\n",
    "    print(f\"âœ… 60-minute model: {result_60min['accuracy']*100:.2f}%\")\n",
    "else:\n",
    "    print(\"âŒ 60-minute model not trained\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"ğŸ‰ TRAINING SUMMARY: {len(results)}/3 models completed\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5520a4f3",
   "metadata": {},
   "source": [
    "## ğŸ“Š 11. Visualize Results\n",
    "\n",
    "Generate comparison plots and charts for all trained models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e7c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Training History - 3 Different Architectures\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Training History - 3 Different Architectures', fontsize=16, fontweight='bold')\n",
    "\n",
    "architectures = {\n",
    "    '15min': 'Transformer+LSTM',\n",
    "    '30min': 'Bi-LSTM+Attention',\n",
    "    '60min': 'CNN-LSTM'\n",
    "}\n",
    "\n",
    "for idx, (timeframe, result) in enumerate(results.items()):\n",
    "    if 'history' not in result:\n",
    "        continue\n",
    "    \n",
    "    history = result['history']\n",
    "    arch_name = architectures[timeframe]\n",
    "    \n",
    "    # Accuracy\n",
    "    ax1 = axes[0, idx]\n",
    "    ax1.plot(history.history['direction_accuracy'], label='Train')\n",
    "    ax1.plot(history.history['val_direction_accuracy'], label='Validation')\n",
    "    ax1.set_title(f'{timeframe} ({arch_name})\\nAccuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss\n",
    "    ax2 = axes[1, idx]\n",
    "    ax2.plot(history.history['loss'], label='Train')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation')\n",
    "    ax2.set_title(f'{timeframe} ({arch_name})\\nLoss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODELS_DIR / 'training_history_multi_currency.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Training history plots saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Per-Pair Accuracy Comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Per-Pair Accuracy Across Timeframes', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (timeframe, result) in enumerate(results.items()):\n",
    "    if 'pair_results' not in result:\n",
    "        continue\n",
    "    \n",
    "    pair_df = pd.DataFrame(result['pair_results'])\n",
    "    pair_df['Accuracy_Val'] = pair_df['Accuracy'].str.rstrip('%').astype(float)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    bars = ax.bar(range(len(pair_df)), pair_df['Accuracy_Val'], color='skyblue', edgecolor='black')\n",
    "    ax.set_xticks(range(len(pair_df)))\n",
    "    ax.set_xticklabels(pair_df['Pair'], rotation=45, ha='right')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    arch_name = architectures[timeframe]\n",
    "    ax.set_title(f'{timeframe} ({arch_name})')\n",
    "    ax.set_ylim([0, 100])\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODELS_DIR / 'per_pair_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Per-pair accuracy plots saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5fb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Confusion Matrices - All Timeframes', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (timeframe, result) in enumerate(results.items()):\n",
    "    if 'predictions' not in result:\n",
    "        continue\n",
    "    \n",
    "    cm = confusion_matrix(result['true_labels'], result['predictions'])\n",
    "    arch_name = architectures[timeframe]\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=['SELL', 'NEUTRAL', 'BUY'],\n",
    "               yticklabels=['SELL', 'NEUTRAL', 'BUY'],\n",
    "               ax=ax)\n",
    "    ax.set_title(f'{timeframe} ({arch_name})\\nAcc: {result[\"accuracy\"]*100:.2f}%')\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODELS_DIR / 'confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Confusion matrices saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d547d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Architecture Comparison (Overall Accuracy)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "timeframes_list = []\n",
    "accuracies_list = []\n",
    "arch_labels = []\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "for timeframe, result in results.items():\n",
    "    if result is None:\n",
    "        continue\n",
    "    timeframes_list.append(timeframe)\n",
    "    accuracies_list.append(result['accuracy'] * 100)\n",
    "    arch_labels.append(architectures[timeframe])\n",
    "\n",
    "bars = ax.bar(range(len(timeframes_list)), accuracies_list, color=colors, edgecolor='black', linewidth=2)\n",
    "ax.set_xticks(range(len(timeframes_list)))\n",
    "ax.set_xticklabels([f\"{tf}\\n{arch}\" for tf, arch in zip(timeframes_list, arch_labels)], fontsize=11)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Architecture Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim([0, 100])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "            f'{height:.2f}%',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add target lines\n",
    "targets = {'15min': 88, '30min': 85, '60min': 82}\n",
    "for i, tf in enumerate(timeframes_list):\n",
    "    if tf in targets:\n",
    "        ax.axhline(y=targets[tf], xmin=(i-0.4)/len(timeframes_list), xmax=(i+0.4)/len(timeframes_list),\n",
    "                   color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "        ax.text(i, targets[tf] + 2, f'Target: {targets[tf]}%', \n",
    "                ha='center', fontsize=9, color='red', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODELS_DIR / 'architecture_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Architecture comparison plot saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e26fa",
   "metadata": {},
   "source": [
    "## ğŸ“Š 12. Summary Table\n",
    "\n",
    "Final comprehensive summary of all trained models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ba781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "architectures = {\n",
    "    '15min': 'Transformer+LSTM',\n",
    "    '30min': 'Bi-LSTM+Attention',\n",
    "    '60min': 'CNN-LSTM'\n",
    "}\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for timeframe, result in results.items():\n",
    "    if result is None:\n",
    "        continue\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Timeframe': timeframe,\n",
    "        'Architecture': architectures[timeframe],\n",
    "        'Overall Accuracy': f\"{result['accuracy']*100:.2f}%\",\n",
    "        'Test Samples': len(result['predictions']),\n",
    "        'High Conf (>85%)': f\"{(result['confidence'] > 0.85).sum()}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š FINAL SUMMARY - MULTI-CURRENCY MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv(MODELS_DIR / 'multi_currency_summary.csv', index=False)\n",
    "print(f\"\\nğŸ’¾ Summary saved: {MODELS_DIR / 'multi_currency_summary.csv'}\")\n",
    "\n",
    "# Display per-pair results for each timeframe\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š PER-PAIR ACCURACY BREAKDOWN\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for timeframe, result in results.items():\n",
    "    if 'pair_results' in result:\n",
    "        print(f\"\\n{timeframe.upper()} ({architectures[timeframe]}):\")\n",
    "        pair_df = pd.DataFrame(result['pair_results'])\n",
    "        print(pair_df.to_string(index=False))\n",
    "\n",
    "# Architecture details\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ—ï¸  ARCHITECTURE DETAILS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "arch_details = {\n",
    "    '15min': {\n",
    "        'name': 'Transformer + LSTM',\n",
    "        'layers': 'Multi-Head Attention (8 heads) â†’ LSTM (128, 64)',\n",
    "        'params': '~500K parameters',\n",
    "        'strength': 'Pattern recognition, short-term predictions',\n",
    "        'ideal_for': 'Scalping, 15-min trades'\n",
    "    },\n",
    "    '30min': {\n",
    "        'name': 'Bi-LSTM + Attention',\n",
    "        'layers': 'Bi-LSTM (128, 64) â†’ Custom Attention (128)',\n",
    "        'params': '~400K parameters',\n",
    "        'strength': 'Bidirectional context, focus mechanism',\n",
    "        'ideal_for': 'Swing trading, 30-min to 1-hour'\n",
    "    },\n",
    "    '60min': {\n",
    "        'name': 'CNN-LSTM Hybrid',\n",
    "        'layers': 'Conv1D (64,128,64) â†’ MaxPool â†’ LSTM (128, 64)',\n",
    "        'params': '~450K parameters',\n",
    "        'strength': 'Feature extraction, trend detection',\n",
    "        'ideal_for': 'Trend following, 1-hour to 4-hour'\n",
    "    }\n",
    "}\n",
    "\n",
    "for tf, details in arch_details.items():\n",
    "    print(f\"{tf.upper()}:\")\n",
    "    for key, value in details.items():\n",
    "        print(f\"  {key.title()}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72a509",
   "metadata": {},
   "source": [
    "## ğŸ‰ Summary\n",
    "\n",
    "### âœ… Ğ¥Ğ¸Ğ¹Ğ³Ğ´ÑÑĞ½:\n",
    "\n",
    "1. âœ… **3 Ó©Ó©Ñ€ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€** Ğ°ÑˆĞ¸Ğ³Ğ»Ğ°ÑĞ°Ğ½:\n",
    "\n",
    "   - 15-Ğ¼Ğ¸Ğ½: **Transformer + LSTM** (scalping)\n",
    "   - 30-Ğ¼Ğ¸Ğ½: **Bi-LSTM + Attention** (swing trading)\n",
    "   - 60-Ğ¼Ğ¸Ğ½: **CNN-LSTM Hybrid** (trend following)\n",
    "\n",
    "2. âœ… **Ğ‘Ò¯Ñ… Ğ²Ğ°Ğ»ÑÑ‚Ñ‹Ğ½ Ğ´Ğ°Ñ‚Ğ°Ğ³ Ğ½ÑĞ³Ñ‚Ğ³ÑÑÑĞ½** (EUR/USD, GBP/USD, USD/JPY, USD/CAD, USD/CHF, XAU/USD)\n",
    "\n",
    "3. âœ… **Pair encoding** Ğ½ÑĞ¼ÑÑĞ½ (6 Ğ²Ğ°Ğ»ÑÑ‚ â†’ one-hot features)\n",
    "\n",
    "4. âœ… **ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ±Ò¯Ñ€Ğ¸Ğ¹Ğ½ Ğ¾Ğ½Ñ†Ğ»Ğ¾Ğ³:**\n",
    "\n",
    "   - Transformer+LSTM: Multi-head attention + temporal dependencies\n",
    "   - Bi-LSTM+Attention: Bidirectional context + custom attention\n",
    "   - CNN-LSTM: Feature extraction + temporal modeling\n",
    "\n",
    "5. âœ… **Ğ“Ñ€Ğ°Ñ„Ğ¸ĞºÑƒÑƒĞ´ Ò¯Ò¯ÑĞ³ÑÑÑĞ½**:\n",
    "\n",
    "   - Training history (architecture comparison)\n",
    "   - Per-pair accuracy breakdown\n",
    "   - Confusion matrices per architecture\n",
    "\n",
    "6. âœ… **Metadata Ñ…Ğ°Ğ´Ğ³Ğ°Ğ»ÑĞ°Ğ½** (architecture info included)\n",
    "\n",
    "### ğŸ“ˆ Ò®Ñ€ Ğ´Ò¯Ğ½:\n",
    "\n",
    "ĞĞ´Ğ¾Ğ¾ Ñ‚Ğ°Ğ½Ğ´:\n",
    "\n",
    "- **3 Ó©Ó©Ñ€ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€** Ğ±Ò¯Ñ…Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ğ°Ğ¹Ğ½Ğ°\n",
    "- ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ò¯Ñ€ **Ó©Ó©Ñ€Ğ¸Ğ¹Ğ½ trading strategy-Ğ´** Ñ‚Ğ¾Ñ…Ğ¸Ñ€ÑĞ¾Ğ½\n",
    "- ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ò¯Ñ€ **Ğ±Ò¯Ñ… 6 Ğ²Ğ°Ğ»ÑÑ‚** Ğ´ÑÑÑ€ ÑÑƒÑ€Ğ³Ğ°Ğ³Ğ´ÑĞ°Ğ½\n",
    "- ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ±Ò¯Ñ€Ğ¸Ğ¹Ğ½ performance Ñ…Ğ°Ñ€ÑŒÑ†ÑƒÑƒĞ»Ğ°Ñ… Ğ±Ğ¾Ğ»Ğ¾Ğ¼Ğ¶Ñ‚Ğ¾Ğ¹\n",
    "\n",
    "### ğŸ”¬ ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹Ğ½ ÑĞ»Ğ³Ğ°Ğ°:\n",
    "\n",
    "| Timeframe | Architecture      | Best For        | Key Features                              |\n",
    "| --------- | ----------------- | --------------- | ----------------------------------------- |\n",
    "| 15-min    | Transformer+LSTM  | Scalping        | Multi-head attention, pattern recognition |\n",
    "| 30-min    | Bi-LSTM+Attention | Swing Trading   | Bidirectional context, time-step focus    |\n",
    "| 60-min    | CNN-LSTM          | Trend Following | Feature extraction, trend capture         |\n",
    "\n",
    "### ğŸ”œ Ğ”Ğ°Ñ€Ğ°Ğ°Ğ³Ğ¸Ğ¹Ğ½ Ğ°Ğ»Ñ…Ğ°Ğ¼:\n",
    "\n",
    "1. Meta-learner (XGBoost) Ò¯Ò¯ÑĞ³ÑÑ…\n",
    "2. Architecture ensemble\n",
    "3. Flask API-Ğ´ deploy Ñ…Ğ¸Ğ¹Ñ…\n",
    "4. Mobile app-Ñ‚ÑĞ¹ Ñ…Ğ¾Ğ»Ğ±Ğ¾Ñ…\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
