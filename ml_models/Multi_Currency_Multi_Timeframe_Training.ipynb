{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "558ed385",
   "metadata": {},
   "source": [
    "# 🚀 Multi-Currency Multi-Timeframe Deep Learning Training\n",
    "\n",
    "## Системийн тодорхойлолт\n",
    "\n",
    "Энэ notebook нь **3 өөр архитектураар** гурван модель сургана:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│  TRAINING DATA (ALL PAIRS COMBINED)             │\n",
    "│  - EUR/USD, GBP/USD, USD/JPY                    │\n",
    "│  - USD/CAD, USD/CHF, XAU/USD                    │\n",
    "│  Total: ~3-4M 1-minute candles                  │\n",
    "└─────────────────────────────────────────────────┘\n",
    "                    ↓\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│  3 PARALLEL MODELS WITH DIFFERENT ARCHITECTURES │\n",
    "│                                                 │\n",
    "│  ┌────────────────┬───────────────────────┐    │\n",
    "│  │ 15-min Model   │ Transformer + LSTM    │    │\n",
    "│  │ Expected: 88%  │ Focus: Quick scalping │    │\n",
    "│  └────────────────┴───────────────────────┘    │\n",
    "│                                                 │\n",
    "│  ┌────────────────┬───────────────────────┐    │\n",
    "│  │ 30-min Model   │ Bi-LSTM + Attention   │    │\n",
    "│  │ Expected: 85%  │ Focus: Swing trades   │    │\n",
    "│  └────────────────┴───────────────────────┘    │\n",
    "│                                                 │\n",
    "│  ┌────────────────┬───────────────────────┐    │\n",
    "│  │ 60-min Model   │ CNN-LSTM Hybrid       │    │\n",
    "│  │ Expected: 82%  │ Focus: Trend following│    │\n",
    "│  └────────────────┴───────────────────────┘    │\n",
    "└─────────────────────────────────────────────────┘\n",
    "                    ↓\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│  TEST ON ALL PAIRS                              │\n",
    "│  - Overall accuracy per model                   │\n",
    "│  - Per-pair performance breakdown               │\n",
    "│  - Architecture comparison                       │\n",
    "└─────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## Зорилго\n",
    "\n",
    "- 🎯 **15-минут (Transformer+LSTM):** 88%+ accuracy (scalping)\n",
    "- 🎯 **30-минут (Bi-LSTM+Attention):** 85%+ accuracy (swing trading)\n",
    "- 🎯 **60-минут (CNN-LSTM):** 82%+ accuracy (trend following)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa13087",
   "metadata": {},
   "source": [
    "## 📦 1. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87573b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add backend to path\n",
    "sys.path.append(str(Path.cwd().parent / 'backend'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks, optimizers\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Custom modules\n",
    "from ml.features.technical_indicators import calculate_all_features\n",
    "from ml.preprocessing.data_loader import ForexDataLoader\n",
    "from ml.preprocessing.sequence_generator import create_sequences\n",
    "\n",
    "# Import different model architectures\n",
    "from ml.models.transformer_lstm import build_transformer_lstm_model\n",
    "from ml.models.bilstm_attention import build_bilstm_attention_model\n",
    "from ml.models.cnn_lstm import build_cnn_lstm_model\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"✅ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"✅ GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"✅ Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd0722f",
   "metadata": {},
   "source": [
    "## ⚙️ 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9806fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = Path.cwd().parent / 'data'\n",
    "TRAIN_DIR = DATA_DIR / 'train'\n",
    "TEST_DIR = DATA_DIR / 'test'\n",
    "MODELS_DIR = Path.cwd().parent / 'models'\n",
    "LOGS_DIR = Path.cwd().parent / 'logs'\n",
    "\n",
    "# Create directories\n",
    "for timeframe in ['15min', '30min', '60min']:\n",
    "    (MODELS_DIR / timeframe).mkdir(parents=True, exist_ok=True)\n",
    "    (LOGS_DIR / timeframe / 'train').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Currency pairs\n",
    "CURRENCY_PAIRS = ['EUR_USD', 'GBP_USD', 'USD_JPY', 'USD_CAD', 'USD_CHF', 'XAU_USD']\n",
    "\n",
    "# Training configuration - DIFFERENT ARCHITECTURES\n",
    "CONFIG = {\n",
    "    '15min': {\n",
    "        'architecture': 'transformer_lstm',  # Scalping strategy\n",
    "        'resample_period': '15T',\n",
    "        'sequence_length': 60,\n",
    "        'prediction_steps': 1,\n",
    "        'n_heads': 8,\n",
    "        'ff_dim': 256,\n",
    "        'lstm_units': [128, 64],\n",
    "        'dropout': 0.3,\n",
    "        'batch_size': 128,  # Increased for memory efficiency\n",
    "        'epochs': 30,  # Reduced for faster training\n",
    "        'learning_rate': 0.001,\n",
    "        'description': 'Transformer + LSTM for quick scalping',\n",
    "        'sample_ratio': 1\n",
    "    },\n",
    "    '30min': {\n",
    "        'architecture': 'bilstm_attention',  # Swing trading\n",
    "        'resample_period': '30T',\n",
    "        'sequence_length': 48,\n",
    "        'prediction_steps': 1,\n",
    "        'lstm_units': [128, 64],\n",
    "        'attention_units': 128,\n",
    "        'dropout': 0.3,\n",
    "        'batch_size': 128,  # Increased for memory efficiency\n",
    "        'epochs': 30,  # Reduced for faster training\n",
    "        'learning_rate': 0.001,\n",
    "        'description': 'Bi-LSTM + Attention for swing trades',\n",
    "        'sample_ratio': 1\n",
    "    },\n",
    "    '60min': {\n",
    "        'architecture': 'cnn_lstm',  # Trend following\n",
    "        'resample_period': '60T',\n",
    "        'sequence_length': 48,\n",
    "        'prediction_steps': 1,\n",
    "        'cnn_filters': [64, 128, 64],\n",
    "        'kernel_size': 3,\n",
    "        'lstm_units': [128, 64],\n",
    "        'dropout': 0.3,\n",
    "        'batch_size': 128,  # Increased for memory efficiency\n",
    "        'epochs': 30,  # Reduced for faster training\n",
    "        'learning_rate': 0.001,\n",
    "        'description': 'CNN-LSTM for trend following',\n",
    "        'sample_ratio': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ Configuration loaded\")\n",
    "print(f\"📁 Data directory: {DATA_DIR}\")\n",
    "print(f\"💾 Models directory: {MODELS_DIR}\")\n",
    "print(f\"📊 Currency pairs: {', '.join(CURRENCY_PAIRS)}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🏗️  ARCHITECTURE CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "for tf, cfg in CONFIG.items():\n",
    "    print(f\"\\n{tf}:\")\n",
    "    print(f\"  Architecture: {cfg['architecture']}\")\n",
    "    print(f\"  Strategy: {cfg['description']}\")\n",
    "    print(f\"  Sample ratio: {cfg['sample_ratio']*100:.0f}%\")\n",
    "    print(f\"  Batch size: {cfg['batch_size']}\")\n",
    "    print(f\"  Epochs: {cfg['epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916327a",
   "metadata": {},
   "source": [
    "## 📥 3. Load ALL Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_training_data():\n",
    "    \"\"\"\n",
    "    Load and combine ALL currency pairs from data/train/\n",
    "    \n",
    "    Returns:\n",
    "        combined_df: DataFrame with all pairs combined\n",
    "        pair_stats: Statistics per pair\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📥 LOADING ALL TRAINING DATA\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    loader = ForexDataLoader(data_dir=DATA_DIR)\n",
    "    all_dfs = []\n",
    "    pair_stats = []\n",
    "    \n",
    "    for pair in CURRENCY_PAIRS:\n",
    "        print(f\"Loading {pair}...\")\n",
    "        df = loader.load_train_data(pair=pair)\n",
    "        \n",
    "        if df is not None:\n",
    "            # Add pair identifier\n",
    "            df['pair'] = pair\n",
    "            all_dfs.append(df)\n",
    "            \n",
    "            pair_stats.append({\n",
    "                'pair': pair,\n",
    "                'rows': len(df),\n",
    "                'start': df.index.min(),\n",
    "                'end': df.index.max()\n",
    "            })\n",
    "            print(f\"  ✅ {len(df):,} rows\")\n",
    "        else:\n",
    "            print(f\"  ❌ Failed to load\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(all_dfs, axis=0)\n",
    "    combined_df = combined_df.sort_index()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"✅ COMBINED TRAINING DATA\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total rows: {len(combined_df):,}\")\n",
    "    print(f\"Date range: {combined_df.index.min()} to {combined_df.index.max()}\")\n",
    "    print(f\"Columns: {combined_df.columns.tolist()}\")\n",
    "    \n",
    "    # Statistics table\n",
    "    stats_df = pd.DataFrame(pair_stats)\n",
    "    print(f\"\\n📊 Per-Pair Statistics:\")\n",
    "    print(stats_df.to_string(index=False))\n",
    "    \n",
    "    return combined_df, stats_df\n",
    "\n",
    "# Load all training data\n",
    "df_train_all, train_stats = load_all_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429e507",
   "metadata": {},
   "source": [
    "## 📥 4. Load ALL Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b8c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_test_data():\n",
    "    \"\"\"\n",
    "    Load and combine ALL currency pairs from data/test/\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📥 LOADING ALL TEST DATA\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    loader = ForexDataLoader(data_dir=DATA_DIR)\n",
    "    all_dfs = []\n",
    "    pair_stats = []\n",
    "    \n",
    "    for pair in CURRENCY_PAIRS:\n",
    "        print(f\"Loading {pair} test data...\")\n",
    "        df = loader.load_test_data(pair=pair)\n",
    "        \n",
    "        if df is not None:\n",
    "            df['pair'] = pair\n",
    "            all_dfs.append(df)\n",
    "            \n",
    "            pair_stats.append({\n",
    "                'pair': pair,\n",
    "                'rows': len(df)\n",
    "            })\n",
    "            print(f\"  ✅ {len(df):,} rows\")\n",
    "        else:\n",
    "            print(f\"  ❌ Failed to load\")\n",
    "    \n",
    "    combined_df = pd.concat(all_dfs, axis=0)\n",
    "    combined_df = combined_df.sort_index()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"✅ COMBINED TEST DATA\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total rows: {len(combined_df):,}\")\n",
    "    \n",
    "    stats_df = pd.DataFrame(pair_stats)\n",
    "    print(f\"\\n📊 Per-Pair Statistics:\")\n",
    "    print(stats_df.to_string(index=False))\n",
    "    \n",
    "    return combined_df, stats_df\n",
    "\n",
    "# Load all test data\n",
    "df_test_all, test_stats = load_all_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bef60b",
   "metadata": {},
   "source": [
    "## 🔧 5. Feature Engineering for Multi-Currency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f8e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_multi_currency_data(df, timeframe_config, fit_scaler=None, fit_encoder=None):\n",
    "    \"\"\"\n",
    "    Prepare data for specific timeframe with multi-currency support\n",
    "    \n",
    "    Key difference: Adds pair encoding as feature\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔄 Processing multi-currency data for {timeframe_config['resample_period']}...\")\n",
    "    \n",
    "    all_pairs_data = []\n",
    "    \n",
    "    # Process each pair separately then combine\n",
    "    for pair in CURRENCY_PAIRS:\n",
    "        print(f\"  Processing {pair}...\")\n",
    "        \n",
    "        # Filter data for this pair\n",
    "        df_pair = df[df['pair'] == pair].copy()\n",
    "        df_pair = df_pair.drop('pair', axis=1)\n",
    "        \n",
    "        if len(df_pair) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Resample\n",
    "        df_resampled = df_pair.resample(timeframe_config['resample_period']).agg({\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'close': 'last',\n",
    "            'tick_volume': 'sum'\n",
    "        }).dropna()\n",
    "        \n",
    "        # Calculate features\n",
    "        df_features = calculate_all_features(df_resampled)\n",
    "        \n",
    "        # Create labels\n",
    "        prediction_steps = timeframe_config['prediction_steps']\n",
    "        df_features['future_return'] = df_features['close'].shift(-prediction_steps) / df_features['close'] - 1\n",
    "        \n",
    "        threshold = 0.0005\n",
    "        conditions = [\n",
    "            df_features['future_return'] < -threshold,\n",
    "            (df_features['future_return'] >= -threshold) & (df_features['future_return'] <= threshold),\n",
    "            df_features['future_return'] > threshold\n",
    "        ]\n",
    "        df_features['label'] = np.select(conditions, [0, 1, 2], default=1)\n",
    "        \n",
    "        # Add pair identifier back\n",
    "        df_features['pair'] = pair\n",
    "        \n",
    "        df_features = df_features.dropna()\n",
    "        all_pairs_data.append(df_features)\n",
    "        print(f\"    ✅ {len(df_features):,} rows\")\n",
    "    \n",
    "    # Combine all pairs\n",
    "    df_combined = pd.concat(all_pairs_data, axis=0)\n",
    "    df_combined = df_combined.sort_index()\n",
    "    \n",
    "    print(f\"\\n✅ Combined: {len(df_combined):,} rows\")\n",
    "    print(f\"\\n📊 Label distribution:\")\n",
    "    print(df_combined['label'].value_counts())\n",
    "    print(f\"\\nClass percentages:\")\n",
    "    print(df_combined['label'].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    # Encode pair as one-hot\n",
    "    if fit_encoder is None:\n",
    "        pair_encoder = LabelEncoder()\n",
    "        pair_encoded = pair_encoder.fit_transform(df_combined['pair'])\n",
    "    else:\n",
    "        pair_encoder = fit_encoder\n",
    "        pair_encoded = pair_encoder.transform(df_combined['pair'])\n",
    "    \n",
    "    # One-hot encode pair (6 columns)\n",
    "    pair_onehot = pd.get_dummies(pair_encoded, prefix='pair')\n",
    "    \n",
    "    # Separate features and labels\n",
    "    feature_cols = [col for col in df_combined.columns \n",
    "                   if col not in ['label', 'future_return', 'open', 'high', 'low', 'close', 'pair']]\n",
    "    \n",
    "    X = df_combined[feature_cols].values\n",
    "    y = df_combined['label'].values\n",
    "    \n",
    "    # Add pair encoding to features\n",
    "    X = np.concatenate([X, pair_onehot.values], axis=1)\n",
    "    feature_cols_with_pair = feature_cols + [f'pair_{i}' for i in range(len(CURRENCY_PAIRS))]\n",
    "    \n",
    "    # Normalize\n",
    "    print(\"\\n🔄 Normalizing features...\")\n",
    "    if fit_scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "    else:\n",
    "        scaler = fit_scaler\n",
    "        X_scaled = scaler.transform(X)\n",
    "    \n",
    "    print(f\"✅ Final shape: X={X_scaled.shape}, y={y.shape}\")\n",
    "    print(f\"✅ Total features: {len(feature_cols_with_pair)}\")\n",
    "    \n",
    "    return X_scaled, y, scaler, pair_encoder, feature_cols_with_pair, df_combined['pair'].values\n",
    "\n",
    "print(\"✅ Multi-currency feature engineering function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b434ccb6",
   "metadata": {},
   "source": [
    "## 🏋️ 6. Train Models on ALL Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47842cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multi_currency_model(timeframe):\n",
    "    \"\"\"\n",
    "    Train model on ALL currency pairs combined\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"🚀 TRAINING {timeframe.upper()} MODEL ON ALL PAIRS\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    config = CONFIG[timeframe]\n",
    "    sample_ratio = config.get('sample_ratio', 1.0)\n",
    "    \n",
    "    # Prepare training data\n",
    "    print(f\"🔧 Preparing training data (all pairs, {sample_ratio*100:.0f}% sample)...\")\n",
    "    X_train, y_train, scaler, pair_encoder, feature_cols, train_pairs = prepare_multi_currency_data(\n",
    "        df_train_all, config\n",
    "    )\n",
    "    \n",
    "    # Sample data if needed\n",
    "    if sample_ratio < 1.0:\n",
    "        n_samples = int(len(X_train) * sample_ratio)\n",
    "        indices = np.random.choice(len(X_train), n_samples, replace=False)\n",
    "        indices.sort()  # Keep temporal order\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "        train_pairs = train_pairs[indices]\n",
    "        print(f\"📉 Sampled to {len(X_train):,} rows ({sample_ratio*100:.0f}%)\")\n",
    "    \n",
    "    # Prepare test data\n",
    "    print(f\"\\n🔧 Preparing test data (all pairs)...\")\n",
    "    X_test, y_test, _, _, _, test_pairs = prepare_multi_currency_data(\n",
    "        df_test_all, config, fit_scaler=scaler, fit_encoder=pair_encoder\n",
    "    )\n",
    "    \n",
    "    # Create sequences\n",
    "    print(f\"\\n🔄 Creating sequences (length={config['sequence_length']})...\")\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train, y_train, config['sequence_length'])\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test, y_test, config['sequence_length'])\n",
    "    \n",
    "    # Keep track of pairs for test sequences\n",
    "    test_pairs_seq = test_pairs[config['sequence_length']:]\n",
    "    \n",
    "    print(f\"✅ Training sequences: {X_train_seq.shape}\")\n",
    "    print(f\"✅ Test sequences: {X_test_seq.shape}\")\n",
    "    \n",
    "    # Convert to float32 to save memory\n",
    "    X_train_seq = X_train_seq.astype(np.float32)\n",
    "    X_test_seq = X_test_seq.astype(np.float32)\n",
    "    y_train_seq = y_train_seq.astype(np.int32)\n",
    "    y_test_seq = y_test_seq.astype(np.int32)\n",
    "    print(f\"✅ Converted to float32/int32 for memory efficiency\")\n",
    "    \n",
    "    # Split validation\n",
    "    X_train_seq, X_val_seq, y_train_seq, y_val_seq = train_test_split(\n",
    "        X_train_seq, y_train_seq, test_size=0.2, random_state=42, stratify=y_train_seq\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📊 Final splits:\")\n",
    "    print(f\"   Train: {X_train_seq.shape}\")\n",
    "    print(f\"   Val:   {X_val_seq.shape}\")\n",
    "    print(f\"   Test:  {X_test_seq.shape}\")\n",
    "    \n",
    "    # Build model - DIFFERENT ARCHITECTURE PER TIMEFRAME\n",
    "    print(f\"\\n🏗️  Building {config['architecture']} model...\")\n",
    "    print(f\"📝 Strategy: {config['description']}\")\n",
    "    n_features = X_train_seq.shape[2]\n",
    "    \n",
    "    if timeframe == '15min':\n",
    "        # Transformer + LSTM for scalping\n",
    "        model = build_transformer_lstm_model(\n",
    "            sequence_length=config['sequence_length'],\n",
    "            n_features=n_features,\n",
    "            n_heads=config['n_heads'],\n",
    "            ff_dim=config['ff_dim'],\n",
    "            lstm_units=config['lstm_units'],\n",
    "            dropout_rate=config['dropout'],\n",
    "            n_classes=3\n",
    "        )\n",
    "    elif timeframe == '30min':\n",
    "        # Bi-LSTM + Attention for swing trading\n",
    "        model = build_bilstm_attention_model(\n",
    "            sequence_length=config['sequence_length'],\n",
    "            n_features=n_features,\n",
    "            lstm_units=config['lstm_units'],\n",
    "            attention_units=config['attention_units'],\n",
    "            dropout_rate=config['dropout'],\n",
    "            n_classes=3\n",
    "        )\n",
    "    else:  # 60min\n",
    "        # CNN-LSTM for trend following\n",
    "        model = build_cnn_lstm_model(\n",
    "            sequence_length=config['sequence_length'],\n",
    "            n_features=n_features,\n",
    "            cnn_filters=config['cnn_filters'],\n",
    "            kernel_size=config['kernel_size'],\n",
    "            lstm_units=config['lstm_units'],\n",
    "            dropout_rate=config['dropout'],\n",
    "            n_classes=3\n",
    "        )\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=config['learning_rate']),\n",
    "        loss={\n",
    "            'direction': 'sparse_categorical_crossentropy',\n",
    "            'confidence': 'mse'\n",
    "        },\n",
    "        loss_weights={'direction': 1.0, 'confidence': 0.5},\n",
    "        metrics={\n",
    "            'direction': ['accuracy'],\n",
    "            'confidence': ['mae']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    # Callbacks\n",
    "    model_path = MODELS_DIR / timeframe / f\"multi_currency_{timeframe}_best.keras\"\n",
    "    log_dir = LOGS_DIR / timeframe / 'train' / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    callbacks_list = [\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath=str(model_path),\n",
    "            monitor='val_direction_accuracy',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='val_direction_accuracy',\n",
    "            patience=7,  # Reduced patience\n",
    "            mode='max',\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,  # Reduced patience\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        callbacks.TensorBoard(\n",
    "            log_dir=str(log_dir),\n",
    "            histogram_freq=0  # Disabled for speed\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\n🏋️  Training model on ALL currency pairs...\")\n",
    "    print(f\"📊 Batch size: {config['batch_size']}\")\n",
    "    print(f\"📊 Epochs: {config['epochs']}\")\n",
    "    print(f\"⚡ Optimized for memory and speed\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_seq,\n",
    "        {\n",
    "            'direction': y_train_seq,\n",
    "            'confidence': np.ones_like(y_train_seq, dtype=np.float32)\n",
    "        },\n",
    "        validation_data=(\n",
    "            X_val_seq,\n",
    "            {\n",
    "                'direction': y_val_seq,\n",
    "                'confidence': np.ones_like(y_val_seq, dtype=np.float32)\n",
    "            }\n",
    "        ),\n",
    "        batch_size=config['batch_size'],\n",
    "        epochs=config['epochs'],\n",
    "        callbacks=callbacks_list,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Test\n",
    "    print(f\"\\n📊 Testing on all pairs...\")\n",
    "    predictions = model.predict(X_test_seq, batch_size=config['batch_size'])\n",
    "    y_pred = np.argmax(predictions[0], axis=1)\n",
    "    y_confidence = predictions[1].flatten()\n",
    "    \n",
    "    # Overall accuracy\n",
    "    accuracy = accuracy_score(y_test_seq, y_pred)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"📈 OVERALL TEST RESULTS FOR {timeframe.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n🎯 Overall Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(\"\\n📊 Classification Report:\")\n",
    "    print(classification_report(y_test_seq, y_pred, target_names=['SELL', 'NEUTRAL', 'BUY']))\n",
    "    \n",
    "    # Per-pair accuracy\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 PER-PAIR ACCURACY BREAKDOWN\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    pair_results = []\n",
    "    for pair in CURRENCY_PAIRS:\n",
    "        mask = test_pairs_seq == pair\n",
    "        if mask.sum() > 0:\n",
    "            pair_acc = accuracy_score(y_test_seq[mask], y_pred[mask])\n",
    "            pair_results.append({\n",
    "                'Pair': pair,\n",
    "                'Samples': mask.sum(),\n",
    "                'Accuracy': f\"{pair_acc*100:.2f}%\"\n",
    "            })\n",
    "            print(f\"{pair:8s}: {pair_acc*100:.2f}% ({mask.sum():,} samples)\")\n",
    "    \n",
    "    # Save artifacts\n",
    "    scaler_path = MODELS_DIR / timeframe / f\"multi_currency_{timeframe}_scaler.pkl\"\n",
    "    encoder_path = MODELS_DIR / timeframe / f\"multi_currency_{timeframe}_encoder.pkl\"\n",
    "    \n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    joblib.dump(pair_encoder, encoder_path)\n",
    "    \n",
    "    metadata = {\n",
    "        'timeframe': timeframe,\n",
    "        'architecture': config['architecture'],\n",
    "        'strategy': config['description'],\n",
    "        'training_mode': 'multi_currency',\n",
    "        'sample_ratio': sample_ratio,\n",
    "        'pairs': CURRENCY_PAIRS,\n",
    "        'n_features': n_features,\n",
    "        'sequence_length': config['sequence_length'],\n",
    "        'feature_columns': feature_cols,\n",
    "        'train_samples': len(X_train_seq),\n",
    "        'test_samples': len(X_test_seq),\n",
    "        'overall_accuracy': float(accuracy),\n",
    "        'per_pair_results': pair_results,\n",
    "        'training_date': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    metadata_path = MODELS_DIR / timeframe / f\"multi_currency_{timeframe}_metadata.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n💾 Model saved: {model_path}\")\n",
    "    print(f\"💾 Scaler saved: {scaler_path}\")\n",
    "    print(f\"💾 Encoder saved: {encoder_path}\")\n",
    "    print(f\"💾 Metadata saved: {metadata_path}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred,\n",
    "        'confidence': y_confidence,\n",
    "        'true_labels': y_test_seq,\n",
    "        'test_pairs': test_pairs_seq,\n",
    "        'pair_results': pair_results\n",
    "    }\n",
    "\n",
    "print(\"✅ Multi-currency training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df449102",
   "metadata": {},
   "source": [
    "## 🚀 7. Train 15-Minute Model (Transformer + LSTM)\n",
    "\n",
    "**Strategy:** Quick scalping  \n",
    "**Architecture:** Multi-head attention + LSTM layers  \n",
    "**Target Accuracy:** 88%+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10266aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 15-minute model\n",
    "print(\"=\"*80)\n",
    "print(\"🎯 TRAINING 15-MINUTE MODEL (TRANSFORMER + LSTM)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    result_15min = train_multi_currency_model('15min')\n",
    "    print(f\"\\n✅ 15-minute model training completed!\")\n",
    "    print(f\"🎯 Accuracy: {result_15min['accuracy']*100:.2f}%\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error training 15-minute model: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    result_15min = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16694593",
   "metadata": {},
   "source": [
    "## 🚀 8. Train 30-Minute Model (Bi-LSTM + Attention)\n",
    "\n",
    "**Strategy:** Swing trading  \n",
    "**Architecture:** Bidirectional LSTM + Custom attention mechanism  \n",
    "**Target Accuracy:** 85%+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 30-minute model\n",
    "print(\"=\"*80)\n",
    "print(\"🎯 TRAINING 30-MINUTE MODEL (BI-LSTM + ATTENTION)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    result_30min = train_multi_currency_model('30min')\n",
    "    print(f\"\\n✅ 30-minute model training completed!\")\n",
    "    print(f\"🎯 Accuracy: {result_30min['accuracy']*100:.2f}%\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error training 30-minute model: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    result_30min = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f4f1d3",
   "metadata": {},
   "source": [
    "## 🚀 9. Train 60-Minute Model (CNN-LSTM Hybrid)\n",
    "\n",
    "**Strategy:** Trend following  \n",
    "**Architecture:** Convolutional layers + LSTM for temporal modeling  \n",
    "**Target Accuracy:** 82%+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8271bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 60-minute model\n",
    "print(\"=\"*80)\n",
    "print(\"🎯 TRAINING 60-MINUTE MODEL (CNN-LSTM HYBRID)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    result_60min = train_multi_currency_model('60min')\n",
    "    print(f\"\\n✅ 60-minute model training completed!\")\n",
    "    print(f\"🎯 Accuracy: {result_60min['accuracy']*100:.2f}%\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error training 60-minute model: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    result_60min = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05953e9e",
   "metadata": {},
   "source": [
    "## 📊 10. Collect All Results\n",
    "\n",
    "Combine results from all 3 models for visualization and comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd5733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "results = {}\n",
    "\n",
    "if 'result_15min' in locals() and result_15min is not None:\n",
    "    results['15min'] = result_15min\n",
    "    print(f\"✅ 15-minute model: {result_15min['accuracy']*100:.2f}%\")\n",
    "else:\n",
    "    print(\"❌ 15-minute model not trained\")\n",
    "\n",
    "if 'result_30min' in locals() and result_30min is not None:\n",
    "    results['30min'] = result_30min\n",
    "    print(f\"✅ 30-minute model: {result_30min['accuracy']*100:.2f}%\")\n",
    "else:\n",
    "    print(\"❌ 30-minute model not trained\")\n",
    "\n",
    "if 'result_60min' in locals() and result_60min is not None:\n",
    "    results['60min'] = result_60min\n",
    "    print(f\"✅ 60-minute model: {result_60min['accuracy']*100:.2f}%\")\n",
    "else:\n",
    "    print(\"❌ 60-minute model not trained\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"🎉 TRAINING SUMMARY: {len(results)}/3 models completed\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5520a4f3",
   "metadata": {},
   "source": [
    "## 📊 11. Visualize Results\n",
    "\n",
    "Generate comparison plots and charts for all trained models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e7c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Training History - 3 Different Architectures\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Training History - 3 Different Architectures', fontsize=16, fontweight='bold')\n",
    "\n",
    "architectures = {\n",
    "    '15min': 'Transformer+LSTM',\n",
    "    '30min': 'Bi-LSTM+Attention',\n",
    "    '60min': 'CNN-LSTM'\n",
    "}\n",
    "\n",
    "for idx, (timeframe, result) in enumerate(results.items()):\n",
    "    if 'history' not in result:\n",
    "        continue\n",
    "    \n",
    "    history = result['history']\n",
    "    arch_name = architectures[timeframe]\n",
    "    \n",
    "    # Accuracy\n",
    "    ax1 = axes[0, idx]\n",
    "    ax1.plot(history.history['direction_accuracy'], label='Train')\n",
    "    ax1.plot(history.history['val_direction_accuracy'], label='Validation')\n",
    "    ax1.set_title(f'{timeframe} ({arch_name})\\nAccuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss\n",
    "    ax2 = axes[1, idx]\n",
    "    ax2.plot(history.history['loss'], label='Train')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation')\n",
    "    ax2.set_title(f'{timeframe} ({arch_name})\\nLoss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODELS_DIR / 'training_history_multi_currency.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Training history plots saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Per-Pair Accuracy Comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Per-Pair Accuracy Across Timeframes', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (timeframe, result) in enumerate(results.items()):\n",
    "    if 'pair_results' not in result:\n",
    "        continue\n",
    "    \n",
    "    pair_df = pd.DataFrame(result['pair_results'])\n",
    "    pair_df['Accuracy_Val'] = pair_df['Accuracy'].str.rstrip('%').astype(float)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    bars = ax.bar(range(len(pair_df)), pair_df['Accuracy_Val'], color='skyblue', edgecolor='black')\n",
    "    ax.set_xticks(range(len(pair_df)))\n",
    "    ax.set_xticklabels(pair_df['Pair'], rotation=45, ha='right')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    arch_name = architectures[timeframe]\n",
    "    ax.set_title(f'{timeframe} ({arch_name})')\n",
    "    ax.set_ylim([0, 100])\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODELS_DIR / 'per_pair_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Per-pair accuracy plots saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5fb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Confusion Matrices - All Timeframes', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (timeframe, result) in enumerate(results.items()):\n",
    "    if 'predictions' not in result:\n",
    "        continue\n",
    "    \n",
    "    cm = confusion_matrix(result['true_labels'], result['predictions'])\n",
    "    arch_name = architectures[timeframe]\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=['SELL', 'NEUTRAL', 'BUY'],\n",
    "               yticklabels=['SELL', 'NEUTRAL', 'BUY'],\n",
    "               ax=ax)\n",
    "    ax.set_title(f'{timeframe} ({arch_name})\\nAcc: {result[\"accuracy\"]*100:.2f}%')\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODELS_DIR / 'confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Confusion matrices saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d547d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Architecture Comparison (Overall Accuracy)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "timeframes_list = []\n",
    "accuracies_list = []\n",
    "arch_labels = []\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "for timeframe, result in results.items():\n",
    "    if result is None:\n",
    "        continue\n",
    "    timeframes_list.append(timeframe)\n",
    "    accuracies_list.append(result['accuracy'] * 100)\n",
    "    arch_labels.append(architectures[timeframe])\n",
    "\n",
    "bars = ax.bar(range(len(timeframes_list)), accuracies_list, color=colors, edgecolor='black', linewidth=2)\n",
    "ax.set_xticks(range(len(timeframes_list)))\n",
    "ax.set_xticklabels([f\"{tf}\\n{arch}\" for tf, arch in zip(timeframes_list, arch_labels)], fontsize=11)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Architecture Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim([0, 100])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "            f'{height:.2f}%',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add target lines\n",
    "targets = {'15min': 88, '30min': 85, '60min': 82}\n",
    "for i, tf in enumerate(timeframes_list):\n",
    "    if tf in targets:\n",
    "        ax.axhline(y=targets[tf], xmin=(i-0.4)/len(timeframes_list), xmax=(i+0.4)/len(timeframes_list),\n",
    "                   color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "        ax.text(i, targets[tf] + 2, f'Target: {targets[tf]}%', \n",
    "                ha='center', fontsize=9, color='red', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODELS_DIR / 'architecture_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Architecture comparison plot saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e26fa",
   "metadata": {},
   "source": [
    "## 📊 12. Summary Table\n",
    "\n",
    "Final comprehensive summary of all trained models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ba781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "architectures = {\n",
    "    '15min': 'Transformer+LSTM',\n",
    "    '30min': 'Bi-LSTM+Attention',\n",
    "    '60min': 'CNN-LSTM'\n",
    "}\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for timeframe, result in results.items():\n",
    "    if result is None:\n",
    "        continue\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Timeframe': timeframe,\n",
    "        'Architecture': architectures[timeframe],\n",
    "        'Overall Accuracy': f\"{result['accuracy']*100:.2f}%\",\n",
    "        'Test Samples': len(result['predictions']),\n",
    "        'High Conf (>85%)': f\"{(result['confidence'] > 0.85).sum()}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 FINAL SUMMARY - MULTI-CURRENCY MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\" + summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv(MODELS_DIR / 'multi_currency_summary.csv', index=False)\n",
    "print(f\"\\n💾 Summary saved: {MODELS_DIR / 'multi_currency_summary.csv'}\")\n",
    "\n",
    "# Display per-pair results for each timeframe\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 PER-PAIR ACCURACY BREAKDOWN\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for timeframe, result in results.items():\n",
    "    if 'pair_results' in result:\n",
    "        print(f\"\\n{timeframe.upper()} ({architectures[timeframe]}):\")\n",
    "        pair_df = pd.DataFrame(result['pair_results'])\n",
    "        print(pair_df.to_string(index=False))\n",
    "\n",
    "# Architecture details\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🏗️  ARCHITECTURE DETAILS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "arch_details = {\n",
    "    '15min': {\n",
    "        'name': 'Transformer + LSTM',\n",
    "        'layers': 'Multi-Head Attention (8 heads) → LSTM (128, 64)',\n",
    "        'params': '~500K parameters',\n",
    "        'strength': 'Pattern recognition, short-term predictions',\n",
    "        'ideal_for': 'Scalping, 15-min trades'\n",
    "    },\n",
    "    '30min': {\n",
    "        'name': 'Bi-LSTM + Attention',\n",
    "        'layers': 'Bi-LSTM (128, 64) → Custom Attention (128)',\n",
    "        'params': '~400K parameters',\n",
    "        'strength': 'Bidirectional context, focus mechanism',\n",
    "        'ideal_for': 'Swing trading, 30-min to 1-hour'\n",
    "    },\n",
    "    '60min': {\n",
    "        'name': 'CNN-LSTM Hybrid',\n",
    "        'layers': 'Conv1D (64,128,64) → MaxPool → LSTM (128, 64)',\n",
    "        'params': '~450K parameters',\n",
    "        'strength': 'Feature extraction, trend detection',\n",
    "        'ideal_for': 'Trend following, 1-hour to 4-hour'\n",
    "    }\n",
    "}\n",
    "\n",
    "for tf, details in arch_details.items():\n",
    "    print(f\"{tf.upper()}:\")\n",
    "    for key, value in details.items():\n",
    "        print(f\"  {key.title()}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72a509",
   "metadata": {},
   "source": [
    "## 🎉 Summary\n",
    "\n",
    "### ✅ Хийгдсэн:\n",
    "\n",
    "1. ✅ **3 өөр архитектур** ашигласан:\n",
    "\n",
    "   - 15-мин: **Transformer + LSTM** (scalping)\n",
    "   - 30-мин: **Bi-LSTM + Attention** (swing trading)\n",
    "   - 60-мин: **CNN-LSTM Hybrid** (trend following)\n",
    "\n",
    "2. ✅ **Бүх валютын датаг нэгтгэсэн** (EUR/USD, GBP/USD, USD/JPY, USD/CAD, USD/CHF, XAU/USD)\n",
    "\n",
    "3. ✅ **Pair encoding** нэмсэн (6 валют → one-hot features)\n",
    "\n",
    "4. ✅ **Архитектур бүрийн онцлог:**\n",
    "\n",
    "   - Transformer+LSTM: Multi-head attention + temporal dependencies\n",
    "   - Bi-LSTM+Attention: Bidirectional context + custom attention\n",
    "   - CNN-LSTM: Feature extraction + temporal modeling\n",
    "\n",
    "5. ✅ **Графикууд үүсгэсэн**:\n",
    "\n",
    "   - Training history (architecture comparison)\n",
    "   - Per-pair accuracy breakdown\n",
    "   - Confusion matrices per architecture\n",
    "\n",
    "6. ✅ **Metadata хадгалсан** (architecture info included)\n",
    "\n",
    "### 📈 Үр дүн:\n",
    "\n",
    "Одоо танд:\n",
    "\n",
    "- **3 өөр архитектур** бүхий модель байна\n",
    "- Модель бүр **өөрийн trading strategy-д** тохирсон\n",
    "- Модель бүр **бүх 6 валют** дээр сургагдсан\n",
    "- Архитектур бүрийн performance харьцуулах боломжтой\n",
    "\n",
    "### 🔬 Архитектурын ялгаа:\n",
    "\n",
    "| Timeframe | Architecture      | Best For        | Key Features                              |\n",
    "| --------- | ----------------- | --------------- | ----------------------------------------- |\n",
    "| 15-min    | Transformer+LSTM  | Scalping        | Multi-head attention, pattern recognition |\n",
    "| 30-min    | Bi-LSTM+Attention | Swing Trading   | Bidirectional context, time-step focus    |\n",
    "| 60-min    | CNN-LSTM          | Trend Following | Feature extraction, trend capture         |\n",
    "\n",
    "### 🔜 Дараагийн алхам:\n",
    "\n",
    "1. Meta-learner (XGBoost) үүсгэх\n",
    "2. Architecture ensemble\n",
    "3. Flask API-д deploy хийх\n",
    "4. Mobile app-тэй холбох\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
