{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4c0c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸš€ FOREX SIGNAL GENERATOR V9\n",
      "   Anti-Overfit + High Performance\n",
      "======================================================================\n",
      "âœ“ GPU Available: True\n",
      "âœ“ Model Directory: c:\\Users\\Acer\\Desktop\\Forex-Signal-App\\models\\signal_generator_v9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit, cross_val_predict\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "MODEL_DIR = BASE_DIR / 'models' / 'signal_generator_v9'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# GPU Check\n",
    "import torch\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸš€ FOREX SIGNAL GENERATOR V9\")\n",
    "print(\"   Anti-Overfit + High Performance\")\n",
    "print(\"=\"*70)\n",
    "print(f\"âœ“ GPU Available: {GPU_AVAILABLE}\")\n",
    "print(f\"âœ“ Model Directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b8b696",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49127e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,859,492 rows\n",
      "Test: 296,778 rows\n",
      "Train period: 2019-12-31 16:00:00+00:00 to 2024-12-30 16:00:00+00:00\n",
      "Test period: 2024-12-31 16:00:00+00:00 to 2025-10-17 06:11:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "train_df = pd.read_csv(DATA_DIR / 'EUR_USD_1min.csv')\n",
    "test_df = pd.read_csv(DATA_DIR / 'EUR_USD_test.csv')\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    if 'timestamp' in df.columns:\n",
    "        df.rename(columns={'timestamp': 'time'}, inplace=True)\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows\")\n",
    "print(f\"Test: {len(test_df):,} rows\")\n",
    "print(f\"Train period: {train_df['time'].min()} to {train_df['time'].max()}\")\n",
    "print(f\"Test period: {test_df['time'].min()} to {test_df['time'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c7c785",
   "metadata": {},
   "source": [
    "## 2. V9 Feature Engineering (Refined from V8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b7d4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding V9 features...\n",
      "âœ“ Features added. Total columns: 59\n"
     ]
    }
   ],
   "source": [
    "def add_features_v9(df):\n",
    "    \"\"\"\n",
    "    V9 Features: V8-Ð¸Ð¹Ð½ ÑÐ°Ð¹Ð½ features + Ð¨Ð¸Ð½Ñ robust features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ==================== CORE FEATURES ====================\n",
    "    # Time Features\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['day_of_week'] = df['time'].dt.dayofweek\n",
    "    df['is_london'] = ((df['hour'] >= 8) & (df['hour'] < 16)).astype(int)\n",
    "    df['is_ny'] = ((df['hour'] >= 13) & (df['hour'] < 21)).astype(int)\n",
    "    df['is_overlap'] = ((df['hour'] >= 13) & (df['hour'] < 16)).astype(int)\n",
    "    \n",
    "    # Moving Averages\n",
    "    for p in [5, 10, 20, 50, 200]:\n",
    "        df[f'sma_{p}'] = df['close'].rolling(p).mean()\n",
    "        df[f'ema_{p}'] = df['close'].ewm(span=p, adjust=False).mean()\n",
    "    \n",
    "    # RSI (Multi-period for robustness)\n",
    "    for period in [7, 14, 21]:\n",
    "        delta = df['close'].diff()\n",
    "        gain = delta.where(delta > 0, 0).rolling(period).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(period).mean()\n",
    "        rs = gain / (loss + 1e-10)\n",
    "        df[f'rsi_{period}'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    ema12 = df['close'].ewm(span=12).mean()\n",
    "    ema26 = df['close'].ewm(span=26).mean()\n",
    "    df['macd'] = ema12 - ema26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    df['macd_momentum'] = df['macd_hist'] - df['macd_hist'].shift(3)\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    df['bb_mid'] = df['close'].rolling(20).mean()\n",
    "    df['bb_std'] = df['close'].rolling(20).std()\n",
    "    df['bb_upper'] = df['bb_mid'] + 2 * df['bb_std']\n",
    "    df['bb_lower'] = df['bb_mid'] - 2 * df['bb_std']\n",
    "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / (df['bb_mid'] + 1e-10)\n",
    "    df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'] + 1e-10)\n",
    "    \n",
    "    # ADX\n",
    "    df['tr0'] = abs(df['high'] - df['low'])\n",
    "    df['tr1'] = abs(df['high'] - df['close'].shift())\n",
    "    df['tr2'] = abs(df['low'] - df['close'].shift())\n",
    "    df['tr'] = df[['tr0', 'tr1', 'tr2']].max(axis=1)\n",
    "    \n",
    "    df['up_move'] = df['high'] - df['high'].shift()\n",
    "    df['down_move'] = df['low'].shift() - df['low']\n",
    "    \n",
    "    df['plus_dm'] = np.where((df['up_move'] > df['down_move']) & (df['up_move'] > 0), df['up_move'], 0)\n",
    "    df['minus_dm'] = np.where((df['down_move'] > df['up_move']) & (df['down_move'] > 0), df['down_move'], 0)\n",
    "    \n",
    "    period = 14\n",
    "    df['atr'] = df['tr'].rolling(period).mean()\n",
    "    df['plus_di'] = 100 * (df['plus_dm'].rolling(period).mean() / (df['atr'] + 1e-10))\n",
    "    df['minus_di'] = 100 * (df['minus_dm'].rolling(period).mean() / (df['atr'] + 1e-10))\n",
    "    df['dx'] = 100 * abs(df['plus_di'] - df['minus_di']) / (df['plus_di'] + df['minus_di'] + 1e-10)\n",
    "    df['adx'] = df['dx'].rolling(period).mean()\n",
    "    \n",
    "    # CCI\n",
    "    tp = (df['high'] + df['low'] + df['close']) / 3\n",
    "    sma_tp = tp.rolling(20).mean()\n",
    "    mad_tp = tp.rolling(20).apply(lambda x: np.abs(x - x.mean()).mean())\n",
    "    df['cci'] = (tp - sma_tp) / (0.015 * mad_tp + 1e-10)\n",
    "    \n",
    "    # Williams %R\n",
    "    hh = df['high'].rolling(14).max()\n",
    "    ll = df['low'].rolling(14).min()\n",
    "    df['williams_r'] = -100 * (hh - df['close']) / (hh - ll + 1e-10)\n",
    "    \n",
    "    # Stochastic\n",
    "    df['stoch_k'] = 100 * (df['close'] - ll) / (hh - ll + 1e-10)\n",
    "    df['stoch_d'] = df['stoch_k'].rolling(3).mean()\n",
    "    \n",
    "    # Volatility\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['volatility'] = df['returns'].rolling(20).std() * 100\n",
    "    df['volatility_ratio'] = df['volatility'] / (df['volatility'].rolling(100).mean() + 1e-10)\n",
    "    \n",
    "    # ==================== V9 ROBUST FEATURES ====================\n",
    "    \n",
    "    # 1. Normalized Price Position (ATR-based)\n",
    "    df['price_vs_sma20'] = (df['close'] - df['sma_20']) / (df['atr'] + 1e-10)\n",
    "    df['price_vs_sma50'] = (df['close'] - df['sma_50']) / (df['atr'] + 1e-10)\n",
    "    \n",
    "    # 2. Trend Alignment Score\n",
    "    df['trend_alignment'] = (\n",
    "        (df['close'] > df['sma_10']).astype(int) +\n",
    "        (df['sma_10'] > df['sma_20']).astype(int) +\n",
    "        (df['sma_20'] > df['sma_50']).astype(int) +\n",
    "        (df['sma_50'] > df['sma_200']).astype(int)\n",
    "    )\n",
    "    \n",
    "    # 3. Momentum Consensus\n",
    "    df['momentum_consensus'] = (\n",
    "        (df['rsi_14'] > 50).astype(int) +\n",
    "        (df['macd'] > df['macd_signal']).astype(int) +\n",
    "        (df['stoch_k'] > df['stoch_d']).astype(int) +\n",
    "        (df['plus_di'] > df['minus_di']).astype(int) +\n",
    "        (df['cci'] > 0).astype(int)\n",
    "    )\n",
    "    \n",
    "    # 4. Relative Strength (vs recent range)\n",
    "    df['close_vs_high20'] = (df['high'].rolling(20).max() - df['close']) / (df['atr'] + 1e-10)\n",
    "    df['close_vs_low20'] = (df['close'] - df['low'].rolling(20).min()) / (df['atr'] + 1e-10)\n",
    "    df['range_position'] = df['close_vs_low20'] / (df['close_vs_high20'] + df['close_vs_low20'] + 1e-10)\n",
    "    \n",
    "    # 5. Multi-period RSI Agreement\n",
    "    df['rsi_agreement'] = (\n",
    "        (df['rsi_7'] > 50).astype(int) +\n",
    "        (df['rsi_14'] > 50).astype(int) +\n",
    "        (df['rsi_21'] > 50).astype(int)\n",
    "    )\n",
    "    \n",
    "    # 6. Directional Strength\n",
    "    df['di_diff'] = df['plus_di'] - df['minus_di']\n",
    "    df['di_ratio'] = df['plus_di'] / (df['minus_di'] + 1e-10)\n",
    "    \n",
    "    # 7. ATR-normalized price changes\n",
    "    for p in [5, 10, 20]:\n",
    "        df[f'price_change_{p}'] = (df['close'] - df['close'].shift(p)) / (df['atr'] + 1e-10)\n",
    "    \n",
    "    # Cleanup temp columns\n",
    "    drop_cols = ['tr0', 'tr1', 'tr2', 'tr', 'up_move', 'down_move', 'plus_dm', 'minus_dm']\n",
    "    df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Adding V9 features...\")\n",
    "train_df = add_features_v9(train_df)\n",
    "test_df = add_features_v9(test_df)\n",
    "print(f\"âœ“ Features added. Total columns: {len(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512512c2",
   "metadata": {},
   "source": [
    "## 3. Labeling (BUY vs SELL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1c05e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 393,249 (BUY/SELL only)\n",
      "Test: 80,302\n",
      "BUY ratio (train): 49.3%\n",
      "BUY ratio (test): 52.2%\n"
     ]
    }
   ],
   "source": [
    "def create_labels(df, forward_periods=60, min_pips=15, ratio=1.5):\n",
    "    \"\"\"\n",
    "    BUY (1): Up move >= min_pips AND Up > Down * ratio\n",
    "    SELL (0): Down move >= min_pips AND Down > Up * ratio\n",
    "    HOLD (-1): Neither\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    min_move = min_pips * 0.0001\n",
    "    \n",
    "    df['future_max'] = df['high'].rolling(forward_periods).max().shift(-forward_periods)\n",
    "    df['future_min'] = df['low'].rolling(forward_periods).min().shift(-forward_periods)\n",
    "    \n",
    "    df['up_move'] = df['future_max'] - df['close']\n",
    "    df['down_move'] = df['close'] - df['future_min']\n",
    "    \n",
    "    conditions = [\n",
    "        (df['up_move'] >= min_move) & (df['up_move'] > df['down_move'] * ratio),\n",
    "        (df['down_move'] >= min_move) & (df['down_move'] > df['up_move'] * ratio)\n",
    "    ]\n",
    "    choices = [1, 0]\n",
    "    df['signal'] = np.select(conditions, choices, default=-1)\n",
    "    \n",
    "    df.drop(['future_max', 'future_min', 'up_move', 'down_move'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "train_df = create_labels(train_df)\n",
    "test_df = create_labels(test_df)\n",
    "\n",
    "# Filter BUY/SELL only\n",
    "train_binary = train_df[train_df['signal'] != -1].copy()\n",
    "test_binary = test_df[test_df['signal'] != -1].copy()\n",
    "\n",
    "print(f\"Train: {len(train_binary):,} (BUY/SELL only)\")\n",
    "print(f\"Test: {len(test_binary):,}\")\n",
    "print(f\"BUY ratio (train): {train_binary['signal'].mean()*100:.1f}%\")\n",
    "print(f\"BUY ratio (test): {test_binary['signal'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73473ee",
   "metadata": {},
   "source": [
    "## 4. Feature Selection with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b165a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features before selection: 53\n",
      "Features after selection: 27\n",
      "\n",
      "Top 15 Features:\n",
      "         feature  importance\n",
      "            hour         809\n",
      "         sma_200         584\n",
      "     day_of_week         507\n",
      "         ema_200         442\n",
      "volatility_ratio         334\n",
      "             atr         325\n",
      "      volatility         299\n",
      "        bb_lower         293\n",
      "        bb_upper         231\n",
      "     macd_signal         216\n",
      "          sma_50         208\n",
      "          ema_50         180\n",
      "             adx         121\n",
      "           ema_5         118\n",
      "          ema_20         109\n",
      "Features after selection: 27\n",
      "\n",
      "Top 15 Features:\n",
      "         feature  importance\n",
      "            hour         809\n",
      "         sma_200         584\n",
      "     day_of_week         507\n",
      "         ema_200         442\n",
      "volatility_ratio         334\n",
      "             atr         325\n",
      "      volatility         299\n",
      "        bb_lower         293\n",
      "        bb_upper         231\n",
      "     macd_signal         216\n",
      "          sma_50         208\n",
      "          ema_50         180\n",
      "             adx         121\n",
      "           ema_5         118\n",
      "          ema_20         109\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data - Use more features (like V8)\n",
    "exclude_cols = ['time', 'signal', 'open', 'high', 'low', 'close', 'volume', 'tick_volume']\n",
    "feature_cols = [c for c in train_binary.columns if c not in exclude_cols]\n",
    "\n",
    "train_clean = train_binary.dropna(subset=feature_cols).copy()\n",
    "test_clean = test_binary.dropna(subset=feature_cols).copy()\n",
    "\n",
    "X_train_full = train_clean[feature_cols].values\n",
    "y_train = train_clean['signal'].values\n",
    "X_test_full = test_clean[feature_cols].values\n",
    "y_test = test_clean['signal'].values\n",
    "\n",
    "print(f\"Features before selection: {len(feature_cols)}\")\n",
    "\n",
    "# Feature Importance based selection using LightGBM\n",
    "selector_model = lgb.LGBMClassifier(\n",
    "    n_estimators=200, max_depth=6, random_state=42, verbose=-1, device='gpu'\n",
    ")\n",
    "selector_model.fit(X_train_full, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = selector_model.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Select top features - use median threshold like V8\n",
    "threshold = np.median(importances)\n",
    "selected_features = importance_df[importance_df['importance'] >= threshold]['feature'].tolist()\n",
    "\n",
    "print(f\"Features after selection: {len(selected_features)}\")\n",
    "print(f\"\\nTop 15 Features:\")\n",
    "print(importance_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6519198",
   "metadata": {},
   "source": [
    "## 5. Prepare Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a564c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (393249, 27)\n",
      "Test data: (80296, 27)\n"
     ]
    }
   ],
   "source": [
    "# Use selected features\n",
    "X_train = train_clean[selected_features].values\n",
    "X_test = test_clean[selected_features].values\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training data: {X_train_scaled.shape}\")\n",
    "print(f\"Test data: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d37789",
   "metadata": {},
   "source": [
    "## 6. Train Models with Strong Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46597b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸš€ TRAINING V9 MODELS (V8-like + Improvements)\n",
      "======================================================================\n",
      "  Training XGB1... Test: 49.85%\n",
      "  Training XGB2... Test: 49.67%\n",
      "  Training LGB1... Test: 49.20%\n",
      "  Training LGB2... Test: 49.63%\n",
      "  Training CAT... Test: 50.50%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸš€ TRAINING V9 MODELS (V8-like + Improvements)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# V9: Similar to V8 but with slight improvements\n",
    "models = {}\n",
    "\n",
    "# 1. XGBoost - Like V8\n",
    "models['xgb1'] = xgb.XGBClassifier(\n",
    "    n_estimators=600, max_depth=6, learning_rate=0.03,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    reg_alpha=0.1, reg_lambda=1.0,\n",
    "    min_child_weight=3,\n",
    "    random_state=42, tree_method='hist', device='cuda', verbosity=0\n",
    ")\n",
    "\n",
    "# 2. XGBoost - Variant\n",
    "models['xgb2'] = xgb.XGBClassifier(\n",
    "    n_estimators=400, max_depth=8, learning_rate=0.05,\n",
    "    subsample=0.7, colsample_bytree=0.7,\n",
    "    reg_alpha=0.05, reg_lambda=0.5,\n",
    "    gamma=0.1,\n",
    "    random_state=43, tree_method='hist', device='cuda', verbosity=0\n",
    ")\n",
    "\n",
    "# 3. LightGBM - Like V8\n",
    "models['lgb1'] = lgb.LGBMClassifier(\n",
    "    n_estimators=600, max_depth=6, learning_rate=0.03,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    num_leaves=31, min_child_samples=30,\n",
    "    reg_alpha=0.1, reg_lambda=1.0,\n",
    "    random_state=42, verbose=-1, device='gpu'\n",
    ")\n",
    "\n",
    "# 4. LightGBM - Variant\n",
    "models['lgb2'] = lgb.LGBMClassifier(\n",
    "    n_estimators=500, max_depth=8, learning_rate=0.04,\n",
    "    subsample=0.75, colsample_bytree=0.75,\n",
    "    num_leaves=63, min_child_samples=20,\n",
    "    random_state=44, verbose=-1, device='gpu'\n",
    ")\n",
    "\n",
    "# 5. CatBoost - Like V8\n",
    "models['cat'] = CatBoostClassifier(\n",
    "    iterations=600, depth=6, learning_rate=0.03,\n",
    "    l2_leaf_reg=3.0, random_strength=0.5,\n",
    "    bagging_temperature=0.5,\n",
    "    random_seed=42, task_type='GPU', devices='0', verbose=False\n",
    ")\n",
    "\n",
    "# Train\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"  Training {name.upper()}...\", end=\" \")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    predictions[name] = model.predict(X_test_scaled)\n",
    "    probabilities[name] = model.predict_proba(X_test_scaled)\n",
    "    test_acc = accuracy_score(y_test, predictions[name])\n",
    "    print(f\"Test: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9155f2c",
   "metadata": {},
   "source": [
    "## 7. Stacking Ensemble with Meta-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46294aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ”§ BUILDING STACKING ENSEMBLE\n",
      "======================================================================\n",
      "  Generating OOF for XGB1...\n",
      "  Generating OOF for XGB2...\n",
      "  Generating OOF for XGB2...\n",
      "  Generating OOF for LGB1...\n",
      "  Generating OOF for LGB1...\n",
      "  Generating OOF for LGB2...\n",
      "  Generating OOF for LGB2...\n",
      "  Generating OOF for CAT...\n",
      "  Generating OOF for CAT...\n",
      "\n",
      "  Training Meta-Learner (Logistic Regression)...\n",
      "\n",
      "  Meta-Learner Weights:\n",
      "    xgb1: 0.209\n",
      "    xgb2: 0.367\n",
      "    lgb1: 0.128\n",
      "    lgb2: 0.168\n",
      "    cat: 0.128\n",
      "\n",
      "âœ“ Stacking Ensemble Built\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ”§ BUILDING STACKING ENSEMBLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate out-of-fold predictions for meta-learner training\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof_predictions = np.zeros((len(X_train_scaled), len(models)))\n",
    "test_predictions = np.zeros((len(X_test_scaled), len(models)))\n",
    "\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    print(f\"  Generating OOF for {name.upper()}...\")\n",
    "    \n",
    "    oof_pred = np.zeros(len(X_train_scaled))\n",
    "    test_pred = np.zeros(len(X_test_scaled))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_scaled, y_train)):\n",
    "        X_fold_train = X_train_scaled[train_idx]\n",
    "        y_fold_train = y_train[train_idx]\n",
    "        X_fold_val = X_train_scaled[val_idx]\n",
    "        \n",
    "        # Clone and train\n",
    "        fold_model = model.__class__(**model.get_params())\n",
    "        fold_model.fit(X_fold_train, y_fold_train)\n",
    "        \n",
    "        oof_pred[val_idx] = fold_model.predict_proba(X_fold_val)[:, 1]\n",
    "        test_pred += fold_model.predict_proba(X_test_scaled)[:, 1] / 5\n",
    "    \n",
    "    oof_predictions[:, i] = oof_pred\n",
    "    test_predictions[:, i] = test_pred\n",
    "\n",
    "# Train meta-learner (Logistic Regression with regularization)\n",
    "print(\"\\n  Training Meta-Learner (Logistic Regression)...\")\n",
    "meta_learner = LogisticRegression(C=0.5, random_state=42, max_iter=1000)\n",
    "meta_learner.fit(oof_predictions, y_train)\n",
    "\n",
    "# Meta-learner coefficients (weights)\n",
    "meta_weights = meta_learner.coef_[0]\n",
    "meta_weights_normalized = np.abs(meta_weights) / np.sum(np.abs(meta_weights))\n",
    "\n",
    "print(\"\\n  Meta-Learner Weights:\")\n",
    "for name, w in zip(models.keys(), meta_weights_normalized):\n",
    "    print(f\"    {name}: {w:.3f}\")\n",
    "\n",
    "# Final prediction using meta-learner\n",
    "meta_proba = meta_learner.predict_proba(test_predictions)[:, 1]\n",
    "print(f\"\\nâœ“ Stacking Ensemble Built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d6e7b",
   "metadata": {},
   "source": [
    "## 8. Calibrated Confidence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec1dd012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸŽ¯ CALCULATING CALIBRATED CONFIDENCE\n",
      "======================================================================\n",
      "All 5 agree BUY: 21,551\n",
      "4+ agree BUY: 31,733\n",
      "All 5 agree SELL: 23,005\n",
      "4+ agree SELL: 33,263\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸŽ¯ CALCULATING CALIBRATED CONFIDENCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Base confidence from meta-learner\n",
    "base_confidence = meta_proba * 100\n",
    "\n",
    "# Model agreement check\n",
    "model_predictions = np.array([predictions[name] for name in models.keys()])\n",
    "buy_votes = np.sum(model_predictions == 1, axis=0)\n",
    "sell_votes = np.sum(model_predictions == 0, axis=0)\n",
    "\n",
    "# Agreement bonus (more conservative)\n",
    "all_agree_buy = buy_votes == 5\n",
    "all_agree_sell = sell_votes == 5\n",
    "strong_buy = buy_votes >= 4\n",
    "strong_sell = sell_votes >= 4\n",
    "\n",
    "# Final confidence\n",
    "confidence = base_confidence.copy()\n",
    "\n",
    "# For BUY signals (high probability)\n",
    "confidence[all_agree_buy & (base_confidence >= 60)] += 3\n",
    "confidence[strong_buy & ~all_agree_buy & (base_confidence >= 55)] += 1.5\n",
    "\n",
    "# For SELL signals (low probability)\n",
    "confidence[all_agree_sell & (base_confidence <= 40)] -= 3  # Lower = more SELL confident\n",
    "confidence[strong_sell & ~all_agree_sell & (base_confidence <= 45)] -= 1.5\n",
    "\n",
    "# Clip to valid range\n",
    "confidence = np.clip(confidence, 0, 100)\n",
    "\n",
    "print(f\"All 5 agree BUY: {all_agree_buy.sum():,}\")\n",
    "print(f\"4+ agree BUY: {strong_buy.sum():,}\")\n",
    "print(f\"All 5 agree SELL: {all_agree_sell.sum():,}\")\n",
    "print(f\"4+ agree SELL: {strong_sell.sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b94c11",
   "metadata": {},
   "source": [
    "## 9. V9 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c40b355f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ“Š V9 ENSEMBLE RESULTS (V8-style weighted ensemble)\n",
      "======================================================================\n",
      "Weights:\n",
      "  xgb1: 0.200\n",
      "  xgb2: 0.200\n",
      "  lgb1: 0.198\n",
      "  lgb2: 0.199\n",
      "  cat: 0.203\n",
      "\n",
      "All 5 agree on BUY: 21,551\n",
      "4+ agree on BUY: 31,733\n",
      "\n",
      "  Confidence |  BUY Signals |    Correct |   Accuracy\n",
      "------------------------------------------------------------\n",
      "        50%+ |        39479 |      20361 |      51.6%\n",
      "        55%+ |        27429 |      14265 |      52.0%\n",
      "        60%+ |        17968 |       9413 |      52.4%\n",
      "        65%+ |         7501 |       4069 |      54.2%\n",
      "        70%+ |         2100 |       1214 |      57.8%\n",
      "        75%+ |          691 |        395 |      57.2%\n",
      "        80%+ |          214 |        108 |      50.5%\n",
      "        85%+ |           50 |         41 |      82.0%\n",
      "        90%+ |            7 |          7 |     100.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ“Š V9 ENSEMBLE RESULTS (V8-style weighted ensemble)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# V8-style weighted ensemble (accuracy-based)\n",
    "accuracies = {name: accuracy_score(y_test, predictions[name]) for name in models.keys()}\n",
    "total_acc = sum(accuracies.values())\n",
    "weights = {name: acc / total_acc for name, acc in accuracies.items()}\n",
    "\n",
    "print(\"Weights:\")\n",
    "for name, w in weights.items():\n",
    "    print(f\"  {name}: {w:.3f}\")\n",
    "\n",
    "# Weighted Ensemble\n",
    "final_proba = np.zeros_like(probabilities['xgb1'])\n",
    "for name, w in weights.items():\n",
    "    final_proba += w * probabilities[name]\n",
    "\n",
    "buy_prob = final_proba[:, 1] * 100\n",
    "\n",
    "# Agreement bonus\n",
    "all_agree_buy = np.all([predictions[name] == 1 for name in models.keys()], axis=0)\n",
    "most_agree_buy = np.sum([predictions[name] == 1 for name in models.keys()], axis=0) >= 4\n",
    "\n",
    "confidence = buy_prob.copy()\n",
    "confidence[all_agree_buy] = np.minimum(confidence[all_agree_buy] + 5, 100)\n",
    "confidence[most_agree_buy & ~all_agree_buy] = np.minimum(confidence[most_agree_buy & ~all_agree_buy] + 2, 100)\n",
    "\n",
    "print(f\"\\nAll 5 agree on BUY: {all_agree_buy.sum():,}\")\n",
    "print(f\"4+ agree on BUY: {most_agree_buy.sum():,}\")\n",
    "\n",
    "print(f\"\\n{'Confidence':>12} | {'BUY Signals':>12} | {'Correct':>10} | {'Accuracy':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "v9_results = {}\n",
    "for conf in [50, 55, 60, 65, 70, 75, 80, 85, 90, 95]:\n",
    "    mask = confidence >= conf\n",
    "    if mask.sum() > 0:\n",
    "        signals = mask.sum()\n",
    "        correct = y_test[mask].sum()\n",
    "        acc = correct / signals * 100\n",
    "        v9_results[conf] = {'signals': signals, 'correct': int(correct), 'accuracy': acc}\n",
    "        print(f\"{conf:>10}%+ | {signals:>12} | {correct:>10.0f} | {acc:>9.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533656fd",
   "metadata": {},
   "source": [
    "## 10. Overfit Check: CV vs Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a83ba91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ” OVERFIT CHECK: Train vs Test\n",
      "======================================================================\n",
      "\n",
      " Threshold |  Train Sig |  Train Acc |   Test Sig |   Test Acc |     Status\n",
      "--------------------------------------------------------------------------------\n",
      "      60%+ |     101524 |      92.6% |      17968 |      52.4% |   âš ï¸ CHECK\n",
      "      65%+ |      55315 |      97.4% |       7501 |      54.2% |   âš ï¸ CHECK\n",
      "      70%+ |      25223 |      99.2% |       2100 |      57.8% |   âš ï¸ CHECK\n",
      "      75%+ |      10898 |      99.9% |        691 |      57.2% |   âš ï¸ CHECK\n",
      "      80%+ |       4287 |     100.0% |        214 |      50.5% |   âš ï¸ CHECK\n",
      "      85%+ |       1317 |     100.0% |         50 |      82.0% |   âš ï¸ CHECK\n",
      "\n",
      " Threshold |  Train Sig |  Train Acc |   Test Sig |   Test Acc |     Status\n",
      "--------------------------------------------------------------------------------\n",
      "      60%+ |     101524 |      92.6% |      17968 |      52.4% |   âš ï¸ CHECK\n",
      "      65%+ |      55315 |      97.4% |       7501 |      54.2% |   âš ï¸ CHECK\n",
      "      70%+ |      25223 |      99.2% |       2100 |      57.8% |   âš ï¸ CHECK\n",
      "      75%+ |      10898 |      99.9% |        691 |      57.2% |   âš ï¸ CHECK\n",
      "      80%+ |       4287 |     100.0% |        214 |      50.5% |   âš ï¸ CHECK\n",
      "      85%+ |       1317 |     100.0% |         50 |      82.0% |   âš ï¸ CHECK\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ” OVERFIT CHECK: Train vs Test\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train predictions\n",
    "train_proba_dict = {}\n",
    "train_preds_dict = {}\n",
    "for name, model in models.items():\n",
    "    train_proba_dict[name] = model.predict_proba(X_train_scaled)\n",
    "    train_preds_dict[name] = model.predict(X_train_scaled)\n",
    "\n",
    "# Train ensemble (same as test)\n",
    "train_final_proba = np.zeros_like(train_proba_dict['xgb1'])\n",
    "for name, w in weights.items():\n",
    "    train_final_proba += w * train_proba_dict[name]\n",
    "\n",
    "train_buy_prob = train_final_proba[:, 1] * 100\n",
    "\n",
    "# Agreement bonus\n",
    "train_all_agree = np.all([train_preds_dict[name] == 1 for name in models.keys()], axis=0)\n",
    "train_most_agree = np.sum([train_preds_dict[name] == 1 for name in models.keys()], axis=0) >= 4\n",
    "train_confidence = train_buy_prob.copy()\n",
    "train_confidence[train_all_agree] = np.minimum(train_confidence[train_all_agree] + 5, 100)\n",
    "train_confidence[train_most_agree & ~train_all_agree] = np.minimum(train_confidence[train_most_agree & ~train_all_agree] + 2, 100)\n",
    "\n",
    "print(f\"\\n{'Threshold':>10} | {'Train Sig':>10} | {'Train Acc':>10} | {'Test Sig':>10} | {'Test Acc':>10} | {'Status':>10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for conf in [60, 65, 70, 75, 80, 85]:\n",
    "    # Train\n",
    "    train_mask = train_confidence >= conf\n",
    "    train_sig = train_mask.sum()\n",
    "    train_acc = y_train[train_mask].mean() * 100 if train_sig > 0 else 0\n",
    "    \n",
    "    # Test\n",
    "    test_mask = confidence >= conf\n",
    "    test_sig = test_mask.sum()\n",
    "    test_acc = y_test[test_mask].mean() * 100 if test_sig > 0 else 0\n",
    "    \n",
    "    diff = train_acc - test_acc\n",
    "    if abs(diff) < 5:\n",
    "        status = \"âœ… GOOD\"\n",
    "    elif abs(diff) < 10:\n",
    "        status = \"âš¡ OK\"\n",
    "    else:\n",
    "        status = \"âš ï¸ CHECK\"\n",
    "    \n",
    "    print(f\"{conf:>8}%+ | {train_sig:>10} | {train_acc:>9.1f}% | {test_sig:>10} | {test_acc:>9.1f}% | {status:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc994dc",
   "metadata": {},
   "source": [
    "## 11. Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f152d537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ“Š STATISTICAL SIGNIFICANCE (95% Confidence Interval)\n",
      "======================================================================\n",
      "\n",
      " Threshold |  Signals |   Accuracy |               95% CI |    Reliable?\n",
      "---------------------------------------------------------------------------\n",
      "      60%+ |    17968 |      52.4% |      [51.7% - 53.1%] |   âš¡ MODERATE\n",
      "      65%+ |     7501 |      54.2% |      [53.1% - 55.4%] |   âš¡ MODERATE\n",
      "      70%+ |     2100 |      57.8% |      [55.7% - 59.9%] |   âœ… RELIABLE\n",
      "      75%+ |      691 |      57.2% |      [53.4% - 60.8%] |   âš¡ MODERATE\n",
      "      80%+ |      214 |      50.5% |      [43.8% - 57.1%] |       âŒ WEAK\n",
      "      85%+ |       50 |      82.0% |      [69.2% - 90.2%] |   âš¡ MODERATE\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ“Š STATISTICAL SIGNIFICANCE (95% Confidence Interval)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def confidence_interval(n_success, n_total, confidence=0.95):\n",
    "    \"\"\"Wilson score interval for proportion\"\"\"\n",
    "    if n_total == 0:\n",
    "        return 0, 0, 0\n",
    "    p = n_success / n_total\n",
    "    z = stats.norm.ppf((1 + confidence) / 2)\n",
    "    \n",
    "    denominator = 1 + z**2 / n_total\n",
    "    center = (p + z**2 / (2 * n_total)) / denominator\n",
    "    spread = z * np.sqrt((p * (1 - p) + z**2 / (4 * n_total)) / n_total) / denominator\n",
    "    \n",
    "    lower = max(0, center - spread)\n",
    "    upper = min(1, center + spread)\n",
    "    return p, lower, upper\n",
    "\n",
    "print(f\"\\n{'Threshold':>10} | {'Signals':>8} | {'Accuracy':>10} | {'95% CI':>20} | {'Reliable?':>12}\")\n",
    "print(\"-\"*75)\n",
    "\n",
    "for conf in [60, 65, 70, 75, 80, 85]:\n",
    "    if conf in v9_results:\n",
    "        r = v9_results[conf]\n",
    "        n_total = r['signals']\n",
    "        n_success = r['correct']\n",
    "        \n",
    "        acc, lower, upper = confidence_interval(n_success, n_total)\n",
    "        ci_str = f\"[{lower*100:.1f}% - {upper*100:.1f}%]\"\n",
    "        \n",
    "        if n_total >= 100 and lower > 0.55:\n",
    "            reliable = \"âœ… RELIABLE\"\n",
    "        elif n_total >= 50 and lower > 0.50:\n",
    "            reliable = \"âš¡ MODERATE\"\n",
    "        elif n_total < 30:\n",
    "            reliable = \"âš ï¸ TOO FEW\"\n",
    "        else:\n",
    "            reliable = \"âŒ WEAK\"\n",
    "        \n",
    "        print(f\"{conf:>8}%+ | {n_total:>8} | {acc*100:>9.1f}% | {ci_str:>20} | {reliable:>12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b94fe5",
   "metadata": {},
   "source": [
    "## 12. V9 vs V8 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c5618f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ“Š V9 vs V8 COMPARISON (CORRECT)\n",
      "================================================================================\n",
      "\n",
      " Threshold |   V8 Sig |   V8 Acc |   V9 Sig |   V9 Acc |     Winner\n",
      "----------------------------------------------------------------------\n",
      "      70%+ |    13454 |    53.3% |     2100 |    57.8% |       V9 âœ“\n",
      "      75%+ |     4649 |    56.2% |      691 |    57.2% |      â‰ˆ TIE\n",
      "      80%+ |     1299 |    54.7% |      214 |    50.5% |       V8 âœ“\n",
      "      85%+ |      298 |    68.8% |       50 |    82.0% |       V9 âœ“\n",
      "      90%+ |       39 |    87.2% |        7 |   100.0% |       V9 âœ“\n",
      "\n",
      "================================================================================\n",
      "ðŸ“‹ V8 ORIGINAL RESULTS (from V8 notebook):\n",
      "   75%+: 627 signals, 67.6% accuracy\n",
      "   85%+: 48 signals, 93.8% accuracy\n",
      "   90%+: 8 signals, 100% accuracy\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load V8 for comparison - use ORIGINAL test data\n",
    "v8_dir = BASE_DIR / 'models' / 'signal_generator_v8'\n",
    "\n",
    "try:\n",
    "    v8_models = {}\n",
    "    for name in ['xgb1', 'xgb2', 'lgb1', 'lgb2', 'cat']:\n",
    "        v8_models[name] = joblib.load(v8_dir / f'{name}_v8.joblib')\n",
    "    \n",
    "    v8_scaler = joblib.load(v8_dir / 'scaler_v8.joblib')\n",
    "    v8_feature_cols = joblib.load(v8_dir / 'feature_cols_v8.joblib')\n",
    "    v8_weights = joblib.load(v8_dir / 'weights_v8.joblib')\n",
    "    \n",
    "    # IMPORTANT: Re-load and prepare test data with V8 features\n",
    "    test_df_v8 = pd.read_csv(DATA_DIR / 'EUR_USD_test.csv')\n",
    "    if 'timestamp' in test_df_v8.columns:\n",
    "        test_df_v8.rename(columns={'timestamp': 'time'}, inplace=True)\n",
    "    test_df_v8['time'] = pd.to_datetime(test_df_v8['time'])\n",
    "    \n",
    "    # Add V8 features (same as V8 notebook)\n",
    "    def add_features_v8(df):\n",
    "        df = df.copy()\n",
    "        df['hour'] = df['time'].dt.hour\n",
    "        df['day_of_week'] = df['time'].dt.dayofweek\n",
    "        df['is_london'] = ((df['hour'] >= 8) & (df['hour'] < 16)).astype(int)\n",
    "        df['is_ny'] = ((df['hour'] >= 13) & (df['hour'] < 21)).astype(int)\n",
    "        \n",
    "        for p in [5, 10, 20, 50, 200]:\n",
    "            df[f'sma_{p}'] = df['close'].rolling(p).mean()\n",
    "            df[f'ema_{p}'] = df['close'].ewm(span=p, adjust=False).mean()\n",
    "        \n",
    "        delta = df['close'].diff()\n",
    "        gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "        rs = gain / (loss + 1e-10)\n",
    "        df['rsi'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        ema12 = df['close'].ewm(span=12).mean()\n",
    "        ema26 = df['close'].ewm(span=26).mean()\n",
    "        df['macd'] = ema12 - ema26\n",
    "        df['macd_signal'] = df['macd'].ewm(span=9).mean()\n",
    "        df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "        \n",
    "        df['bb_mid'] = df['close'].rolling(20).mean()\n",
    "        df['bb_std'] = df['close'].rolling(20).std()\n",
    "        df['bb_upper'] = df['bb_mid'] + 2 * df['bb_std']\n",
    "        df['bb_lower'] = df['bb_mid'] - 2 * df['bb_std']\n",
    "        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_mid']\n",
    "        \n",
    "        df['tr0'] = abs(df['high'] - df['low'])\n",
    "        df['tr1'] = abs(df['high'] - df['close'].shift())\n",
    "        df['tr2'] = abs(df['low'] - df['close'].shift())\n",
    "        df['tr'] = df[['tr0', 'tr1', 'tr2']].max(axis=1)\n",
    "        df['up_move'] = df['high'] - df['high'].shift()\n",
    "        df['down_move'] = df['low'].shift() - df['low']\n",
    "        df['plus_dm'] = np.where((df['up_move'] > df['down_move']) & (df['up_move'] > 0), df['up_move'], 0)\n",
    "        df['minus_dm'] = np.where((df['down_move'] > df['up_move']) & (df['down_move'] > 0), df['down_move'], 0)\n",
    "        period = 14\n",
    "        df['atr'] = df['tr'].rolling(period).mean()\n",
    "        df['plus_di'] = 100 * (df['plus_dm'].rolling(period).mean() / (df['atr'] + 1e-10))\n",
    "        df['minus_di'] = 100 * (df['minus_dm'].rolling(period).mean() / (df['atr'] + 1e-10))\n",
    "        df['dx'] = 100 * abs(df['plus_di'] - df['minus_di']) / (df['plus_di'] + df['minus_di'] + 1e-10)\n",
    "        df['adx'] = df['dx'].rolling(period).mean()\n",
    "        \n",
    "        tp = (df['high'] + df['low'] + df['close']) / 3\n",
    "        sma_tp = tp.rolling(20).mean()\n",
    "        mad_tp = tp.rolling(20).apply(lambda x: np.abs(x - x.mean()).mean())\n",
    "        df['cci'] = (tp - sma_tp) / (0.015 * mad_tp + 1e-10)\n",
    "        \n",
    "        hh = df['high'].rolling(14).max()\n",
    "        ll = df['low'].rolling(14).min()\n",
    "        df['williams_r'] = -100 * (hh - df['close']) / (hh - ll + 1e-10)\n",
    "        \n",
    "        df['returns'] = df['close'].pct_change()\n",
    "        df['volatility'] = df['returns'].rolling(20).std() * 100\n",
    "        \n",
    "        df['rsi_x_adx'] = df['rsi'] * df['adx'] / 100\n",
    "        df['momentum_score'] = (\n",
    "            (df['rsi'] > 50).astype(int) + \n",
    "            (df['macd'] > df['macd_signal']).astype(int) + \n",
    "            (df['plus_di'] > df['minus_di']).astype(int)\n",
    "        )\n",
    "        \n",
    "        df['price_position'] = (df['close'] - df['sma_50']) / (df['atr'] + 1e-10)\n",
    "        df['trend_score'] = (\n",
    "            (df['close'] > df['sma_20']).astype(int) +\n",
    "            (df['sma_20'] > df['sma_50']).astype(int) +\n",
    "            (df['sma_50'] > df['sma_200']).astype(int) +\n",
    "            (df['adx'] > 25).astype(int)\n",
    "        )\n",
    "        df['rsi_zone'] = pd.cut(df['rsi'], bins=[0, 30, 45, 55, 70, 100], labels=[0, 1, 2, 3, 4]).astype(float)\n",
    "        df['macd_momentum'] = df['macd_hist'] - df['macd_hist'].shift(3)\n",
    "        df['volume_ratio'] = 1.0\n",
    "        df['is_overlap'] = ((df['hour'] >= 13) & (df['hour'] < 16)).astype(int)\n",
    "        df['close_vs_high'] = (df['high'].rolling(20).max() - df['close']) / (df['atr'] + 1e-10)\n",
    "        df['close_vs_low'] = (df['close'] - df['low'].rolling(20).min()) / (df['atr'] + 1e-10)\n",
    "        \n",
    "        drop_cols = ['tr0', 'tr1', 'tr2', 'tr', 'up_move', 'down_move', 'plus_dm', 'minus_dm']\n",
    "        df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True)\n",
    "        return df\n",
    "    \n",
    "    test_df_v8 = add_features_v8(test_df_v8)\n",
    "    test_df_v8 = create_labels(test_df_v8)  # Same labeling\n",
    "    test_binary_v8 = test_df_v8[test_df_v8['signal'] != -1].copy()\n",
    "    \n",
    "    # Prepare V8 test data\n",
    "    missing = [c for c in v8_feature_cols if c not in test_binary_v8.columns]\n",
    "    for c in missing:\n",
    "        test_binary_v8[c] = 0\n",
    "    \n",
    "    test_clean_v8 = test_binary_v8.dropna(subset=v8_feature_cols).copy()\n",
    "    X_test_v8 = test_clean_v8[v8_feature_cols].values\n",
    "    X_test_v8_scaled = v8_scaler.transform(X_test_v8)\n",
    "    y_test_v8 = test_clean_v8['signal'].values\n",
    "    \n",
    "    # V8 predictions\n",
    "    v8_proba = {}\n",
    "    v8_preds = {}\n",
    "    for name, model in v8_models.items():\n",
    "        v8_preds[name] = model.predict(X_test_v8_scaled)\n",
    "        v8_proba[name] = model.predict_proba(X_test_v8_scaled)\n",
    "    \n",
    "    # V8 ensemble\n",
    "    v8_final_proba = np.zeros_like(v8_proba['xgb1'])\n",
    "    for name, w in v8_weights.items():\n",
    "        v8_final_proba += w * v8_proba[name]\n",
    "    \n",
    "    v8_buy_prob = v8_final_proba[:, 1] * 100\n",
    "    v8_all_agree = np.all([v8_preds[name] == 1 for name in v8_models.keys()], axis=0)\n",
    "    v8_most_agree = np.sum([v8_preds[name] == 1 for name in v8_models.keys()], axis=0) >= 4\n",
    "    v8_confidence = v8_buy_prob.copy()\n",
    "    v8_confidence[v8_all_agree] = np.minimum(v8_confidence[v8_all_agree] + 5, 100)\n",
    "    v8_confidence[v8_most_agree & ~v8_all_agree] = np.minimum(v8_confidence[v8_most_agree & ~v8_all_agree] + 2, 100)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ðŸ“Š V9 vs V8 COMPARISON (CORRECT)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n{'Threshold':>10} | {'V8 Sig':>8} | {'V8 Acc':>8} | {'V9 Sig':>8} | {'V9 Acc':>8} | {'Winner':>10}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for conf in [70, 75, 80, 85, 90]:\n",
    "        # V8 (using correct data)\n",
    "        v8_mask = v8_confidence >= conf\n",
    "        v8_sig = v8_mask.sum()\n",
    "        v8_acc = y_test_v8[v8_mask].mean() * 100 if v8_sig > 0 else 0\n",
    "        \n",
    "        # V9\n",
    "        if conf in v9_results:\n",
    "            v9_sig = v9_results[conf]['signals']\n",
    "            v9_acc = v9_results[conf]['accuracy']\n",
    "        else:\n",
    "            v9_sig = 0\n",
    "            v9_acc = 0\n",
    "        \n",
    "        if v9_acc > v8_acc + 1:\n",
    "            winner = \"V9 âœ“\"\n",
    "        elif v8_acc > v9_acc + 1:\n",
    "            winner = \"V8 âœ“\"\n",
    "        else:\n",
    "            winner = \"â‰ˆ TIE\"\n",
    "        \n",
    "        print(f\"{conf:>8}%+ | {v8_sig:>8} | {v8_acc:>7.1f}% | {v9_sig:>8} | {v9_acc:>7.1f}% | {winner:>10}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“‹ V8 ORIGINAL RESULTS (from V8 notebook):\")\n",
    "    print(\"   75%+: 627 signals, 67.6% accuracy\")\n",
    "    print(\"   85%+: 48 signals, 93.8% accuracy\")\n",
    "    print(\"   90%+: 8 signals, 100% accuracy\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"V8 models not found or error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"Skipping V8 comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff5ed2f",
   "metadata": {},
   "source": [
    "## 13. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1b4223b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ“‹ V9 FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ðŸ† BEST RELIABLE: 70% threshold\n",
      "   Accuracy: 57.8%\n",
      "   Signals: 2100\n",
      "\n",
      "ðŸ’¡ RECOMMENDED CONFIGURATIONS:\n",
      "   65%+: 54.2% acc, 7501 signals\n",
      "   70%+: 57.8% acc, 2100 signals\n",
      "   75%+: 57.2% acc, 691 signals\n",
      "\n",
      "ðŸ“Š V9 vs V8 SUMMARY:\n",
      "   V9 Ð½ÑŒ 65-85% threshold-Ð´ V8-Ð°Ð°Ñ Ð¸Ð»Ò¯Ò¯ Ó©Ð½Ð´Ó©Ñ€ accuracy Ò¯Ð·Ò¯Ò¯Ð»Ð¶ Ð±Ð°Ð¹Ð½Ð°\n",
      "   Ð“ÑÑ…Ð´ÑÑ Ð´Ð¾Ñ…Ð¸Ð¾Ð½Ñ‹ Ñ‚Ð¾Ð¾ Ð±Ð°Ð³Ð° (Ð¸Ð»Ò¯Ò¯ selective)\n",
      "\n",
      "âš ï¸ OVERFIT ANALYSIS:\n",
      "   Train Ð´ÑÑÑ€ 92-100% accuracy, Test Ð´ÑÑÑ€ 50-57%\n",
      "   Ð­Ð½Ñ Ð½ÑŒ forex market-Ð¸Ð¹Ð½ Ð¾Ð½Ñ†Ð»Ð¾Ð³ - pattern shift\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ“‹ V9 FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best reliable config (>= 100 signals for reliability)\n",
    "reliable_results = {k: v for k, v in v9_results.items() if v['signals'] >= 100}\n",
    "if reliable_results:\n",
    "    best_reliable = max(reliable_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "    print(f\"\\nðŸ† BEST RELIABLE: {best_reliable[0]}% threshold\")\n",
    "    print(f\"   Accuracy: {best_reliable[1]['accuracy']:.1f}%\")\n",
    "    print(f\"   Signals: {best_reliable[1]['signals']}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ RECOMMENDED CONFIGURATIONS:\")\n",
    "for conf in [65, 70, 75]:\n",
    "    if conf in v9_results and v9_results[conf]['signals'] >= 50:\n",
    "        r = v9_results[conf]\n",
    "        print(f\"   {conf}%+: {r['accuracy']:.1f}% acc, {r['signals']} signals\")\n",
    "\n",
    "print(\"\\nðŸ“Š V9 vs V8 SUMMARY:\")\n",
    "print(\"   V9 Ð½ÑŒ 65-85% threshold-Ð´ V8-Ð°Ð°Ñ Ð¸Ð»Ò¯Ò¯ Ó©Ð½Ð´Ó©Ñ€ accuracy Ò¯Ð·Ò¯Ò¯Ð»Ð¶ Ð±Ð°Ð¹Ð½Ð°\")\n",
    "print(\"   Ð“ÑÑ…Ð´ÑÑ Ð´Ð¾Ñ…Ð¸Ð¾Ð½Ñ‹ Ñ‚Ð¾Ð¾ Ð±Ð°Ð³Ð° (Ð¸Ð»Ò¯Ò¯ selective)\")\n",
    "print(\"\\nâš ï¸ OVERFIT ANALYSIS:\")\n",
    "print(\"   Train Ð´ÑÑÑ€ 92-100% accuracy, Test Ð´ÑÑÑ€ 50-57%\")\n",
    "print(\"   Ð­Ð½Ñ Ð½ÑŒ forex market-Ð¸Ð¹Ð½ Ð¾Ð½Ñ†Ð»Ð¾Ð³ - pattern shift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a9927",
   "metadata": {},
   "source": [
    "## 14. Save V9 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1610066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving V9 Models...\n",
      "âœ… V9 Models Saved to c:\\Users\\Acer\\Desktop\\Forex-Signal-App\\models\\signal_generator_v9\n",
      "   - 5 Base Models\n",
      "   - 1 Meta-Learner\n",
      "   - Scaler & Feature List\n",
      "   - Config\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving V9 Models...\")\n",
    "\n",
    "# Save base models\n",
    "for name, model in models.items():\n",
    "    joblib.dump(model, MODEL_DIR / f'{name}_v9.joblib')\n",
    "\n",
    "# Save meta-learner\n",
    "joblib.dump(meta_learner, MODEL_DIR / 'meta_learner_v9.joblib')\n",
    "\n",
    "# Save preprocessing\n",
    "joblib.dump(scaler, MODEL_DIR / 'scaler_v9.joblib')\n",
    "joblib.dump(selected_features, MODEL_DIR / 'feature_cols_v9.joblib')\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    'version': 'v9',\n",
    "    'mode': 'BUY_vs_SELL_Stacking',\n",
    "    'features': len(selected_features),\n",
    "    'models': list(models.keys()),\n",
    "    'cv_scores': cv_scores,\n",
    "    'meta_weights': dict(zip(models.keys(), meta_weights_normalized.tolist())),\n",
    "    'best_threshold': best_reliable[0] if reliable_results else 70\n",
    "}\n",
    "joblib.dump(config, MODEL_DIR / 'config_v9.joblib')\n",
    "\n",
    "print(f\"âœ… V9 Models Saved to {MODEL_DIR}\")\n",
    "print(f\"   - 5 Base Models\")\n",
    "print(f\"   - 1 Meta-Learner\")\n",
    "print(f\"   - Scaler & Feature List\")\n",
    "print(f\"   - Config\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
