{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02eca6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸš€ FOREX SIGNAL GENERATOR V4 - DYNAMIC ATR & STACKING\n",
      "============================================================\n",
      "âœ“ Libraries loaded\n",
      "âœ“ GPU Available: True\n",
      "âœ“ Models Directory: c:\\Users\\Acer\\Desktop\\Forex-Signal-App\\models\\signal_generator_v4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "# GPU check\n",
    "import torch\n",
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "MODEL_DIR = BASE_DIR / 'models' / 'signal_generator_v4'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸš€ FOREX SIGNAL GENERATOR V4 - DYNAMIC ATR & STACKING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ“ Libraries loaded\")\n",
    "print(f\"âœ“ GPU Available: {GPU_AVAILABLE}\")\n",
    "print(f\"âœ“ Models Directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2dfe00",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33b68dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 1,859,492 rows\n",
      "Test data: 296,778 rows\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(DATA_DIR / 'EUR_USD_1min.csv')\n",
    "test_df = pd.read_csv(DATA_DIR / 'EUR_USD_test.csv')\n",
    "\n",
    "# Rename timestamp to time if needed\n",
    "if 'timestamp' in train_df.columns:\n",
    "    train_df.rename(columns={'timestamp': 'time'}, inplace=True)\n",
    "if 'timestamp' in test_df.columns:\n",
    "    test_df.rename(columns={'timestamp': 'time'}, inplace=True)\n",
    "\n",
    "# Convert time to datetime\n",
    "train_df['time'] = pd.to_datetime(train_df['time'])\n",
    "test_df['time'] = pd.to_datetime(test_df['time'])\n",
    "\n",
    "print(f\"Train data: {len(train_df):,} rows\")\n",
    "print(f\"Test data: {len(test_df):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dae283a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding V4 features (with Lags)...\n",
      "âœ“ Features added: 47\n",
      "âœ“ Features added: 47\n"
     ]
    }
   ],
   "source": [
    "def add_features_v4(df):\n",
    "    \"\"\"Add technical indicators + Time features for V4.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # --- 1. Time Features (NEW in v4) ---\n",
    "    # Forex sessions matter. London/NY overlap is most volatile.\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    df['day_of_week'] = df['time'].dt.dayofweek\n",
    "    \n",
    "    # Session flags (Approximate UTC)\n",
    "    # London: 07:00 - 16:00 UTC\n",
    "    # NY: 12:00 - 21:00 UTC\n",
    "    df['is_london'] = ((df['hour'] >= 7) & (df['hour'] < 16)).astype(int)\n",
    "    df['is_ny'] = ((df['hour'] >= 12) & (df['hour'] < 21)).astype(int)\n",
    "    df['is_overlap'] = ((df['hour'] >= 12) & (df['hour'] < 16)).astype(int)\n",
    "    \n",
    "    # --- 2. Standard Indicators ---\n",
    "    # Moving Averages\n",
    "    for period in [10, 20, 50, 200]:\n",
    "        df[f'sma_{period}'] = df['close'].rolling(period).mean()\n",
    "        df[f'ema_{period}'] = df['close'].ewm(span=period, adjust=False).mean()\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    df['rsi_14'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    ema12 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    ema26 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['macd'] = ema12 - ema26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # ATR (Crucial for V4 Dynamic Labeling)\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = abs(df['high'] - df['close'].shift())\n",
    "    low_close = abs(df['low'] - df['close'].shift())\n",
    "    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['atr_14'] = tr.rolling(14).mean()\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    df['bb_middle'] = df['close'].rolling(20).mean()\n",
    "    bb_std = df['close'].rolling(20).std()\n",
    "    df['bb_upper'] = df['bb_middle'] + 2 * bb_std\n",
    "    df['bb_lower'] = df['bb_middle'] - 2 * bb_std\n",
    "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']\n",
    "    \n",
    "    # --- 3. Advanced Features (NEW in v4) ---\n",
    "    # Distance from MA (Mean Reversion)\n",
    "    df['dist_sma50'] = (df['close'] - df['sma_50']) / df['sma_50'] * 100\n",
    "    df['dist_sma200'] = (df['close'] - df['sma_200']) / df['sma_200'] * 100\n",
    "    \n",
    "    # Volatility Ratio (Current ATR vs Long term ATR)\n",
    "    df['atr_50'] = tr.rolling(50).mean()\n",
    "    df['volatility_ratio'] = df['atr_14'] / (df['atr_50'] + 1e-10)\n",
    "    \n",
    "    # Momentum\n",
    "    df['mom_10'] = df['close'].pct_change(10)\n",
    "    df['mom_30'] = df['close'].pct_change(30)\n",
    "\n",
    "    # --- 4. Lag Features (NEW) ---\n",
    "    # Capture trends by looking at previous values\n",
    "    for col in ['rsi_14', 'macd', 'mom_10', 'volatility_ratio']:\n",
    "        df[f'{col}_lag1'] = df[col].shift(1)\n",
    "        df[f'{col}_lag2'] = df[col].shift(2)\n",
    "        df[f'{col}_lag3'] = df[col].shift(3)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Adding V4 features (with Lags)...\")\n",
    "train_df = add_features_v4(train_df)\n",
    "test_df = add_features_v4(test_df)\n",
    "print(f\"âœ“ Features added: {len(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c960200",
   "metadata": {},
   "source": [
    "## 2. Dynamic Labeling (ATR-Based)\n",
    "\n",
    "**V4 Innovation:**\n",
    "Instead of fixed 15 pips, we use **ATR-based targets**.\n",
    "- **Target:** Price must rise by `2.0 * ATR` (Take Profit)\n",
    "- **Stop:** Price must NOT fall by `1.5 * ATR` (Stop Loss)\n",
    "- This adapts to market volatility. In quiet markets, targets are smaller. In volatile markets, targets are larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "269cdd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dynamic Labels (TP=1.5*ATR, SL=1.0*ATR)...\n",
      "Creating Dynamic Labels (TP=1.5*ATR, SL=1.0*ATR)...\n",
      "Creating Dynamic Labels (TP=1.5*ATR, SL=1.0*ATR)...\n",
      "\n",
      "Label Distribution (Train):\n",
      "target\n",
      "0    0.587238\n",
      "1    0.412762\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Label Distribution (Train):\n",
      "target\n",
      "0    0.587238\n",
      "1    0.412762\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def create_dynamic_labels(df, forward_periods=60):\n",
    "    \"\"\"\n",
    "    V4 Labeling: Dynamic ATR-based targets.\n",
    "    TP = 1.5 * ATR\n",
    "    SL = 1.0 * ATR\n",
    "    Risk:Reward = 1:1.5\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    print(f\"Creating Dynamic Labels (TP=1.5*ATR, SL=1.0*ATR)...\")\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    # Vectorized approach for speed\n",
    "    # We'll iterate but optimize\n",
    "    closes = df['close'].values\n",
    "    highs = df['high'].values\n",
    "    lows = df['low'].values\n",
    "    atrs = df['atr_14'].values\n",
    "    \n",
    "    n = len(df)\n",
    "    \n",
    "    for i in range(n):\n",
    "        if i + forward_periods >= n or np.isnan(atrs[i]):\n",
    "            labels.append(0)\n",
    "            continue\n",
    "            \n",
    "        entry = closes[i]\n",
    "        atr = atrs[i]\n",
    "        \n",
    "        # Adjusted Targets for better hit rate\n",
    "        tp_dist = atr * 1.5\n",
    "        sl_dist = atr * 1.0\n",
    "        \n",
    "        tp_price = entry + tp_dist\n",
    "        sl_price = entry - sl_dist\n",
    "        \n",
    "        # Look forward\n",
    "        future_highs = highs[i+1 : i+forward_periods+1]\n",
    "        future_lows = lows[i+1 : i+forward_periods+1]\n",
    "        \n",
    "        # Check if SL hit first\n",
    "        sl_hit_indices = np.where(future_lows <= sl_price)[0]\n",
    "        first_sl_idx = sl_hit_indices[0] if len(sl_hit_indices) > 0 else forward_periods + 1\n",
    "        \n",
    "        # Check if TP hit first\n",
    "        tp_hit_indices = np.where(future_highs >= tp_price)[0]\n",
    "        first_tp_idx = tp_hit_indices[0] if len(tp_hit_indices) > 0 else forward_periods + 1\n",
    "        \n",
    "        if first_tp_idx < first_sl_idx:\n",
    "            labels.append(1) # WIN\n",
    "        else:\n",
    "            labels.append(0) # LOSS or TIMEOUT\n",
    "            \n",
    "    df['target'] = labels\n",
    "    return df\n",
    "\n",
    "train_df = create_dynamic_labels(train_df)\n",
    "test_df = create_dynamic_labels(test_df)\n",
    "\n",
    "print(f\"\\nLabel Distribution (Train):\")\n",
    "print(train_df['target'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1281e4",
   "metadata": {},
   "source": [
    "## 3. Feature Selection (Noise Reduction)\n",
    "\n",
    "**V4 Innovation:**\n",
    "Instead of using all features, we select the most important ones using a Random Forest selector. This reduces overfitting and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea0f9c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting best features...\n",
      "Original features: 40\n",
      "Selected features: 20\n",
      "Top features: ['rsi_14', 'macd_signal', 'macd_hist', 'atr_14', 'bb_width', 'dist_sma50', 'dist_sma200', 'atr_50', 'volatility_ratio', 'mom_10']\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "exclude_cols = ['time', 'target', 'open', 'high', 'low', 'close', 'tick_volume', 'volume']\n",
    "feature_cols = [c for c in train_df.columns if c not in exclude_cols]\n",
    "\n",
    "train_clean = train_df.dropna()\n",
    "test_clean = test_df.dropna()\n",
    "\n",
    "X_train_raw = train_clean[feature_cols]\n",
    "y_train = train_clean['target']\n",
    "X_test_raw = test_clean[feature_cols]\n",
    "y_test = test_clean['target']\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_test_scaled = scaler.transform(X_test_raw)\n",
    "\n",
    "# Feature Selection\n",
    "print(\"Selecting best features...\")\n",
    "selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1), threshold=\"median\")\n",
    "selector.fit(X_train_scaled, y_train)\n",
    "\n",
    "selected_indices = selector.get_support(indices=True)\n",
    "selected_features = [feature_cols[i] for i in selected_indices]\n",
    "\n",
    "print(f\"Original features: {len(feature_cols)}\")\n",
    "print(f\"Selected features: {len(selected_features)}\")\n",
    "print(\"Top features:\", selected_features[:10])\n",
    "\n",
    "# Transform X\n",
    "X_train = selector.transform(X_train_scaled)\n",
    "X_test = selector.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea816db7",
   "metadata": {},
   "source": [
    "## 4. Stacking Ensemble Training\n",
    "\n",
    "**V4 Innovation:**\n",
    "Using `StackingClassifier`. \n",
    "- **Base Models:** XGBoost, LightGBM, Random Forest\n",
    "- **Meta Model:** Logistic Regression (learns how to combine base models optimally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43229484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Soft Voting Ensemble (Regularized)...\n",
      "\n",
      "========================================\n",
      "V4 VOTING RESULTS (IMPROVED)\n",
      "========================================\n",
      "Accuracy:  0.5882\n",
      "Precision: 0.4800\n",
      "Recall:    0.0038\n",
      "F1 Score:  0.0076\n",
      "\n",
      "Probability Distribution:\n",
      "Min: 0.1962\n",
      "Max: 0.7201\n",
      "Mean: 0.4103\n",
      "Std: 0.0218\n",
      "Signals >= 0.5: 977\n",
      "Signals >= 0.6: 67\n",
      "Signals >= 0.7: 2\n",
      "Signals >= 0.8: 0\n",
      "\n",
      "========================================\n",
      "V4 VOTING RESULTS (IMPROVED)\n",
      "========================================\n",
      "Accuracy:  0.5882\n",
      "Precision: 0.4800\n",
      "Recall:    0.0038\n",
      "F1 Score:  0.0076\n",
      "\n",
      "Probability Distribution:\n",
      "Min: 0.1962\n",
      "Max: 0.7201\n",
      "Mean: 0.4103\n",
      "Std: 0.0218\n",
      "Signals >= 0.5: 977\n",
      "Signals >= 0.6: 67\n",
      "Signals >= 0.7: 2\n",
      "Signals >= 0.8: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Base Models - Increased Regularization & Reduced Complexity\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    n_estimators=500, \n",
    "    max_depth=6, \n",
    "    learning_rate=0.05, \n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=0.5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    device='cuda' if GPU_AVAILABLE else 'cpu'\n",
    ")\n",
    "\n",
    "lgb_clf = lgb.LGBMClassifier(\n",
    "    n_estimators=500, \n",
    "    max_depth=6, \n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=0.5,\n",
    "    random_state=42,\n",
    "    verbose=-1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=300, \n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Voting Classifier (Soft Voting)\n",
    "# We use Soft Voting to average the probabilities. This is often more robust than Stacking for noisy data.\n",
    "print(\"Training Soft Voting Ensemble (Regularized)...\")\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_clf),\n",
    "        ('lgb', lgb_clf),\n",
    "        ('rf', rf_clf)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "voting_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = voting_model.predict(X_test)\n",
    "y_proba = voting_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"V4 VOTING RESULTS (IMPROVED)\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Probability Distribution Check\n",
    "print(\"\\nProbability Distribution:\")\n",
    "print(f\"Min: {y_proba.min():.4f}\")\n",
    "print(f\"Max: {y_proba.max():.4f}\")\n",
    "print(f\"Mean: {y_proba.mean():.4f}\")\n",
    "print(f\"Std: {y_proba.std():.4f}\")\n",
    "\n",
    "# Check how many signals at different thresholds\n",
    "for th in [0.5, 0.6, 0.7, 0.8]:\n",
    "    count = (y_proba >= th).sum()\n",
    "    print(f\"Signals >= {th}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45628d18",
   "metadata": {},
   "source": [
    "## 5. Backtest with Dynamic SL/TP\n",
    "\n",
    "Since we trained with Dynamic Labels, the model should perform much better on the Dynamic SL/TP backtest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "557ac443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backtesting 67 signals (Conf >= 60%)...\n",
      "Conf >= 60%: 67 signals | WR: 61.2% | Pips: 51.3 | PF: 1.20\n",
      "Backtesting 12 signals (Conf >= 65%)...\n",
      "Conf >= 65%: 12 signals | WR: 75.0% | Pips: 46.4 | PF: 3.43\n",
      "Backtesting 2 signals (Conf >= 70%)...\n",
      "Conf >= 70%: 2 signals | WR: 100.0% | Pips: 16.0 | PF: 0.00\n",
      "Backtesting 0 signals (Conf >= 75%)...\n",
      "Backtesting 0 signals (Conf >= 80%)...\n"
     ]
    }
   ],
   "source": [
    "def backtest_v4(df, probabilities, threshold=0.80):\n",
    "    \"\"\"Backtest with Dynamic SL/TP (1.0 SL, 1.5 TP)\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Filter by threshold\n",
    "    indices = np.where(probabilities >= threshold)[0]\n",
    "    \n",
    "    print(f\"Backtesting {len(indices)} signals (Conf >= {threshold*100:.0f}%)...\")\n",
    "    \n",
    "    for idx in indices:\n",
    "        if idx + 60 >= len(df): continue\n",
    "        \n",
    "        entry = df['close'].iloc[idx]\n",
    "        atr = df['atr_14'].iloc[idx]\n",
    "        \n",
    "        # Dynamic SL/TP (Same as training logic)\n",
    "        sl_pips = (atr * 1.0) * 10000\n",
    "        tp_pips = (atr * 1.5) * 10000\n",
    "        \n",
    "        # Cap min/max for safety\n",
    "        sl_pips = max(5, min(20, sl_pips))\n",
    "        tp_pips = max(8, min(40, tp_pips))\n",
    "        \n",
    "        sl_price = entry - (sl_pips * 0.0001)\n",
    "        tp_price = entry + (tp_pips * 0.0001)\n",
    "        \n",
    "        # Check outcome\n",
    "        future = df.iloc[idx+1 : idx+61]\n",
    "        result = 'TIMEOUT'\n",
    "        pnl = 0\n",
    "        \n",
    "        for _, row in future.iterrows():\n",
    "            if row['low'] <= sl_price:\n",
    "                result = 'LOSS'\n",
    "                pnl = -sl_pips\n",
    "                break\n",
    "            if row['high'] >= tp_price:\n",
    "                result = 'WIN'\n",
    "                pnl = tp_pips\n",
    "                break\n",
    "        \n",
    "        if result == 'TIMEOUT':\n",
    "            end_price = future['close'].iloc[-1]\n",
    "            pnl = (end_price - entry) * 10000\n",
    "            # result = 'WIN' if pnl > 0 else 'LOSS'\n",
    "            \n",
    "        results.append(pnl)\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Run Backtest\n",
    "for conf in [0.60, 0.65, 0.70, 0.75, 0.80]:\n",
    "    pnls = backtest_v4(test_clean, y_proba, threshold=conf)\n",
    "    if len(pnls) > 0:\n",
    "        wins = len([p for p in pnls if p > 0])\n",
    "        total = len(pnls)\n",
    "        wr = wins / total * 100\n",
    "        total_pips = sum(pnls)\n",
    "        \n",
    "        # Profit Factor\n",
    "        gross_profit = sum([p for p in pnls if p > 0])\n",
    "        gross_loss = abs(sum([p for p in pnls if p < 0]))\n",
    "        pf = gross_profit / gross_loss if gross_loss > 0 else 0\n",
    "        \n",
    "        print(f\"Conf >= {conf*100:.0f}%: {total} signals | WR: {wr:.1f}% | Pips: {total_pips:.1f} | PF: {pf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e05b10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving V4 models...\n",
      "âœ… V4 Models saved successfully!\n",
      "âœ… V4 Models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save V4\n",
    "print(\"Saving V4 models...\")\n",
    "joblib.dump(voting_model, MODEL_DIR / 'voting_v4.joblib') # Changed from stacking to voting\n",
    "joblib.dump(scaler, MODEL_DIR / 'scaler_v4.joblib')\n",
    "joblib.dump(selector, MODEL_DIR / 'selector_v4.joblib')\n",
    "joblib.dump(selected_features, MODEL_DIR / 'features_v4.joblib')\n",
    "\n",
    "print(\"âœ… V4 Models saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
